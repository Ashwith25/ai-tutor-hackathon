{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947bfc78-3d3d-4000-99b7-c82fb1119133",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============== AGENT 2: INFORMATION RETRIEVAL ==============\n",
    "\n",
    "# --- Imports ---\n",
    "import os\n",
    "import socket\n",
    "from typing import List\n",
    "\n",
    "# LangChain and related libraries\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Search library\n",
    "try:\n",
    "    from googlesearch import search\n",
    "except ImportError:\n",
    "    print(\"Error: 'googlesearch-python' is not installed. Please run 'pip install googlesearch-python'\")\n",
    "\n",
    "# It's good practice to set a user agent for web requests\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0\"\n",
    "\n",
    "# --- Function 1: Dynamic Search (Corrected Version 2.0) ---\n",
    "def dynamic_search(query: str, num_results: int = 4) -> list[str]:\n",
    "    \"\"\"\n",
    "    Searches the web for a given query, focusing on recent and relevant\n",
    "    technical blogs and tutorials related to NVIDIA GPU acceleration.\n",
    "    \"\"\"\n",
    "    print(f\"-> Starting dynamic search for: '{query}'\")\n",
    "    enhanced_query = f\"{query} site:developer.nvidia.com OR site:medium.com/rapids-ai OR site:medium.com/cupy-team\"\n",
    "    print(f\"-> Enhanced query: '{enhanced_query}'\")\n",
    "    try:\n",
    "        # THE FIX IS HERE: Removed the 'query=' keyword argument.\n",
    "        search_results = list(search(enhanced_query, num_results=num_results))\n",
    "        \n",
    "        print(f\"-> Found URLs: {search_results}\")\n",
    "        return search_results\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during search: {e}\")\n",
    "        # Fallback to a static list if the search fails\n",
    "        return [\n",
    "            \"https://medium.com/cupy-team/announcing-cupy-v13-66979ee7fab0\",\n",
    "            \"https://www.unum.cloud/blog/2022-01-26-cupy\",\n",
    "        ]\n",
    "\n",
    "# --- Function 2: Main RAG Pipeline ---\n",
    "def process_with_rag(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Processes a user query using a Retrieval-Augmented Generation (RAG) pipeline.\n",
    "    \"\"\"\n",
    "    print(\"--- Running RAG Pipeline ---\")\n",
    "    # Step 1: Get URLs from the dynamic search function\n",
    "    urls = dynamic_search(query)\n",
    "    if not urls:\n",
    "        return \"Sorry, I couldn't find any relevant articles online for your query.\"\n",
    "\n",
    "    # Step 2: Load the documents from the web\n",
    "    print(\"-> Loading documents from URLs...\")\n",
    "    docs = [WebBaseLoader(url).load() for url in urls]\n",
    "    docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "    # Step 3: Split the documents into chunks\n",
    "    print(\"-> Splitting documents into chunks...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=500, chunk_overlap=100\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "    # Step 4: Create embeddings and a vector store retriever\n",
    "    print(\"-> Creating embeddings and vector store...\")\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") #\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=doc_splits,\n",
    "        embedding=embedding_model,\n",
    "        collection_name=\"rag-chroma\"\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    # Step 5: Generate an answer using the LLM and the retrieved context\n",
    "    print(\"-> Retrieving context and generating answer...\")\n",
    "    GENERATE_PROMPT = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are an AI assistant for data scientists. Answer the user's question based ONLY on the context provided.\n",
    "        Focus on practical code examples and clear explanations from the context.\n",
    "        If the context doesn't contain enough information, just say that you couldn't find a specific answer in the provided documents.\n",
    "\n",
    "        CONTEXT:\n",
    "        {context}\n",
    "\n",
    "        QUESTION:\n",
    "        {question}\n",
    "\n",
    "        YOUR ANSWER:\n",
    "        \"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    context_text = \"\\\\n\\\\n---\\\\n\\\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    # Connect to the local LLM on Sol\n",
    "    host_node = socket.gethostname()\n",
    "    llm = OllamaLLM(model=\"qwen3:14b\", base_url=f\"http://apoojar4@{host_node}:11434/\") # We'll need to update the user ID\n",
    "\n",
    "    rag_chain = GENERATE_PROMPT | llm\n",
    "    answer = rag_chain.invoke({\"context\": context_text, \"question\": query})\n",
    "\n",
    "    # Clean up the vector store\n",
    "    vectorstore.delete_collection()\n",
    "    \n",
    "    print(\"--- RAG Pipeline Complete ---\")\n",
    "    return answer\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == '__main__':\n",
    "    test_query = \"I want to do an array multiplication of the dimension 10x10. How can I do that ion Python?\"\n",
    "    final_answer = process_with_rag(test_query)\n",
    "    print(\"\\\\n======= FINAL ANSWER =======\\\\n\")\n",
    "    print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c84f58d-8501-4417-9765-ce486959b9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= TEST 1: Python GPU Question =======\n",
      "--- Running FINAL DYNAMIC RAG Pipeline ---\n",
      "-> Using LLM with DYNAMIC prompt to generate search queries...\n",
      "-> Generated queries: ['Python array multiplication 100x100 matrix CPU', 'How to multiply 100x100 arrays in Python using NumPy', 'GPU accelerated array multiplication Python 100x100', 'CUDA array multiplication Python 100x100 matrix', 'Using CuPy for 100x100 matrix multiplication in Python']\n",
      "-> Executing dynamic search...\n",
      "-> Found 2 unique URLs: ['https://forums.developer.nvidia.com/t/cublas-kernels-always-run-serially-despite-streams-and-asyncmemcpy/39661', 'https://forums.developer.nvidia.com/t/lots-of-small-matrices/20260']\n",
      "-> Found documents. Loading and processing context...\n",
      "--- Pipeline Complete ---\n",
      "\n",
      "======= FINAL ANSWER 1 =======\n",
      "\n",
      "<think>\n",
      "Okay, the user wants to perform array multiplication on a 100x100 matrix in Python. Let me think about how to approach this.\n",
      "\n",
      "First, the core task is matrix multiplication. In Python, the standard libraries for this are NumPy and SciPy. For GPU acceleration, libraries like CuPy or PyTorch might be used. But the user didn't specify GPU, so I should check if they want GPU acceleration. Wait, the original question just says \"How can I do that in Python?\" without mentioning GPU. But looking back at the context provided, there's a lot of discussion about GPU acceleration for small matrices, like in the NVIDIA forums. However, the user's question here is about a single 100x100 matrix multiplication, not a batch of small matrices. \n",
      "\n",
      "Wait, the user's question is about array multiplication, which could be matrix multiplication. The standard way in Python is using NumPy's matmul or @ operator. For GPU acceleration, CuPy is a NumPy-like library that uses CUDA. So, the answer should present both CPU (NumPy) and GPU (CuPy) solutions.\n",
      "\n",
      "But wait, the user's question didn't mention GPU, so maybe they just want the standard method. However, the context from the forums suggests that the user might be interested in GPU acceleration for batch processing of small matrices, but the current question is about a single 100x100 matrix. However, the user might be following up on that context. \n",
      "\n",
      "But according to the instructions, I need to check if a common GPU-accelerated library exists for the task. Matrix multiplication is definitely supported by CuPy. So, PATH 1 applies here. \n",
      "\n",
      "So, the answer should first present the GPU-accelerated solution using CuPy and then the standard CPU solution with NumPy. \n",
      "\n",
      "Wait, but the user's question is about array multiplication. If they're using NumPy, the standard method would be using np.dot or @. For CuPy, it's similar. \n",
      "\n",
      "But I need to make sure the code examples are correct. For example, in NumPy, multiplying two 100x100 matrices would be done with np.dot(a, b) or a @ b. Similarly in CuPy, it's the same syntax. \n",
      "\n",
      "Also, the key points should mention that for single matrices, the overhead of transferring data to the GPU might not be worth it, but for large batches, it's beneficial. However, the user's question is about a single matrix, so maybe the performance note should mention that. \n",
      "\n",
      "Wait, but the user's question is about a single 100x100 matrix. The context mentioned batches of 40,000 matrices, but the current question is different. So, the answer should focus on the single matrix case. However, the user might be interested in GPU acceleration even for a single matrix. \n",
      "\n",
      "In any case, the answer should present both methods. So, the structure would be:\n",
      "\n",
      "- Friendly opening\n",
      "- GPU solution with CuPy\n",
      "- CPU solution with NumPy\n",
      "- Key points about performance for single vs. batched matrices.\n",
      "\n",
      "But the user's question is about a single matrix, so the key points should note that for single matrices, CPU is better, but for batches, GPU is better. However, the user's question is about a single matrix, so maybe the performance note should mention that using GPU for a single matrix might not be efficient due to overhead, but for multiple matrices, it's better.\n",
      "\n",
      "But the code examples would still be provided as per the libraries. \n",
      "\n",
      "So, the answer is following PATH 1, with CuPy as the GPU library and NumPy as the CPU library.\n",
      "</think>\n",
      "\n",
      "### GPU-Accelerated Solution (with CuPy)  \n",
      "To perform matrix multiplication on a 100x100 matrix using a GPU in Python, you can use **CuPy**, a NumPy-like library that leverages CUDA for acceleration. Here's an example:\n",
      "\n",
      "```python\n",
      "import cupy as cp\n",
      "\n",
      "# Create two 100x100 matrices on the GPU\n",
      "a_gpu = cp.random.rand(100, 100)\n",
      "b_gpu = cp.random.rand(100, 100)\n",
      "\n",
      "# Perform matrix multiplication on the GPU\n",
      "result_gpu = a_gpu @ b_gpu  # or cp.dot(a_gpu, b_gpu)\n",
      "\n",
      "# Transfer the result back to the CPU if needed\n",
      "result_cpu = result_gpu.get()\n",
      "```\n",
      "\n",
      "### Standard CPU Solution (with NumPy)  \n",
      "For a CPU-based solution, **NumPy** is the standard library for numerical computing in Python:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "# Create two 100x100 matrices on the CPU\n",
      "a_cpu = np.random.rand(100, 100)\n",
      "b_cpu = np.random.rand(100, 100)\n",
      "\n",
      "# Perform matrix multiplication on the CPU\n",
      "result_cpu = a_cpu @ b_cpu  # or np.dot(a_cpu, b_cpu)\n",
      "```\n",
      "\n",
      "### Key Points  \n",
      "- **GPU Advantage**: For large batches of matrices (e.g., 40,000 matrices), GPU acceleration with CuPy can provide significant speedups due to parallelism. However, for a **single 100x100 matrix**, the overhead of transferring data to and from the GPU may make the CPU solution faster.  \n",
      "- **Scalability**: If you plan to process many matrices (e.g., in a loop or batch), the GPU approach becomes more efficient.  \n",
      "- **Compatibility**: CuPy mimics NumPy's API, making it easy to switch between CPU and GPU code.\n",
      "\n",
      "\n",
      "\n",
      "======= TEST 2: Python NON-GPU Question =======\n",
      "--- Running FINAL DYNAMIC RAG Pipeline ---\n",
      "-> Using LLM with DYNAMIC prompt to generate search queries...\n",
      "-> Generated queries: ['symPy solve differential equation', 'display solution differential equation SymPy', 'SymPy solve ODE formatting', 'GPU acceleration SymPy differential equations', 'SymPy and GPU solving differential equations']\n",
      "-> Executing dynamic search...\n",
      "-> Found 4 unique URLs: ['https://forums.developer.nvidia.com/t/sympy-max-not-working-in-pde-cannot-determine-truth-value-of-relational/257708', 'https://forums.developer.nvidia.com/t/zero-equation-model-error-function-object-has-no-attribute-free-symbols/196047', 'https://forums.developer.nvidia.com/t/handling-an-integral-term-in-the-differential-equation/221079', 'https://forums.developer.nvidia.com/t/values-of-output-variables-keys-at-grid-points/252609']\n",
      "-> Found documents. Loading and processing context...\n",
      "--- Pipeline Complete ---\n",
      "\n",
      "======= FINAL ANSWER 2 =======\n",
      "\n",
      "<think>\n",
      "Okay, the user is asking how to use SymPy to solve a differential equation and display the result in a nicely formatted way. Let me break this down.\n",
      "\n",
      "First, I need to recall the standard approach for solving differential equations with SymPy. The dsolve function is the key here. It can handle both ordinary and partial differential equations. For ODEs, you can specify the equation, the function to solve for, and the independent variable.\n",
      "\n",
      "Next, the user wants the result to be nicely formatted. SymPy's printing capabilities are pretty good. The pprint function can display expressions in a more readable format. Also, using the Eq or Eqn classes might help in presenting the solution as an equation.\n",
      "\n",
      "Wait, the user might not be familiar with the exact syntax for dsolve. So I should provide a clear example. Let's take a simple ODE, like the exponential decay equation. That's a common example and easy to understand.\n",
      "\n",
      "I should outline the steps: define the symbols, set up the differential equation, solve it with dsolve, and then print the solution using pprint. Also, mention the options for different solvers if needed.\n",
      "\n",
      "Another point is the formatting. The user might want to see the solution in terms of LaTeX or a more structured equation. Explaining the use of Eq and the different printing methods would be helpful.\n",
      "\n",
      "I should also consider if there are any common pitfalls. For example, making sure that the equation is correctly defined with the derivative, and that the function and variables are properly declared.\n",
      "\n",
      "Including an example with a more complex equation might be useful, but since the user is asking for a general method, a simple example should suffice. Also, mentioning the possibility of initial conditions if they want a particular solution.\n",
      "\n",
      "Wait, the original question doesn't mention initial conditions, so maybe stick to the general solution. But it's worth noting that dsolve can handle initial conditions as well.\n",
      "\n",
      "In terms of code structure, I'll need to import symbols, define the function, set up the equation, solve, and then print. Let me make sure the code example is correct and uses the right syntax.\n",
      "\n",
      "Testing the code in my mind: symbols('t x'), f = Function('f'), eq = Eq(f(t).diff(t), -k*f(t)), sol = dsolve(eq, f(t), ics=[])... then pprint(sol). That should work.\n",
      "\n",
      "For the nicely formatted display, using pprint with the solution should show it in a more readable form. Also, maybe mention the use of the 'pretty' printer or converting to LaTeX.\n",
      "\n",
      "I think that's all. The answer should guide the user through solving the ODE with SymPy and formatting the output.\n",
      "</think>\n",
      "\n",
      "### GPU-Accelerated Solution (with [No Applicable Library])\n",
      "\n",
      "SymPy is a CPU-based symbolic mathematics library and does not have a direct GPU-accelerated counterpart for solving differential equations. However, for numerical solutions of differential equations, GPU acceleration is typically handled by libraries like **SciPy** (CPU) or **CuPy** (GPU). Since your task involves symbolic computation (not numerical solving), GPU acceleration is not directly applicable here.\n",
      "\n",
      "### Standard CPU Solution (with SymPy)\n",
      "\n",
      "Here's how you can solve a differential equation using SymPy and display the result in a nicely formatted way:\n",
      "\n",
      "```python\n",
      "from sympy import symbols, Eq, Function, dsolve, pprint\n",
      "\n",
      "# Define symbols\n",
      "t = symbols('t')\n",
      "x = symbols('x')\n",
      "k = symbols('k')\n",
      "\n",
      "# Define the function and differential equation\n",
      "f = Function('f')(t)\n",
      "eq = Eq(f.diff(t), -k * f)\n",
      "\n",
      "# Solve the differential equation\n",
      "solution = dsolve(eq, f, ics=[])\n",
      "\n",
      "# Display the solution in a nicely formatted way\n",
      "pprint(solution)\n",
      "```\n",
      "\n",
      "**Key Points:**\n",
      "- SymPy is designed for symbolic mathematics and does not leverage GPUs. For large-scale numerical computations, consider using GPU-accelerated libraries like **CuPy** or **JAX** for numerical solvers (e.g., solving ODEs with `scipy.integrate.solve_ivp` on the GPU via CuPy or JAX).\n",
      "- The `pprint` function from SymPy provides a human-readable format for equations, which is ideal for displaying solutions in a clear and structured manner.\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# AI TUTOR AGENT (DEFINITIVE, DYNAMIC & ABSTRACT FINAL)\n",
    "# Language-and-Library-Agnostic with a Conversational Persona\n",
    "# =================================================================\n",
    "\n",
    "import os\n",
    "import socket\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "# LangChain and related libraries\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "# Search library\n",
    "try:\n",
    "    from googlesearch import search\n",
    "except ImportError:\n",
    "    print(\"Error: 'googlesearch-python' is not installed. Please run 'pip install googlesearch-python'\")\n",
    "\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0\"\n",
    "\n",
    "class SearchQueryGenerator(BaseModel):\n",
    "    queries: List[str] = Field(description=\"A list of targeted, keyword-focused search queries.\")\n",
    "\n",
    "# --- Part 1: Fully Abstracted Search Query Generation ---\n",
    "def generate_search_queries(query: str, llm) -> List[str]:\n",
    "    \"\"\"\n",
    "    Uses a dynamic prompt to generate language-and-library-specific search queries.\n",
    "    \"\"\"\n",
    "    print(\"-> Using LLM with DYNAMIC prompt to generate search queries...\")\n",
    "    \n",
    "    # DYNAMIC PROMPT: The LLM now identifies the correct libraries and language.\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are an expert at generating web search queries for a technical audience.\n",
    "        Analyze the user's question to identify the core technical task, the programming language, and any relevant libraries or dimensions.\n",
    "        Generate 5 concise, targeted search queries based on this analysis. Two queries should be for the standard, CPU-based approach for the identified task and language. Three queries should be for GPU-accelerated approaches, if they exist for that language and task.\n",
    "\n",
    "        User Question: \"{question}\"\n",
    "        \n",
    "        Generate a JSON list of 5 search query strings.\n",
    "        \"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "    \n",
    "    query_generation_chain = prompt_template | llm.with_structured_output(SearchQueryGenerator)\n",
    "\n",
    "    try:\n",
    "        response_model = query_generation_chain.invoke({\"question\": query})\n",
    "        print(f\"-> Generated queries: {response_model.queries}\")\n",
    "        return response_model.queries\n",
    "    except Exception as e:\n",
    "        print(f\"-> LLM failed to generate structured output: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Part 2: The Main Search Function (Unchanged) ---\n",
    "def dynamic_search_agentic(queries: List[str]) -> list[str]:\n",
    "    print(\"-> Executing dynamic search...\")\n",
    "    all_urls = set()\n",
    "    for q in queries:\n",
    "        try:\n",
    "            enhanced_query = f\"{q} site:developer.nvidia.com OR site:medium.com/rapids-ai OR site:medium.com/cupy-team\"\n",
    "            search_results = list(search(enhanced_query, num_results=2))\n",
    "            for url in search_results:\n",
    "                all_urls.add(url)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during search for query '{q}': {e}\")\n",
    "            continue\n",
    "    final_urls = [url for url in list(all_urls) if url]\n",
    "    print(f\"-> Found {len(final_urls)} unique URLs: {final_urls}\")\n",
    "    return final_urls\n",
    "\n",
    "# --- Part 3: The Definitive Dynamic RAG Pipeline ---\n",
    "def process_with_rag(query: str) -> str:\n",
    "    print(\"--- Running FINAL DYNAMIC RAG Pipeline ---\")\n",
    "\n",
    "    host_node = socket.gethostname()\n",
    "    llm = ChatOllama(model=\"qwen3:14b\", base_url=f\"http://jgarc111@{host_node}:11434/\") # Placeholder ID\n",
    "\n",
    "    search_queries = generate_search_queries(query, llm)\n",
    "    urls = []\n",
    "    if search_queries:\n",
    "        urls = dynamic_search_agentic(search_queries)\n",
    "\n",
    "    context_text = \"\"\n",
    "    if urls:\n",
    "        print(\"-> Found documents. Loading and processing context...\")\n",
    "        docs = [WebBaseLoader(url).load() for url in urls]\n",
    "        docs_list = [item for sublist in docs for item in sublist]\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500, chunk_overlap=100)\n",
    "        doc_splits = text_splitter.split_documents(docs_list)\n",
    "        embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        vectorstore = Chroma.from_documents(documents=doc_splits, embedding=embedding_model, collection_name=\"rag-chroma\")\n",
    "        retriever = vectorstore.as_retriever()\n",
    "        retrieved_docs = retriever.invoke(query)\n",
    "        context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "        vectorstore.delete_collection()\n",
    "\n",
    "    # THE DEFINITIVE, FULLY ABSTRACTED \"EDUCATIONAL PRIORITY\" PROMPT\n",
    "    final_prompt_template = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are a friendly and knowledgeable AI Tutor for data science. Your goal is to provide a helpful, accurate, and encouraging answer to the user's question, adapting to the specific programming language and libraries they ask about.\n",
    "\n",
    "        First, analyze the user's QUESTION to identify the core task. Then, based on your knowledge, determine if a common GPU-accelerated library exists for that specific task and programming language.\n",
    "\n",
    "        Then, follow the appropriate path below to structure your conversational answer:\n",
    "\n",
    "        **PATH 1: A common GPU-accelerated library EXISTS for this task.**\n",
    "        1.  Identify the standard CPU library and the GPU-accelerated library for the user's specified language and task.\n",
    "        2.  Start with a friendly opening that explains that you will show both the GPU-accelerated and standard methods.\n",
    "        3.  Provide a heading for the GPU solution, dynamically inserting the library name (e.g., `### GPU-Accelerated Solution (with [GPU Library Name])`).\n",
    "        4.  Write the code example using the identified GPU library.\n",
    "        5.  Add a \"Key Points\" or \"Performance Note\" section. Explain the benefits of the GPU approach (e.g., for large datasets) and any relevant trade-offs (e.g., performance on small data vs. large data).\n",
    "        6.  Provide a heading for the CPU solution, dynamically inserting the library name (e.g., `### Standard CPU Solution (with [CPU Library Name])`).\n",
    "        7.  Write the CPU-based code example for comparison.\n",
    "        8.  Do NOT add any disclaimer note at the end.\n",
    "\n",
    "        **PATH 2: A common GPU-accelerated library DOES NOT EXIST for this task.**\n",
    "        1.  Identify the standard library for the user's specified language and task.\n",
    "        2.  Start with a friendly opening that explains the standard approach for the task.\n",
    "        3.  Provide a heading for the standard solution, dynamically inserting the library name (e.g., `### Standard Solution (with [Library Name])`).\n",
    "        4.  Write the code example using the identified standard library.\n",
    "        5.  End your entire response with the exact sentence: \"Note: The provided solution is the standard method for this task, as it is not typically accelerated by GPUs.\"\n",
    "\n",
    "        Use the CONTEXT below to inform your answer if it is relevant, but your primary instruction is to follow the logic paths and conversational tone described above.\n",
    "\n",
    "        CONTEXT:\n",
    "        {context}\n",
    "\n",
    "        QUESTION:\n",
    "        {question}\n",
    "\n",
    "        YOUR FINAL ANSWER:\n",
    "        \"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "    \n",
    "    final_chain = final_prompt_template | llm\n",
    "    answer = final_chain.invoke({\"context\": context_text, \"question\": query}).content\n",
    "\n",
    "    print(\"--- Pipeline Complete ---\")\n",
    "    return answer\n",
    "\n",
    "# --- Example Usage Block ---\n",
    "if __name__ == '__main__':\n",
    "    # Test 1: Python GPU-related question\n",
    "    print(\"======= TEST 1: Python GPU Question =======\")\n",
    "    test_query_gpu = \"I want to do an array multiplication of the dimension 100x100. How can I do that in Python?\"\n",
    "    final_answer_gpu = process_with_rag(test_query_gpu)\n",
    "    print(\"\\n======= FINAL ANSWER 1 =======\\n\")\n",
    "    print(final_answer_gpu)\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # Test 2: Python Non-GPU question\n",
    "    print(\"======= TEST 2: Python NON-GPU Question =======\")\n",
    "    test_query_cpu = \"How can I use SymPy to solve a differential equation and display the result in a nicely formatted way?\"\n",
    "    final_answer_cpu = process_with_rag(test_query_cpu)\n",
    "    print(\"\\n======= FINAL ANSWER 2 =======\\n\")\n",
    "    print(final_answer_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414516ff-c259-4898-bb4c-3492fdac9549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1302331/790829847.py:51: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
      "  chatbot = gr.Chatbot(label=\"Conversation\", height=450, bubble_full_width=False, type=\"messages\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* Running on public URL: https://8f59c4c0242d8eb35a.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://8f59c4c0242d8eb35a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running FINAL DYNAMIC RAG Pipeline ---\n",
      "-> Using LLM with DYNAMIC prompt to generate search queries...\n",
      "-> LLM failed to generate structured output: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/httpx/_transports/default.py\", line 250, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n",
      "    raise exc from None\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n",
      "    response = connection.handle_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 101, in handle_request\n",
      "    raise exc\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 78, in handle_request\n",
      "    stream = self._connect(request)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 124, in _connect\n",
      "    stream = self._network_backend.connect_tcp(**kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/httpcore/_backends/sync.py\", line 207, in connect_tcp\n",
      "    with map_exceptions(exc_map):\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n",
      "    raise to_exc(exc) from exc\n",
      "httpcore.ConnectError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/gradio/queueing.py\", line 715, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/gradio/blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/gradio/blocks.py\", line 1743, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/gradio/utils.py\", line 739, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/gradio/utils.py\", line 733, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/gradio/utils.py\", line 716, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/gradio/utils.py\", line 877, in gen_wrapper\n",
      "    response = next(iterator)\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1302331/790829847.py\", line 71, in handle_submit\n",
      "    for history_state, thought_process in response_generator:\n",
      "                                          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1302331/790829847.py\", line 22, in tutor_chat_interface\n",
      "    full_response = process_with_rag(user_message)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1302331/2113105727.py\", line 145, in process_with_rag\n",
      "    answer = final_chain.invoke({\"context\": context_text, \"question\": query}).content\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/ollama/_client.py\", line 165, in inner\n",
      "    with self._client.stream(*args, **kwargs) as r:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/contextlib.py\", line 137, in __enter__\n",
      "    return next(self.gen)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/httpx/_client.py\", line 868, in stream\n",
      "    response = self.send(\n",
      "               ^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/httpx/_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/httpx/_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/httpx/_client.py\", line 1014, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/httpx/_transports/default.py\", line 249, in handle_request\n",
      "    with map_httpcore_exceptions():\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"/packages/envs/genai25.06/lib/python3.12/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.ConnectError: [Errno 111] Connection refused\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# GRADIO CHAT APPLICATION FOR AI TUTOR (FINAL)\n",
    "#\n",
    "# This version fixes the UserWarning by specifying the modern 'messages'\n",
    "# format for the chatbot component.\n",
    "# =================================================================\n",
    "\n",
    "import gradio as gr\n",
    "import re\n",
    "import time\n",
    "\n",
    "# This wrapper function connects our backend logic to the Gradio UI.\n",
    "# It assumes 'process_with_rag' is defined and available in the notebook environment.\n",
    "def tutor_chat_interface(user_message, history):\n",
    "    # The 'history' is now a list of dictionaries. Append the new user message.\n",
    "    history.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    # Show a \"thinking...\" message while processing\n",
    "    yield history + [{\"role\": \"assistant\", \"content\": \"Thinking...\"}], \"Thinking...\"\n",
    "\n",
    "    # Call our existing RAG pipeline function\n",
    "    full_response = process_with_rag(user_message)\n",
    "\n",
    "    # Use regex to separate the <think> block from the final answer\n",
    "    think_pattern = re.compile(r\"<think>(.*?)</think>\", re.DOTALL)\n",
    "    think_match = think_pattern.search(full_response)\n",
    "    \n",
    "    thought_process = \"No thought process was found in the response.\"\n",
    "    if think_match:\n",
    "        # Extract the thought process and clean it up\n",
    "        thought_process = think_match.group(1).strip()\n",
    "        # Remove the <think> block from the final answer shown to the user\n",
    "        final_answer = think_pattern.sub(\"\", full_response).strip()\n",
    "    else:\n",
    "        # If no <think> block, the whole response is the answer\n",
    "        final_answer = full_response\n",
    "\n",
    "    # Update the chatbot history with the AI's clean answer\n",
    "    history.append({\"role\": \"assistant\", \"content\": final_answer})\n",
    "    \n",
    "    # Return the final history and the extracted thought process\n",
    "    yield history, thought_process\n",
    "\n",
    "\n",
    "# Build the Gradio UI using Blocks for more control\n",
    "with gr.Blocks(theme=gr.themes.Soft(), css=\".gradio-container {background-color: #f5f5f5;}\") as demo:\n",
    "    gr.Markdown(\"# ðŸ¤– AI Accelerated Data Science Tutor\")\n",
    "    gr.Markdown(\"Ask a question about a data science task. The tutor will provide an explanation and code, prioritizing NVIDIA GPU-accelerated solutions where possible.\")\n",
    "\n",
    "    # THE FIX IS HERE: Added 'type=\"messages\"' to the chatbot component.\n",
    "    chatbot = gr.Chatbot(label=\"Conversation\", height=450, bubble_full_width=False, type=\"messages\")\n",
    "    \n",
    "    with gr.Accordion(\"ðŸ”Ž Show Agent's Thought Process\", open=False):\n",
    "        cot_output = gr.Markdown(\"The agent's reasoning will appear here after it responds.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        msg_textbox = gr.Textbox(\n",
    "            label=\"Your Question\",\n",
    "            placeholder=\"e.g., How do I multiply two 10x10 arrays in Python?\",\n",
    "            scale=4,\n",
    "            autofocus=True,\n",
    "            container=False # This makes the textbox look cleaner\n",
    "        )\n",
    "        submit_btn = gr.Button(\"Ask\", variant=\"primary\", scale=1, min_width=150)\n",
    "\n",
    "    # Main function to handle the chat logic\n",
    "    def handle_submit(user_message, chat_history):\n",
    "        response_generator = tutor_chat_interface(user_message, chat_history)\n",
    "        \n",
    "        # The generator now yields the full message history and the thought process\n",
    "        for history_state, thought_process in response_generator:\n",
    "             yield history_state, thought_process\n",
    "\n",
    "\n",
    "    # Connect the submit button and textbox to the handler function\n",
    "    submit_btn.click(\n",
    "        handle_submit, \n",
    "        [msg_textbox, chatbot], \n",
    "        [chatbot, cot_output]\n",
    "    ).then(lambda: gr.update(value=\"\"), None, [msg_textbox], queue=False)\n",
    "\n",
    "    msg_textbox.submit(\n",
    "        handle_submit, \n",
    "        [msg_textbox, chatbot], \n",
    "        [chatbot, cot_output]\n",
    "    ).then(lambda: gr.update(value=\"\"), None, [msg_textbox], queue=False)\n",
    "\n",
    "\n",
    "# Launch the application\n",
    "# Set share=True if you need a public link from the Sol jupyter notebook\n",
    "demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564167a0-3f71-4004-8c3f-0822950deb33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai25.06",
   "language": "python",
   "name": "genai25.06"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
