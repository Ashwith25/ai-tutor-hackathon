{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a431c68-e602-4df4-b2ac-6de5c51a90e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/envs/genai25.06/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3672: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully connected to Ollama LLM.\n",
      "\n",
      "ðŸ¤– Generating a personalized questionnaire for you...\n",
      "ðŸ¤– Let's get started. Please answer the following:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1. Have you used Python before, and if so, how long have you been programming in it?  2 years\n",
      "2. Are you familiar with libraries like NumPy or Pandas for data manipulation and analysis?  yes\n",
      "3. Have you worked with GPU computing or heard about using GPUs to accelerate data processing tasks?  no \n",
      "4. Are you aware of any NVIDIA libraries designed for GPU acceleration, such as CuPy or RAPIDS?  no\n",
      "5. Have you used any libraries or tools for data science that leverage GPU computing, and if so, which ones?  no\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– Analyzing your responses and creating a profile...\n",
      "\n",
      "--- Generated Profile Report ---\n",
      "--- User Profile for 2 years ---\n",
      "<think>\n",
      "Okay, let's start by analyzing the user's answers. They mentioned having 2 years of experience with Python, which is a good start. They are familiar with NumPy and Pandas, so that's intermediate knowledge in data manipulation. However, they haven't worked with GPU computing or NVIDIA libraries like CuPy or RAPIDS. They also haven't used any libraries that leverage GPU computing.\n",
      "\n",
      "So, their knowledge level is probably intermediate. They have some experience with data science tools but no experience with GPU acceleration. The next step is to check the strategy format for intermediate users. The strategy should involve code comparisons between NumPy and CuPy, focus on performance benefits, and include benchmarking examples. That makes sense because they already know the basics but need to see how GPU can enhance their current workflows. I need to make sure the report starts with the exact phrase and follows the structure given.\n",
      "</think>\n",
      "\n",
      "**Knowledge Level**: Intermediate  \n",
      "**TUTORING STRATEGY: The user is at an intermediate level.**  \n",
      "The tutor should provide direct code comparisons (e.g., NumPy vs. CuPy), focus on performance benefits, and show clear benchmarking examples. Since the user is familiar with NumPy and Pandas but has no experience with GPU computing, the strategy should emphasize practical transitions from CPU-based workflows to GPU-accelerated alternatives. Highlight tangible advantages (e.g., speedups for large datasets) and introduce NVIDIA libraries like CuPy as scalable upgrades to existing tools, using hands-on examples to bridge the gap between current skills and GPU capabilities.\n",
      "--- End of Profile ---\n",
      "\n",
      "âœ… User profile report saved successfully to: user_profiles/2_years.txt\n"
     ]
    }
   ],
   "source": [
    "# ============== AGENT 1: LLM-POWERED USER PROFILER (V3 - Corrected) ==============\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import socket\n",
    "from typing import List\n",
    "\n",
    "# As per the deprecation warning, we should import from Pydantic V2.\n",
    "# This future-proofs our code.\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "# --- Pydantic model for structured LLM output ---\n",
    "class Questionnaire(BaseModel):\n",
    "    questions: List[str] = Field(description=\"A list of 4-5 questions for the user.\")\n",
    "\n",
    "class UserProfilerAgent_V3:\n",
    "    \"\"\"\n",
    "    Agent 1 (V3): Guarantees the user's name is collected first before\n",
    "    using an LLM to dynamically generate the rest of the questionnaire.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, profiles_dir: str = \"user_profiles\"):\n",
    "        self.profiles_dir = profiles_dir\n",
    "        if not os.path.exists(self.profiles_dir):\n",
    "            os.makedirs(self.profiles_dir)\n",
    "\n",
    "        try:\n",
    "            host_node = socket.gethostname()\n",
    "            # NOTE: Replace 'jgarc111' with the ASURITE ID of the user running the Ollama server\n",
    "            self.llm = ChatOllama(model=\"qwen3:14b\", base_url=f\"http://jgarc111@{host_node}:11434/\")\n",
    "            self.structured_llm = self.llm.with_structured_output(Questionnaire)\n",
    "            print(\"âœ… Successfully connected to Ollama LLM.\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error connecting to Ollama: {e}\")\n",
    "            self.llm = None\n",
    "\n",
    "    def _generate_questions_with_llm(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Uses an LLM to dynamically generate a user questionnaire.\n",
    "        **Correction:** The prompt now explicitly tells the LLM to NOT ask for a name.\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ¤– Generating a personalized questionnaire for you...\")\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            template=\"\"\"\n",
    "            You are a helpful assistant for an AI Data Science Tutor. Your goal is to create a short questionnaire (4-5 questions) to understand a user's knowledge level.\n",
    "            \n",
    "            The questions should gently probe their experience with:\n",
    "            1. The Python programming language.\n",
    "            2. Common CPU-based data science libraries (like NumPy, Pandas).\n",
    "            3. Their awareness of GPU computing and hardware acceleration.\n",
    "            4. Their familiarity with any NVIDIA-specific GPU libraries (like CuPy or RAPIDS).\n",
    "\n",
    "            IMPORTANT: Do NOT ask for the user's name, as it has already been collected.\n",
    "            Return the questions as a JSON list. Be conversational and friendly.\n",
    "            \"\"\",\n",
    "            input_variables=[],\n",
    "        )\n",
    "        \n",
    "        query_generation_chain = prompt | self.structured_llm\n",
    "        \n",
    "        try:\n",
    "            response_model = query_generation_chain.invoke({})\n",
    "            return response_model.questions\n",
    "        except Exception as e:\n",
    "            print(f\"-> LLM failed to generate questions, falling back to default. Error: {e}\")\n",
    "            return [\n",
    "                \"On a scale of 1-5, how comfortable are you with Python?\",\n",
    "                \"Which Python data science libraries (like Pandas or NumPy) have you used before?\",\n",
    "                \"Have you ever heard of using GPUs to speed up data analysis?\",\n",
    "                \"What's the first tool you'd reach for to do a large matrix multiplication in Python?\"\n",
    "            ]\n",
    "\n",
    "    def _ask_dynamic_questions(self, questions: List[str]) -> dict:\n",
    "        \"\"\"\n",
    "        Asks the dynamically generated questions and records the answers.\n",
    "        \"\"\"\n",
    "        answers = {}\n",
    "        for i, question in enumerate(questions, start=1):\n",
    "            answer = input(f\"{i+1}. {question} \") # Start numbering from 2\n",
    "            answers[question] = answer\n",
    "        return answers\n",
    "\n",
    "    def _generate_report_with_llm(self, user_name: str, answers: dict) -> str:\n",
    "        \"\"\"\n",
    "        Sends the user's answers to the LLM to generate a profile report.\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ¤– Analyzing your responses and creating a profile...\")\n",
    "        \n",
    "        answers_str = \"\\n\".join([f\"- {q}: {a}\" for q, a in answers.items()])\n",
    "\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"\"\"\n",
    "            You are an expert AI analyst. A user named {user_name} has answered a questionnaire about their data science skills.\n",
    "            Your task is to analyze their answers and generate a \"TUTORING STRATEGY\" report for our AI Tutor.\n",
    "\n",
    "            **User's Answers:**\n",
    "            {answers}\n",
    "\n",
    "            **Your Task:**\n",
    "            1.  Determine the user's knowledge level: 'Beginner', 'Intermediate', or 'Advanced'.\n",
    "            2.  Write a concise report following the correct strategy format below. This report will be given to another AI, so the instructions must be clear.\n",
    "\n",
    "            ---\n",
    "            **STRATEGY FORMATS (Choose ONE):**\n",
    "\n",
    "            **If 'Beginner':**\n",
    "            Start with `Knowledge Level: Beginner` on one line. On the next line, start with the exact phrase `TUTORING STRATEGY: The user is a beginner.` Then, explain that the tutor should use high-level concepts, explain the 'why' of GPU acceleration, and introduce NVIDIA libraries (like CuPy) as a simple, powerful alternative to what they already know.\n",
    "\n",
    "            **If 'Intermediate':**\n",
    "            Start with `Knowledge Level: Intermediate` on one line. On the next line, start with the exact phrase `TUTORING STRATEGY: The user is at an intermediate level.` Then, explain that the tutor should provide direct code comparisons (e.g., NumPy vs. CuPy), focus on performance benefits, and show clear benchmarking examples.\n",
    "\n",
    "            **If 'Advanced':**\n",
    "            Start with `Knowledge Level: Advanced` on one line. On the next line, start with the exact phrase `TUTORING STRATEGY: The user is advanced.` Then, explain that the tutor can provide nuanced advice, discuss the broader NVIDIA RAPIDS ecosystem, and cover specific benchmarking methodologies on the Sol supercomputer.\n",
    "            ---\n",
    "\n",
    "            Now, generate the complete report, including the \"Knowledge Level\" and the \"TUTORING STRATEGY\".\n",
    "            \"\"\",\n",
    "            input_variables=[\"user_name\", \"answers\"],\n",
    "        )\n",
    "\n",
    "        report_generation_chain = prompt | self.llm\n",
    "        response_message = report_generation_chain.invoke({\"user_name\": user_name, \"answers\": answers_str})\n",
    "        report_content = response_message.content\n",
    "        \n",
    "        return report_content\n",
    "\n",
    "    def _save_report(self, report: str, user_name: str):\n",
    "        \"\"\"\n",
    "        Saves the generated report to a text file.\n",
    "        \"\"\"\n",
    "        filename = \"_\".join(user_name.lower().split()) + \".txt\"\n",
    "        filepath = os.path.join(self.profiles_dir, filename)\n",
    "        \n",
    "        with open(filepath, \"w\") as f:\n",
    "            f.write(report)\n",
    "            \n",
    "        print(f\"\\nâœ… User profile report saved successfully to: {filepath}\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        The main method to run the agent's full workflow.\n",
    "        **Correction:** Asks for name first to ensure correct file naming.\n",
    "        \"\"\"\n",
    "        if not self.llm:\n",
    "            return\n",
    "\n",
    "        print(\"ðŸ¤– Hello! I'm your AI Accelerated Data Science Tutor.\")\n",
    "        user_name = input(\"1. To get started, what is your first and last name? \")\n",
    "\n",
    "        questions = self._generate_questions_with_llm()\n",
    "        answers = self._ask_dynamic_questions(questions)\n",
    "        report_content = self._generate_report_with_llm(user_name, answers)\n",
    "        \n",
    "        # Add the final header to the report\n",
    "        full_report = f\"--- User Profile for {user_name} ---\\n{report_content}\\n--- End of Profile ---\"\n",
    "        \n",
    "        print(\"\\n--- Generated Profile Report ---\")\n",
    "        print(full_report)\n",
    "        \n",
    "        self._save_report(full_report, user_name)\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == '__main__':\n",
    "    profiler_agent = UserProfilerAgent_V3()\n",
    "    profiler_agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7209a2-aec7-4348-8280-16f1770f6b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai25.06",
   "language": "python",
   "name": "genai25.06"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
