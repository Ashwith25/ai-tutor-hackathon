{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6768823-283d-4b68-8541-363114f0cc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: sbatch-based SolBenchmarker (CWD & source activate)\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import re\n",
    "\n",
    "class SolBenchmarker:\n",
    "    \"\"\"\n",
    "    An agent that submits a benchmark job via sbatch.\n",
    "\n",
    "    This version writes all scripts and log files to a 'benchmark_files'\n",
    "    folder within the current working directory and uses 'source activate'.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- ACTION REQUIRED ---\n",
    "    # You must change \"rapids-23.10\" to the actual RAPIDS environment name\n",
    "    # you found by running 'mamba info --envs' in the terminal.\n",
    "    def __init__(self, user: str, python_env: str = \"rapids25.02\"):\n",
    "        \"\"\"\n",
    "        Initializes the benchmarker.\n",
    "\n",
    "        Args:\n",
    "            user (str): The ASURITE username, used for monitoring jobs.\n",
    "            python_env (str): The mamba/conda environment to activate on Sol.\n",
    "        \"\"\"\n",
    "        if not user:\n",
    "            raise ValueError(\"ASURITE username is required to monitor SLURM jobs.\")\n",
    "        self.user = user\n",
    "        self.python_env = python_env\n",
    "\n",
    "    def _generate_sbatch_script(self, script_dir: str, cpu_script_name: str, gpu_script_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Generates the content of the sbatch script for the benchmark job.\n",
    "        This version uses 'source activate' as requested.\n",
    "        \"\"\"\n",
    "        sbatch_script_content = f\"\"\"#!/bin/bash\n",
    "#SBATCH -p general\n",
    "#SBATCH -q public\n",
    "#SBATCH -G 1\n",
    "#SBATCH -A grp_hackathon2025\n",
    "#SBATCH --reservation=hackathon2025\n",
    "#SBATCH -t 0-00:10:00\n",
    "#SBATCH -c 1\n",
    "#SBATCH -o {script_dir}/slurm-%j.out\n",
    "#SBATCH -e {script_dir}/slurm-%j.err\n",
    "\n",
    "# Load the necessary modules and environment\n",
    "module load mamba/latest\n",
    "source activate {self.python_env}\n",
    "\n",
    "# --- CPU BENCHMARK ---\n",
    "echo \"--- STARTING CPU BENCHMARK ---\"\n",
    "/usr/bin/time -p python3 {script_dir}/{cpu_script_name} 2>&1\n",
    "echo \"--- FINISHED CPU BENCHMARK ---\"\n",
    "\n",
    "echo \"\"\n",
    "# --- GPU BENCHMARK ---\n",
    "echo \"--- STARTING GPU BENCHMARK ---\"\n",
    "/usr/bin/time -p python3 {script_dir}/{gpu_script_name} 2>&1\n",
    "echo \"--- FINISHED GPU BENCHMARK ---\"\n",
    "\"\"\"\n",
    "        return sbatch_script_content\n",
    "\n",
    "    def _parse_output(self, output_content: str) -> dict:\n",
    "        \"\"\"\n",
    "        Parses the SLURM output file to find the benchmark times.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            real_times = re.findall(r\"real\\s+([\\d.]+)\", output_content)\n",
    "            cpu_time = float(real_times[0]) if len(real_times) > 0 else None\n",
    "            gpu_time = float(real_times[1]) if len(real_times) > 1 else None\n",
    "            cpu_output_match = re.search(r\"--- STARTING CPU BENCHMARK ---\\n(.*?)\\n--- FINISHED CPU BENCHMARK ---\", output_content, re.DOTALL)\n",
    "            gpu_output_match = re.search(r\"--- STARTING GPU BENCHMARK ---\\n(.*?)\\n--- FINISHED GPU BENCHMARK ---\", output_content, re.DOTALL)\n",
    "            cpu_output = cpu_output_match.group(1).strip() if cpu_output_match else \"Could not capture CPU script output.\"\n",
    "            gpu_output = gpu_output_match.group(1).strip() if gpu_output_match else \"Could not capture GPU script output.\"\n",
    "\n",
    "            return {\n",
    "                \"status\": \"success\", \"cpu_time_seconds\": cpu_time, \"gpu_time_seconds\": gpu_time,\n",
    "                \"cpu_script_output\": cpu_output, \"gpu_script_output\": gpu_output, \"raw_log\": output_content\n",
    "            }\n",
    "        except (IndexError, ValueError) as e:\n",
    "            return {\n",
    "                \"status\": \"error\", \"message\": f\"Failed to parse benchmark times. Error: {e}\", \"raw_log\": output_content\n",
    "            }\n",
    "\n",
    "    def run_benchmark(self, cpu_code: str, gpu_code: str) -> dict:\n",
    "        \"\"\"\n",
    "        The main method to orchestrate the benchmarking process via sbatch.\n",
    "        \"\"\"\n",
    "        benchmark_dir = os.path.join(os.getcwd(), \"benchmark_files\")\n",
    "        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "        \n",
    "        cpu_script_path = os.path.join(benchmark_dir, \"cpu_benchmark.py\")\n",
    "        gpu_script_path = os.path.join(benchmark_dir, \"gpu_benchmark.py\")\n",
    "        sbatch_path = os.path.join(benchmark_dir, \"benchmark_job.sh\")\n",
    "\n",
    "        try:\n",
    "            with open(cpu_script_path, \"w\") as f: f.write(cpu_code)\n",
    "            with open(gpu_script_path, \"w\") as f: f.write(gpu_code)\n",
    "            sbatch_script = self._generate_sbatch_script(benchmark_dir, \"cpu_benchmark.py\", \"gpu_benchmark.py\")\n",
    "            with open(sbatch_path, \"w\") as f: f.write(sbatch_script)\n",
    "\n",
    "            process = subprocess.run(f\"sbatch {sbatch_path}\", shell=True, capture_output=True, text=True)\n",
    "\n",
    "            if process.returncode != 0:\n",
    "                raise RuntimeError(f\"sbatch submission failed: {process.stderr}\")\n",
    "\n",
    "            job_id_match = re.search(r\"Submitted batch job (\\d+)\", process.stdout.strip())\n",
    "            if not job_id_match:\n",
    "                raise RuntimeError(f\"Could not parse Job ID from sbatch output: {process.stdout}\")\n",
    "            job_id = job_id_match.group(1)\n",
    "            print(f\"Successfully submitted job with ID: {job_id}\")\n",
    "\n",
    "            print(\"Waiting for job to complete...\")\n",
    "            while True:\n",
    "                queue_process = subprocess.run(f\"squeue -u {self.user} -j {job_id}\", shell=True, capture_output=True, text=True)\n",
    "                if job_id not in queue_process.stdout:\n",
    "                    break\n",
    "                time.sleep(10)\n",
    "\n",
    "            print(f\"Job {job_id} completed.\")\n",
    "            output_file_path = os.path.join(benchmark_dir, f\"slurm-{job_id}.out\")\n",
    "            if not os.path.exists(output_file_path):\n",
    "                err_file_path = os.path.join(benchmark_dir, f\"slurm-{job_id}.err\")\n",
    "                if os.path.exists(err_file_path):\n",
    "                    with open(err_file_path, \"r\") as f: error_content = f.read()\n",
    "                    return {\"status\": \"error\", \"message\": f\"Job failed. See error log: {error_content}\"}\n",
    "                return {\"status\": \"error\", \"message\": f\"Output file {output_file_path} not found.\"}\n",
    "\n",
    "            with open(output_file_path, \"r\") as f:\n",
    "                output_content = f.read()\n",
    "\n",
    "            return self._parse_output(output_content)\n",
    "        except Exception as e:\n",
    "            return {\"status\": \"error\", \"message\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "348adcf2-c3aa-4007-a032-8ddeae012260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted job with ID: 28522144\n",
      "Waiting for job to complete...\n",
      "Job 28522144 completed.\n",
      "\n",
      "--- Benchmark Results ---\n",
      "CPU Time: 505.8900 seconds\n",
      "GPU Time: 5.4100 seconds\n",
      "\n",
      "Speedup: 93.51x faster on GPU!\n",
      "\n",
      "CPU Script Output:\n",
      "CPU (NumPy) execution finished. Shape: (30000, 30000)\n",
      "real 505.89\n",
      "user 494.96\n",
      "sys 8.62\n",
      "\n",
      "GPU Script Output:\n",
      "GPU (CuPy) execution finished. Shape: (30000, 30000)\n",
      "real 5.41\n",
      "user 3.54\n",
      "sys 0.76\n",
      "\n",
      "--- Raw Log From SLURM ---\n",
      "--- STARTING CPU BENCHMARK ---\n",
      "CPU (NumPy) execution finished. Shape: (30000, 30000)\n",
      "real 505.89\n",
      "user 494.96\n",
      "sys 8.62\n",
      "--- FINISHED CPU BENCHMARK ---\n",
      "\n",
      "--- STARTING GPU BENCHMARK ---\n",
      "GPU (CuPy) execution finished. Shape: (30000, 30000)\n",
      "real 5.41\n",
      "user 3.54\n",
      "sys 0.76\n",
      "--- FINISHED GPU BENCHMARK ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Test Driver (with Larger Problem Size)\n",
    "\n",
    "# IMPORTANT: You must replace 'YOUR_ASURITE_USERNAME' with your actual username.\n",
    "ASURITE_USER = \"mrajanva\"\n",
    "\n",
    "if ASURITE_USER == \"YOUR_ASURITE_USERNAME\":\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    print(\"!!! PLEASE REPLACE 'YOUR_ASURITE_USERNAME' WITH YOUR ASURITE ID !!!\")\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "else:\n",
    "    # This section simulates Agent 3 generating the code to be benchmarked.\n",
    "    # We've increased the matrix size to demonstrate a real GPU speedup.\n",
    "    cpu_code_to_benchmark = \"\"\"\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Create two large matrices\n",
    "# The problem size is now much larger (12000*12000)\n",
    "size = 12000\n",
    "matrix_a = np.random.rand(size, size).astype(np.float32)\n",
    "matrix_b = np.random.rand(size, size).astype(np.float32)\n",
    "\n",
    "# Perform matrix multiplication\n",
    "result = np.dot(matrix_a, matrix_b)\n",
    "\n",
    "print(f\"CPU (NumPy) execution finished. Shape: {result.shape}\")\n",
    "\"\"\"\n",
    "\n",
    "    gpu_code_to_benchmark = \"\"\"\n",
    "import cupy as cp\n",
    "import time\n",
    "\n",
    "# Create two large matrices on the GPU\n",
    "# The problem size is now much larger (12000*12000)\n",
    "size = 12000\n",
    "matrix_a = cp.random.rand(size, size).astype(cp.float32)\n",
    "matrix_b = cp.random.rand(size, size).astype(cp.float32)\n",
    "cp.cuda.runtime.deviceSynchronize() # Wait for matrices to be created\n",
    "\n",
    "# Perform matrix multiplication\n",
    "result = cp.dot(matrix_a, matrix_b)\n",
    "cp.cuda.runtime.deviceSynchronize() # Wait for the computation to complete\n",
    "\n",
    "print(f\"GPU (CuPy) execution finished. Shape: {result.shape}\")\n",
    "\"\"\"\n",
    "\n",
    "    # Instantiate and run the benchmarker\n",
    "    # Ensure the SolBenchmarker class in Cell 1 is using the correct RAPIDS environment\n",
    "    benchmarker = SolBenchmarker(user=ASURITE_USER)\n",
    "    benchmark_results = benchmarker.run_benchmark(\n",
    "        cpu_code=cpu_code_to_benchmark,\n",
    "        gpu_code=gpu_code_to_benchmark\n",
    "    )\n",
    "\n",
    "    # This section formats the results and now includes better error handling.\n",
    "    print(\"\\n--- Benchmark Results ---\")\n",
    "    if benchmark_results.get(\"status\") == \"success\":\n",
    "        cpu_time = benchmark_results.get('cpu_time_seconds')\n",
    "        gpu_time = benchmark_results.get('gpu_time_seconds')\n",
    "\n",
    "        if cpu_time is not None:\n",
    "            print(f\"CPU Time: {cpu_time:.4f} seconds\")\n",
    "        else:\n",
    "            print(\"CPU Time: Could not be parsed.\")\n",
    "\n",
    "        if gpu_time is not None:\n",
    "            print(f\"GPU Time: {gpu_time:.4f} seconds\")\n",
    "        else:\n",
    "            print(\"GPU Time: Could not be parsed.\")\n",
    "\n",
    "        if cpu_time and gpu_time and gpu_time > 0:\n",
    "            speedup = cpu_time / gpu_time\n",
    "            print(f\"\\nSpeedup: {speedup:.2f}x faster on GPU!\")\n",
    "\n",
    "        print(\"\\nCPU Script Output:\")\n",
    "        print(benchmark_results.get('cpu_script_output'))\n",
    "        print(\"\\nGPU Script Output:\")\n",
    "        print(benchmark_results.get('gpu_script_output'))\n",
    "        \n",
    "        print(\"\\n--- Raw Log From SLURM ---\")\n",
    "        print(benchmark_results.get('raw_log', 'No raw log available.'))\n",
    "    else:\n",
    "        # This will catch errors from the agent itself\n",
    "        print(f\"An error occurred during the benchmark run: {benchmark_results.get('message')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025dab24-c0c2-4c9a-a1ce-af64ba4c17b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai25.06",
   "language": "python",
   "name": "genai25.06"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
