{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6768823-283d-4b68-8541-363114f0cc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import socket\n",
    "from typing import List, Dict\n",
    "import subprocess\n",
    "import time\n",
    "import re\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "# LangChain and related libraries\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "# Search library\n",
    "try:\n",
    "    from googlesearch import search\n",
    "except ImportError:\n",
    "    print(\"Error: 'googlesearch-python' is not installed. Please run 'pip install googlesearch-python'\")\n",
    "\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "348adcf2-c3aa-4007-a032-8ddeae012260",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolBenchmarker:\n",
    "    \"\"\"\n",
    "    An asynchronous agent that submits and monitors CPU and GPU benchmark\n",
    "    jobs in parallel on the ASU Sol supercomputer using multithreading.\n",
    "    \"\"\"\n",
    "    def __init__(self, user: str, python_env: str = \"rapids25.02\"):\n",
    "        if not user or user == \"YOUR_ASURITE_ID\":\n",
    "            raise ValueError(\"A valid ASURITE username is required for SolBenchmarker.\")\n",
    "        self.user = user\n",
    "        self.python_env = python_env\n",
    "\n",
    "    def _generate_cpu_sbatch(self, script_dir: str, script_name: str) -> str:\n",
    "        # This helper method remains unchanged.\n",
    "        return f\"\"\"#!/bin/bash\n",
    "#SBATCH -p general\n",
    "#SBATCH -q public\n",
    "#SBATCH -t 0-00:10:00\n",
    "#SBATCH -c 1\n",
    "#SBATCH -o {script_dir}/slurm_cpu-%j.out\n",
    "#SBATCH -e {script_dir}/slurm_cpu-%j.err\n",
    "\n",
    "module load mamba/latest\n",
    "source activate {self.python_env}\n",
    "/usr/bin/time -p python3 {script_dir}/{script_name} 2>&1\n",
    "\"\"\"\n",
    "\n",
    "    def _generate_gpu_sbatch(self, script_dir: str, script_name: str) -> str:\n",
    "        # This helper method remains unchanged.\n",
    "        return f\"\"\"#!/bin/bash\n",
    "#SBATCH -p general\n",
    "#SBATCH -q public\n",
    "#SBATCH -G 1\n",
    "#SBATCH -A grp_hackathon2025\n",
    "#SBATCH --reservation=hackathon2025\n",
    "#SBATCH -t 0-00:10:00\n",
    "#SBATCH -c 1\n",
    "#SBATCH -o {script_dir}/slurm_gpu-%j.out\n",
    "#SBATCH -e {script_dir}/slurm_gpu-%j.err\n",
    "\n",
    "module load mamba/latest\n",
    "source activate {self.python_env}\n",
    "/usr/bin/time -p python3 {script_dir}/{script_name} 2>&1\n",
    "\"\"\"\n",
    "\n",
    "    def _submit_job(self, sbatch_script: str) -> str:\n",
    "        # This helper method remains unchanged.\n",
    "        sbatch_path = os.path.join(os.getcwd(), \"benchmark_files\", f\"sbatch_job_{int(time.time())}_{os.getpid()}.sh\")\n",
    "        with open(sbatch_path, \"w\") as f:\n",
    "            f.write(sbatch_script)\n",
    "\n",
    "        process = subprocess.run(f\"sbatch {sbatch_path}\", shell=True, capture_output=True, text=True)\n",
    "        os.remove(sbatch_path)\n",
    "\n",
    "        if process.returncode != 0:\n",
    "            raise RuntimeError(f\"sbatch submission failed: {process.stderr}\")\n",
    "\n",
    "        job_id_match = re.search(r\"Submitted batch job (\\d+)\", process.stdout.strip())\n",
    "        if not job_id_match:\n",
    "            raise RuntimeError(f\"Could not parse Job ID from sbatch output: {process.stdout}\")\n",
    "\n",
    "        return job_id_match.group(1)\n",
    "\n",
    "    def _monitor_job(self, job_id: str, job_type: str, benchmark_dir: str, output_queue: queue.Queue):\n",
    "        \"\"\"\n",
    "        Monitors a single SLURM job in a separate thread.\n",
    "        Polls squeue and, upon completion, parses the output and puts the result on a queue.\n",
    "        This function runs in a separate thread for each job (CPU/GPU).\n",
    "        \"\"\"\n",
    "        print(f\"--> [Thread-{job_type}] Started monitoring Job {job_id}.\")\n",
    "        try:\n",
    "            while True:\n",
    "                # Check if the job is still in the queue\n",
    "                squeue_cmd = f\"squeue -u {self.user} -j {job_id}\"\n",
    "                process = subprocess.run(squeue_cmd, shell=True, capture_output=True, text=True)\n",
    "\n",
    "                if job_id not in process.stdout:\n",
    "                    # Job is no longer in the queue, it has finished.\n",
    "                    output_queue.put(\n",
    "                        (\"status\", f\"--> [Agent 4] Job `{job_id}` ({job_type}) completed. Parsing results...\")\n",
    "                    )\n",
    "\n",
    "                    # Wait a brief moment for the filesystem to catch up and write the log file.\n",
    "                    time.sleep(2)\n",
    "\n",
    "                    log_prefix = \"slurm_cpu\" if job_type == \"CPU\" else \"slurm_gpu\"\n",
    "                    output_file_path = os.path.join(benchmark_dir, f\"{log_prefix}-{job_id}.out\")\n",
    "\n",
    "                    if os.path.exists(output_file_path):\n",
    "                        with open(output_file_path, \"r\") as f:\n",
    "                            content = f.read()\n",
    "\n",
    "                        time_match = re.search(r\"real\\s+([\\d.]+)\", content)\n",
    "                        exec_time = float(time_match.group(1)) if time_match else None\n",
    "                        result = {\"type\": job_type, \"time\": exec_time, \"log\": content}\n",
    "                    else:\n",
    "                        result = {\"type\": job_type, \"time\": None, \"log\": f\"Error: Log file not found at {output_file_path}\"}\n",
    "\n",
    "                    output_queue.put((\"result\", result))\n",
    "                    print(f\"--> [Thread-{job_type}] Finished monitoring Job {job_id}.\")\n",
    "                    break  # Exit the monitoring loop for this thread\n",
    "\n",
    "                # Wait before polling again\n",
    "                time.sleep(5)\n",
    "        except Exception as e:\n",
    "            error_result = {\"type\": job_type, \"time\": None, \"log\": f\"An error occurred in the monitoring thread: {e}\"}\n",
    "            output_queue.put((\"result\", error_result))\n",
    "            print(f\"--> [Thread-{job_type}] Error monitoring Job {job_id}: {e}\")\n",
    "\n",
    "\n",
    "    def run_benchmark_async(self, cpu_code: str, gpu_code: str):\n",
    "        \"\"\"\n",
    "        Submits CPU and GPU jobs and yields their results as they complete.\n",
    "        This is the main generator function. It uses separate threads to monitor\n",
    "        each job independently, ensuring results are streamed to the UI immediately.\n",
    "        \"\"\"\n",
    "        benchmark_dir = os.path.join(os.getcwd(), \"benchmark_files\")\n",
    "        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "\n",
    "        # Write the CPU and GPU scripts\n",
    "        cpu_script_path = os.path.join(benchmark_dir, \"cpu_benchmark.py\")\n",
    "        gpu_script_path = os.path.join(benchmark_dir, \"gpu_benchmark.py\")\n",
    "        with open(cpu_script_path, \"w\") as f: f.write(cpu_code)\n",
    "        with open(gpu_script_path, \"w\") as f: f.write(gpu_code)\n",
    "\n",
    "        try:\n",
    "            # Submit CPU and GPU jobs\n",
    "            cpu_job_id = self._submit_job(self._generate_cpu_sbatch(benchmark_dir, \"cpu_benchmark.py\"))\n",
    "            gpu_job_id = self._submit_job(self._generate_gpu_sbatch(benchmark_dir, \"gpu_benchmark.py\"))\n",
    "\n",
    "            yield f\"--> [Agent 4] Submitted CPU Job `{cpu_job_id}` and GPU Job `{gpu_job_id}`. Monitoring in parallel...\"\n",
    "\n",
    "            # A thread-safe queue to hold results from monitor threads\n",
    "            results_queue = queue.Queue()\n",
    "\n",
    "            # Create and start monitoring threads\n",
    "            cpu_monitor_thread = threading.Thread(\n",
    "                target=self._monitor_job,\n",
    "                args=(cpu_job_id, \"CPU\", benchmark_dir, results_queue),\n",
    "                daemon=True\n",
    "            )\n",
    "            gpu_monitor_thread = threading.Thread(\n",
    "                target=self._monitor_job,\n",
    "                args=(gpu_job_id, \"GPU\", benchmark_dir, results_queue),\n",
    "                daemon=True\n",
    "            )\n",
    "\n",
    "            cpu_monitor_thread.start()\n",
    "            gpu_monitor_thread.start()\n",
    "\n",
    "            # Expect 4 items total: 2 status updates and 2 final results\n",
    "            completed_items = 0\n",
    "            while completed_items < 4:\n",
    "                try:\n",
    "                    # Block and wait for any result to be put on the queue\n",
    "                    event_type, data = results_queue.get(timeout=300)  # 5-minute timeout\n",
    "                    yield data  # Yield the status string OR the result dictionary\n",
    "                    completed_items += 1\n",
    "                except queue.Empty:\n",
    "                    yield \"--> [Agent 4] Benchmark timed out waiting for a result.\"\n",
    "                    break # Exit if no result in 5 mins\n",
    "\n",
    "        except Exception as e:\n",
    "            yield f\"Error in benchmark agent setup: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "025dab24-c0c2-4c9a-a1ce-af64ba4c17b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchQueryGenerator(BaseModel):\n",
    "    queries: List[str] = Field(description=\"A list of targeted, keyword-focused search queries.\")\n",
    "\n",
    "def generate_search_queries(query: str, llm) -> List[str]:\n",
    "    # This function remains unchanged.\n",
    "    print(\"-> Using LLM with FULLY ABSTRACTED prompt to generate search queries...\")\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are an expert at generating web search queries for a technical audience.\n",
    "        Analyze the user's question to identify the core technical task and the programming language.\n",
    "        Based on your knowledge, generate 5 concise, targeted search queries. Two queries should be for the standard, CPU-based library for that task/language. Three queries should be for potential GPU-accelerated libraries for that task/language, prioritizing NVIDIA-based solutions if they exist.\n",
    "\n",
    "        User Question: \"{question}\"\n",
    "\n",
    "        Generate a JSON list of 5 search query strings.\n",
    "        \"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "    query_generation_chain = prompt_template | llm.with_structured_output(SearchQueryGenerator)\n",
    "    try:\n",
    "        response_model = query_generation_chain.invoke({\"question\": query})\n",
    "        print(f\"-> Generated queries: {response_model.queries}\")\n",
    "        return response_model.queries\n",
    "    except Exception as e:\n",
    "        print(f\"-> LLM failed to generate structured output: {e}\")\n",
    "        return []\n",
    "\n",
    "def dynamic_search_agentic(queries: List[str]) -> list[str]:\n",
    "    # This function remains unchanged.\n",
    "    print(\"-> Executing dynamic search...\")\n",
    "    all_urls = set()\n",
    "    for q in queries:\n",
    "        try:\n",
    "            enhanced_query = f\"{q} site:developer.nvidia.com OR site:medium.com/rapids-ai OR site:medium.com/cupy-team\"\n",
    "            search_results = list(search(enhanced_query, num_results=2))\n",
    "            for url in search_results:\n",
    "                all_urls.add(url)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during search for query '{q}': {e}\")\n",
    "            continue\n",
    "    final_urls = [url for url in list(all_urls) if url]\n",
    "    print(f\"-> Found {len(final_urls)} unique URLs: {final_urls}\")\n",
    "    return final_urls\n",
    "\n",
    "def _extract_python_code(markdown_text: str) -> Dict[str, str]:\n",
    "    # This helper function remains unchanged.\n",
    "    code_pattern = r\"```python\\n(.*?)\\n```\"\n",
    "    gpu_heading_pattern = r\"### Recommended GPU Solution.*?\\n\"\n",
    "    cpu_heading_pattern = r\"### Standard CPU Solution.*?\\n\"\n",
    "    gpu_section_match = re.search(gpu_heading_pattern, markdown_text, re.DOTALL | re.IGNORECASE)\n",
    "    cpu_section_match = re.search(cpu_heading_pattern, markdown_text, re.DOTALL | re.IGNORECASE)\n",
    "    gpu_code = \"\"\n",
    "    cpu_code = \"\"\n",
    "    if gpu_section_match:\n",
    "        section_start = gpu_section_match.end()\n",
    "        code_match = re.search(code_pattern, markdown_text[section_start:], re.DOTALL)\n",
    "        if code_match: gpu_code = code_match.group(1).strip()\n",
    "    if cpu_section_match:\n",
    "        section_start = cpu_section_match.end()\n",
    "        code_match = re.search(code_pattern, markdown_text[section_start:], re.DOTALL)\n",
    "        if code_match: cpu_code = code_match.group(1).strip()\n",
    "    return {\"cpu_code\": cpu_code, \"gpu_code\": gpu_code}\n",
    "\n",
    "def process_with_rag(query: str):\n",
    "    # This is the main RAG pipeline, which is a generator function\n",
    "    print(\"--- Running FINAL DYNAMIC RAG Pipeline ---\")\n",
    "    host_node = socket.gethostname()\n",
    "    # Ensure you are using the correct user and port for your Ollama instance\n",
    "    llm = ChatOllama(model=\"qwen3:14b\", base_url=f\"http://mrajanva@{host_node}:11434/\")\n",
    "\n",
    "    search_queries = generate_search_queries(query, llm)\n",
    "    urls = dynamic_search_agentic(search_queries) if search_queries else []\n",
    "\n",
    "    context_text = \"\"\n",
    "    if urls:\n",
    "        print(\"-> Found documents. Loading and processing context...\")\n",
    "        docs = [WebBaseLoader(url).load() for url in urls]\n",
    "        docs_list = [item for sublist in docs for item in sublist]\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500, chunk_overlap=100)\n",
    "        doc_splits = text_splitter.split_documents(docs_list)\n",
    "        embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        vectorstore = Chroma.from_documents(documents=doc_splits, embedding=embedding_model, collection_name=\"rag-chroma\")\n",
    "        retriever = vectorstore.as_retriever()\n",
    "        retrieved_docs = retriever.invoke(query)\n",
    "        context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "        vectorstore.delete_collection()\n",
    "\n",
    "    final_prompt_template = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are a friendly and knowledgeable AI Tutor for a project focused on data science acceleration. Your primary mission is to educate users on leveraging NVIDIA-based GPU libraries.\n",
    "        First, analyze the user's QUESTION to identify the core task and programming language. Based on your knowledge, determine if a common NVIDIA-based GPU-accelerated library (like cuPy, cuDF, Rapids) exists for that specific task and language.\n",
    "        Then, follow the appropriate path below to structure your conversational and helpful answer:\n",
    "\n",
    "        **PATH 1: An NVIDIA-based GPU-accelerated library EXISTS for this task.**\n",
    "        1.  Identify the standard CPU library and the NVIDIA GPU library for the user's language and task.\n",
    "        2.  Start with a friendly opening that explains you will show both the GPU-accelerated and standard methods.\n",
    "        3.  Provide a heading for the GPU solution, dynamically inserting the library name (e.g., `### Recommended GPU Solution (with [GPU Library Name])`).\n",
    "        4.  Write the code example for the GPU solution.\n",
    "        5.  Add a \"Performance Note\" section. Explain the benefits of the GPU approach.\n",
    "        6.  Provide a heading for the CPU solution, dynamically inserting the library name (e.g., `### Standard CPU Solution (with [CPU Library Name])`).\n",
    "        7.  Write the CPU-based code for comparison.\n",
    "        8.  Do NOT add any disclaimer note at the end.\n",
    "\n",
    "        **PATH 2: An NVIDIA-based GPU-accelerated library DOES NOT EXIST for this task.**\n",
    "        1.  Identify the standard library for the user's specified language and task.\n",
    "        2.  Start with a friendly opening explaining the standard approach.\n",
    "        3.  Provide a heading for the standard solution, dynamically inserting the library name (e.g., `### Standard Solution (with [Library Name])`).\n",
    "        4.  Write the code example using the identified standard library.\n",
    "        5.  End your entire response with the exact sentence: \"Note: The provided solution is the standard method for this task, as a direct NVIDIA-based GPU library for it is not common.\"\n",
    "\n",
    "        Use the CONTEXT below to inform your answer if it is relevant, but your primary instruction is to follow the mission and logic paths described above.\n",
    "\n",
    "        CONTEXT:\n",
    "        {context}\n",
    "\n",
    "        QUESTION:\n",
    "        {question}\n",
    "\n",
    "        YOUR FINAL ANSWER:\n",
    "        \"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    final_chain = final_prompt_template | llm\n",
    "    llm_response_text = final_chain.invoke({\"context\": context_text, \"question\": query}).content\n",
    "\n",
    "    # 1. Immediately yield the main answer from the LLM\n",
    "    print(\"SENDING GENERATED RESPONSE\")\n",
    "    yield llm_response_text\n",
    "\n",
    "    extracted_code = _extract_python_code(llm_response_text)\n",
    "    cpu_code = extracted_code.get(\"cpu_code\")\n",
    "    gpu_code = extracted_code.get(\"gpu_code\")\n",
    "    asurite_id = \"mrajanva\" # IMPORTANT: Make sure this is your ASURITE ID\n",
    "\n",
    "    if cpu_code and gpu_code:\n",
    "        print(\"--> [Agent 3] Both CPU and GPU code found. Invoking Agent 4 for benchmarking.\")\n",
    "        try:\n",
    "            benchmarker = SolBenchmarker(user=asurite_id)\n",
    "            benchmark_generator = benchmarker.run_benchmark_async(cpu_code, gpu_code)\n",
    "\n",
    "            # --- MODIFIED SECTION: Handle the real-time stream from the benchmarker ---\n",
    "            # Loop through the benchmark generator and yield each result or status update immediately.\n",
    "            for item in benchmark_generator:\n",
    "                if isinstance(item, str):\n",
    "                    # This is a status update string. Yield it directly to the UI.\n",
    "                    yield f\"\\n\\n_{item}_\"\n",
    "                elif isinstance(item, dict):\n",
    "                    # This is a completed job result. Format it as Markdown and yield it.\n",
    "                    job_type = item.get(\"type\")\n",
    "                    exec_time = item.get(\"time\")\n",
    "                    time_str = f\"{exec_time:.4f} seconds\" if exec_time is not None else \"N/A (script failed)\"\n",
    "                    benchmark_md_chunk = f\"\\n\\n---\\n### âœ… {job_type} Benchmark Complete\\n\"\n",
    "                    benchmark_md_chunk += f\"**Execution Time:** {time_str}\\n\"\n",
    "                    yield benchmark_md_chunk\n",
    "            # --- END OF MODIFIED SECTION ---\n",
    "\n",
    "        except ValueError as e:\n",
    "            yield f\"\\n\\n---\\n### âš ï¸ Benchmark Skipped\\nConfiguration error: {e}\"\n",
    "    else:\n",
    "        print(\"--> [Agent 3] Did not find both CPU and GPU code. Skipping benchmark.\")\n",
    "\n",
    "    print(\"--- Pipeline Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a27266-a837-4f36-b272-9fda1df96818",
   "metadata": {},
   "source": [
    "# Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56a9249f-f5e4-499d-bd40-de476667e86b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+00A0 (1593259314.py, line 29)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m# This part remains mostly the same\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid non-printable character U+00A0\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "\n",
    "# GRADIO CHAT APPLICATION FOR AI TUTOR (FINAL)\n",
    "\n",
    "#\n",
    "\n",
    "# This version fixes the UserWarning by specifying the modern 'messages'\n",
    "\n",
    "# format for the chatbot component.\n",
    "\n",
    "# =================================================================\n",
    "\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "import re\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# This wrapper function connects our backend logic to the Gradio UI.\n",
    "\n",
    "# It assumes 'process_with_rag' is defined and available in the notebook environment.\n",
    "\n",
    "def tutor_chat_interface(user_message, history):\n",
    "\n",
    "Â  Â  # This part remains mostly the same\n",
    "\n",
    "Â  Â  history.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "Â  Â  history.append({\"role\": \"assistant\", \"content\": \"\"}) # Add an empty placeholder for the bot\n",
    "\n",
    "Â  Â  yield history, \"Thinking...\"\n",
    "\n",
    "\n",
    "\n",
    "Â  Â  # The RAG pipeline is now a generator\n",
    "\n",
    "Â  Â  response_generator = process_with_rag(user_message)\n",
    "\n",
    "\n",
    "\n",
    "Â  Â  # --- MODIFIED: Loop through the stream ---\n",
    "\n",
    "Â  Â  full_response_text = \"\"\n",
    "\n",
    "Â  Â  for chunk in response_generator:\n",
    "\n",
    "Â  Â  Â  Â  # Append each new piece of text to the full response\n",
    "\n",
    "Â  Â  Â  Â  full_response_text += chunk\n",
    "\n",
    "Â  Â  Â  Â  # Update the last message in the history with the new combined text\n",
    "\n",
    "Â  Â  Â  Â  history[-1][\"content\"] = full_response_text\n",
    "\n",
    "Â  Â  Â  Â  yield history, \"Agent is working...\" # Update the chatbot with the streaming text\n",
    "\n",
    "\n",
    "\n",
    "# Build the Gradio UI using Blocks for more control\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft(), css=\".gradio-container {background-color: #f5f5f5;}\") as demo:\n",
    "\n",
    "Â  Â  gr.Markdown(\"# ðŸ¤– AI Accelerated Data Science Tutor\")\n",
    "\n",
    "Â  Â  gr.Markdown(\"Ask a question about a data science task. The tutor will provide an explanation and code, prioritizing NVIDIA GPU-accelerated solutions where possible.\")\n",
    "\n",
    "\n",
    "\n",
    "Â  Â  # THE FIX IS HERE: Added 'type=\"messages\"' to the chatbot component.\n",
    "\n",
    "Â  Â  chatbot = gr.Chatbot(label=\"Conversation\", height=450, bubble_full_width=False, type=\"messages\")\n",
    "\n",
    "Â  Â Â \n",
    "\n",
    "Â  Â  with gr.Accordion(\"ðŸ”Ž Show Agent's Thought Process\", open=False):\n",
    "\n",
    "Â  Â  Â  Â  cot_output = gr.Markdown(\"The agent's reasoning will appear here after it responds.\")\n",
    "\n",
    "\n",
    "\n",
    "Â  Â  with gr.Row():\n",
    "\n",
    "Â  Â  Â  Â  msg_textbox = gr.Textbox(\n",
    "\n",
    "Â  Â  Â  Â  Â  Â  label=\"Your Question\",\n",
    "\n",
    "Â  Â  Â  Â  Â  Â  placeholder=\"e.g., How do I multiply two 10x10 arrays in Python?\",\n",
    "\n",
    "Â  Â  Â  Â  Â  Â  scale=4,\n",
    "\n",
    "Â  Â  Â  Â  Â  Â  autofocus=True,\n",
    "\n",
    "Â  Â  Â  Â  Â  Â  container=False # This makes the textbox look cleaner\n",
    "\n",
    "Â  Â  Â  Â  )\n",
    "\n",
    "Â  Â  Â  Â  submit_btn = gr.Button(\"Ask\", variant=\"primary\", scale=1, min_width=150)\n",
    "\n",
    "\n",
    "\n",
    "Â  Â  # Main function to handle the chat logic\n",
    "\n",
    "Â  Â  def handle_submit(user_message, chat_history):\n",
    "\n",
    "Â  Â  Â  Â  response_generator = tutor_chat_interface(user_message, chat_history)\n",
    "\n",
    "Â  Â  Â  Â  for history_state, thought_process in response_generator:\n",
    "\n",
    "Â  Â  Â  Â  Â  Â  yield history_state, thought_process\n",
    "\n",
    "\n",
    "\n",
    "Â  Â  # Connect the submit button and textbox to the handler function\n",
    "\n",
    "Â  Â  submit_btn.click(\n",
    "\n",
    "Â  Â  Â  Â  handle_submit,Â \n",
    "\n",
    "Â  Â  Â  Â  [msg_textbox, chatbot],Â \n",
    "\n",
    "Â  Â  Â  Â  [chatbot, cot_output]\n",
    "\n",
    "Â  Â  ).then(lambda: gr.update(value=\"\"), None, [msg_textbox], queue=False)\n",
    "\n",
    "\n",
    "\n",
    "Â  Â  msg_textbox.submit(\n",
    "\n",
    "Â  Â  Â  Â  handle_submit,Â \n",
    "\n",
    "Â  Â  Â  Â  [msg_textbox, chatbot],Â \n",
    "\n",
    "Â  Â  Â  Â  [chatbot, cot_output]\n",
    "\n",
    "Â  Â  ).then(lambda: gr.update(value=\"\"), None, [msg_textbox], queue=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Launch the application\n",
    "\n",
    "# Set share=True if you need a public link from the Sol jupyter notebook\n",
    "demo.launch(share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai25.06",
   "language": "python",
   "name": "genai25.06"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
