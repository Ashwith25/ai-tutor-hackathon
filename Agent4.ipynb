{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "336e5488-4a33-45de-9083-7639f1fe3916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: The sbatch-based SolBenchmarker Agent Definition (Corrected Parsing)\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import re\n",
    "\n",
    "class SolBenchmarker:\n",
    "    \"\"\"\n",
    "    An agent that dynamically creates and executes benchmark scripts on the\n",
    "    ASU Sol supercomputer by submitting a job via sbatch.\n",
    "\n",
    "    This version writes all scripts and log files to the current working directory.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- ACTION REQUIRED ---\n",
    "    # You must change \"rapids-23.10\" to the actual RAPIDS environment name\n",
    "    # you found by running 'mamba info --envs' in the terminal.\n",
    "    def __init__(self, user: str, python_env: str = \"rapids-23.10\"):\n",
    "        \"\"\"\n",
    "        Initializes the benchmarker.\n",
    "\n",
    "        Args:\n",
    "            user (str): The ASURITE username, used for monitoring jobs.\n",
    "            python_env (str): The mamba/conda environment to activate on Sol.\n",
    "        \"\"\"\n",
    "        if not user:\n",
    "            raise ValueError(\"ASURITE username is required to monitor SLURM jobs.\")\n",
    "        self.user = user\n",
    "        self.python_env = python_env\n",
    "\n",
    "    def _generate_sbatch_script(self, script_dir: str, cpu_script_name: str, gpu_script_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Generates the content of the sbatch script for the benchmark job.\n",
    "        All paths are relative to the script's execution directory.\n",
    "        \"\"\"\n",
    "        sbatch_script_content = f\"\"\"#!/bin/bash\n",
    "#SBATCH -p htc\n",
    "#SBATCH -q public\n",
    "#SBATCH -G 1\n",
    "#SBATCH -t 0-00:10:00\n",
    "#SBATCH -c 1\n",
    "#SBATCH -o {script_dir}/slurm-%j.out\n",
    "#SBATCH -e {script_dir}/slurm-%j.err\n",
    "\n",
    "# --- CPU BENCHMARK ---\n",
    "echo \"--- STARTING CPU BENCHMARK ---\"\n",
    "/usr/bin/time -p bash -c 'module load mamba/latest && mamba run -n {self.python_env} python3 {script_dir}/{cpu_script_name}'\n",
    "echo \"--- FINISHED CPU BENCHMARK ---\"\n",
    "\n",
    "echo \"\"\n",
    "# --- GPU BENCHMARK ---\n",
    "echo \"--- STARTING GPU BENCHMARK ---\"\n",
    "/usr/bin/time -p bash -c 'module load mamba/latest && mamba run -n {self.python_env} python3 {script_dir}/{gpu_script_name}'\n",
    "echo \"--- FINISHED GPU BENCHMARK ---\"\n",
    "\"\"\"\n",
    "        return sbatch_script_content\n",
    "\n",
    "    def _parse_output(self, output_content: str) -> dict:\n",
    "        \"\"\"\n",
    "        Parses the SLURM output file to find the benchmark times.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            real_times = re.findall(r\"real\\s+([\\d.]+)\", output_content)\n",
    "            cpu_time = float(real_times[0]) if len(real_times) > 0 else None\n",
    "            gpu_time = float(real_times[1]) if len(real_times) > 1 else None\n",
    "            cpu_output_match = re.search(r\"--- STARTING CPU BENCHMARK ---\\n(.*?)\\n--- FINISHED CPU BENCHMARK ---\", output_content, re.DOTALL)\n",
    "            gpu_output_match = re.search(r\"--- STARTING GPU BENCHMARK ---\\n(.*?)\\n--- FINISHED GPU BENCHMARK ---\", output_content, re.DOTALL)\n",
    "            cpu_output = cpu_output_match.group(1).strip() if cpu_output_match else \"Could not capture CPU script output.\"\n",
    "            gpu_output = gpu_output_match.group(1).strip() if gpu_output_match else \"Could not capture GPU script output.\"\n",
    "\n",
    "            return {\n",
    "                \"status\": \"success\", \"cpu_time_seconds\": cpu_time, \"gpu_time_seconds\": gpu_time,\n",
    "                \"cpu_script_output\": cpu_output, \"gpu_script_output\": gpu_output, \"raw_log\": output_content\n",
    "            }\n",
    "        except (IndexError, ValueError) as e:\n",
    "            return {\n",
    "                \"status\": \"error\", \"message\": f\"Failed to parse benchmark times. Error: {e}\", \"raw_log\": output_content\n",
    "            }\n",
    "\n",
    "    def run_benchmark(self, cpu_code: str, gpu_code: str) -> dict:\n",
    "        \"\"\"\n",
    "        The main method to orchestrate the benchmarking process via sbatch.\n",
    "        \"\"\"\n",
    "        script_dir = os.getcwd()\n",
    "        cpu_script_path = os.path.join(script_dir, \"cpu_benchmark.py\")\n",
    "        gpu_script_path = os.path.join(script_dir, \"gpu_benchmark.py\")\n",
    "        sbatch_path = os.path.join(script_dir, \"benchmark_job.sh\")\n",
    "\n",
    "        try:\n",
    "            with open(cpu_script_path, \"w\") as f: f.write(cpu_code)\n",
    "            with open(gpu_script_path, \"w\") as f: f.write(gpu_code)\n",
    "            sbatch_script = self._generate_sbatch_script(script_dir, \"cpu_benchmark.py\", \"gpu_benchmark.py\")\n",
    "            with open(sbatch_path, \"w\") as f: f.write(sbatch_script)\n",
    "\n",
    "            process = subprocess.run(f\"sbatch {sbatch_path}\", shell=True, capture_output=True, text=True)\n",
    "\n",
    "            if process.returncode != 0:\n",
    "                raise RuntimeError(f\"sbatch submission failed: {process.stderr}\")\n",
    "\n",
    "            # --- THE FIX IS HERE ---\n",
    "            # We strip whitespace from the output before parsing to handle hidden characters.\n",
    "            job_id_match = re.search(r\"Submitted batch job (\\d+)\", process.stdout.strip())\n",
    "            \n",
    "            if not job_id_match:\n",
    "                raise RuntimeError(f\"Could not parse Job ID from sbatch output: {process.stdout}\")\n",
    "            job_id = job_id_match.group(1)\n",
    "            print(f\"Successfully submitted job with ID: {job_id}\")\n",
    "\n",
    "            print(\"Waiting for job to complete...\")\n",
    "            while True:\n",
    "                queue_process = subprocess.run(f\"squeue -u {self.user} -j {job_id}\", shell=True, capture_output=True, text=True)\n",
    "                if job_id not in queue_process.stdout:\n",
    "                    break\n",
    "                time.sleep(10)\n",
    "\n",
    "            print(f\"Job {job_id} completed.\")\n",
    "            output_file_path = os.path.join(script_dir, f\"slurm-{job_id}.out\")\n",
    "            if not os.path.exists(output_file_path):\n",
    "                return {\"status\": \"error\", \"message\": f\"Output file {output_file_path} not found.\"}\n",
    "\n",
    "            with open(output_file_path, \"r\") as f:\n",
    "                output_content = f.read()\n",
    "\n",
    "            return self._parse_output(output_content)\n",
    "        except Exception as e:\n",
    "            return {\"status\": \"error\", \"message\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9251909a-9d10-4b93-bba1-f586094f3eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted job with ID: 28521008\n",
      "Waiting for job to complete...\n",
      "Job 28521008 completed.\n",
      "\n",
      "--- Benchmark Results ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to NoneType.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     54\u001b[39m cpu_time = benchmark_results.get(\u001b[33m'\u001b[39m\u001b[33mcpu_time_seconds\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     55\u001b[39m gpu_time = benchmark_results.get(\u001b[33m'\u001b[39m\u001b[33mgpu_time_seconds\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCPU Time: \u001b[39m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcpu_time\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[33;43m.4f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGPU Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgpu_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cpu_time \u001b[38;5;129;01mand\u001b[39;00m gpu_time \u001b[38;5;129;01mand\u001b[39;00m gpu_time > \u001b[32m0\u001b[39m:\n",
      "\u001b[31mTypeError\u001b[39m: unsupported format string passed to NoneType.__format__"
     ]
    }
   ],
   "source": [
    "# Cell 2: Test Driver\n",
    "\n",
    "# IMPORTANT: You must replace 'YOUR_ASURITE_USERNAME' with your actual username.\n",
    "ASURITE_USER = \"mrajanva\"\n",
    "\n",
    "if ASURITE_USER == \"YOUR_ASURITE_USERNAME\":\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    print(\"!!! PLEASE REPLACE 'YOUR_ASURITE_USERNAME' WITH YOUR ASURITE ID !!!\")\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "else:\n",
    "    # This section simulates Agent 3 generating the code to be benchmarked.\n",
    "    cpu_code_to_benchmark = \"\"\"\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Create two large matrices\n",
    "size = 4000\n",
    "matrix_a = np.random.rand(size, size).astype(np.float32)\n",
    "matrix_b = np.random.rand(size, size).astype(np.float32)\n",
    "\n",
    "# Perform matrix multiplication\n",
    "result = np.dot(matrix_a, matrix_b)\n",
    "\n",
    "print(f\"CPU (NumPy) execution finished. Shape: {result.shape}\")\n",
    "\"\"\"\n",
    "\n",
    "    gpu_code_to_benchmark = \"\"\"\n",
    "import cupy as cp\n",
    "import time\n",
    "\n",
    "# Create two large matrices on the GPU\n",
    "size = 4000\n",
    "matrix_a = cp.random.rand(size, size).astype(cp.float32)\n",
    "matrix_b = cp.random.rand(size, size).astype(cp.float32)\n",
    "cp.cuda.runtime.deviceSynchronize() # Wait for matrices to be created\n",
    "\n",
    "# Perform matrix multiplication\n",
    "result = cp.dot(matrix_a, matrix_b)\n",
    "cp.cuda.runtime.deviceSynchronize() # Wait for the computation to complete\n",
    "\n",
    "print(f\"GPU (CuPy) execution finished. Shape: {result.shape}\")\n",
    "\"\"\"\n",
    "\n",
    "    # Instantiate and run the benchmarker\n",
    "    benchmarker = SolBenchmarker(user=ASURITE_USER)\n",
    "    benchmark_results = benchmarker.run_benchmark(\n",
    "        cpu_code=cpu_code_to_benchmark,\n",
    "        gpu_code=gpu_code_to_benchmark\n",
    "    )\n",
    "\n",
    "    # This section formats the results, just as Agent 3 would.\n",
    "    print(\"\\n--- Benchmark Results ---\")\n",
    "    if benchmark_results.get(\"status\") == \"success\":\n",
    "        cpu_time = benchmark_results.get('cpu_time_seconds')\n",
    "        gpu_time = benchmark_results.get('gpu_time_seconds')\n",
    "        print(f\"CPU Time: {cpu_time:.4f} seconds\")\n",
    "        print(f\"GPU Time: {gpu_time:.4f} seconds\")\n",
    "\n",
    "        if cpu_time and gpu_time and gpu_time > 0:\n",
    "            speedup = cpu_time / gpu_time\n",
    "            print(f\"\\nSpeedup: {speedup:.2f}x faster on GPU!\")\n",
    "\n",
    "        print(\"\\nCPU Script Output:\")\n",
    "        print(benchmark_results.get('cpu_script_output'))\n",
    "        print(\"\\nGPU Script Output:\")\n",
    "        print(benchmark_results.get('gpu_script_output'))\n",
    "        print(\"\\n--- Raw Log ---\")\n",
    "        print(benchmark_results.get('raw_log', 'No raw log available.'))\n",
    "    else:\n",
    "        print(f\"An error occurred: {benchmark_results.get('message')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e7474-24ca-4279-812f-da080b612f04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai25.06",
   "language": "python",
   "name": "genai25.06"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
