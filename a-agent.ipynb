{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3401a3d5-8099-4d4b-b5b1-a9f091af5a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: googlesearch-python in /home/kvinod/.local/lib/python3.12/site-packages (1.3.0)\n",
      "Requirement already satisfied: lxml in /home/kvinod/.local/lib/python3.12/site-packages (5.4.0)\n",
      "Requirement already satisfied: deepspeed in /home/kvinod/.local/lib/python3.12/site-packages (0.16.8+ee492c30)\n",
      "Requirement already satisfied: sentence-transformers in /home/kvinod/.local/lib/python3.12/site-packages (4.1.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9 in /home/kvinod/.local/lib/python3.12/site-packages (from googlesearch-python) (4.13.4)\n",
      "Requirement already satisfied: requests>=2.20 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from googlesearch-python) (2.32.3)\n",
      "Requirement already satisfied: einops in /home/kvinod/.local/lib/python3.12/site-packages (from deepspeed) (0.8.1)\n",
      "Requirement already satisfied: hjson in /home/kvinod/.local/lib/python3.12/site-packages (from deepspeed) (3.1.0)\n",
      "Requirement already satisfied: msgpack in /home/kvinod/.local/lib/python3.12/site-packages (from deepspeed) (1.1.0)\n",
      "Requirement already satisfied: ninja in /home/kvinod/.local/lib/python3.12/site-packages (from deepspeed) (1.11.1.4)\n",
      "Requirement already satisfied: numpy in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from deepspeed) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from deepspeed) (24.2)\n",
      "Requirement already satisfied: psutil in /home/kvinod/.local/lib/python3.12/site-packages (from deepspeed) (7.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in /home/kvinod/.local/lib/python3.12/site-packages (from deepspeed) (9.0.0)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /home/kvinod/.local/lib/python3.12/site-packages (from deepspeed) (2.11.4)\n",
      "Requirement already satisfied: torch in /home/kvinod/.local/lib/python3.12/site-packages (from deepspeed) (2.7.0)\n",
      "Requirement already satisfied: tqdm in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from deepspeed) (4.67.1)\n",
      "Requirement already satisfied: nvidia-ml-py in /home/kvinod/.local/lib/python3.12/site-packages (from deepspeed) (12.570.86)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/kvinod/.local/lib/python3.12/site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: scikit-learn in /home/kvinod/.local/lib/python3.12/site-packages (from sentence-transformers) (1.7.0)\n",
      "Requirement already satisfied: scipy in /home/kvinod/.local/lib/python3.12/site-packages (from sentence-transformers) (1.16.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/kvinod/.local/lib/python3.12/site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /home/kvinod/.local/lib/python3.12/site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/kvinod/.local/lib/python3.12/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/kvinod/.local/lib/python3.12/site-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.7)\n",
      "Requirement already satisfied: filelock in /home/kvinod/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/kvinod/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/kvinod/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/kvinod/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/kvinod/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/kvinod/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (0.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from requests>=2.20->googlesearch-python) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from requests>=2.20->googlesearch-python) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from requests>=2.20->googlesearch-python) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from requests>=2.20->googlesearch-python) (2025.1.31)\n",
      "Requirement already satisfied: setuptools in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from torch->deepspeed) (75.8.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/kvinod/.local/lib/python3.12/site-packages (from torch->deepspeed) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/kvinod/.local/lib/python3.12/site-packages (from torch->deepspeed) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/kvinod/.local/lib/python3.12/site-packages (from torch->deepspeed) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/kvinod/.local/lib/python3.12/site-packages (from torch->deepspeed) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/kvinod/.local/lib/python3.12/site-packages (from torch->deepspeed) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/kvinod/.local/lib/python3.12/site-packages (from torch->deepspeed) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/kvinod/.local/lib/python3.12/site-packages (from torch->deepspeed) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/kvinod/.local/lib/python3.12/site-packages (from torch->deepspeed) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/kvinod/.local/lib/python3.12/site-packages (from torch->deepspeed) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/kvinod/.local/lib/python3.12/site-packages (from torch->deepspeed) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/kvinod/.local/lib/python3.12/site-packages (from torch->deepspeed) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/kvinod/.local/lib/python3.12/site-packages (from torch->deepspeed) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/kvinod/.local/lib/python3.12/site-packages (from torch->deepspeed) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/kvinod/.local/lib/python3.12/site-packages (from torch->deepspeed) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/kvinod/.local/lib/python3.12/site-packages (from torch->deepspeed) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/kvinod/.local/lib/python3.12/site-packages (from torch->deepspeed) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/kvinod/.local/lib/python3.12/site-packages (from torch->deepspeed) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/kvinod/.local/lib/python3.12/site-packages (from torch->deepspeed) (3.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/kvinod/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/kvinod/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/kvinod/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/kvinod/.local/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/kvinod/.local/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/kvinod/.local/lib/python3.12/site-packages (from sympy>=1.13.3->torch->deepspeed) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/kvinod/.local/lib/python3.12/site-packages (from jinja2->torch->deepspeed) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install googlesearch-python lxml deepspeed sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "947bfc78-3d3d-4000-99b7-c82fb1119133",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running RAG Pipeline ---\n",
      "-> Starting dynamic search for: 'I want to do an array multiplication of the dimension 10x10. How can I do that ion Python?'\n",
      "-> Enhanced query: 'I want to do an array multiplication of the dimension 10x10. How can I do that ion Python? site:developer.nvidia.com OR site:medium.com/rapids-ai OR site:medium.com/cupy-team'\n",
      "-> Found URLs: ['/search?num=6', '', '', '']\n",
      "-> Loading documents from URLs...\n"
     ]
    },
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL '/search?num=6': No scheme supplied. Perhaps you meant https:///search?num=6?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMissingSchema\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 122\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    121\u001b[39m     test_query = \u001b[33m\"\u001b[39m\u001b[33mI want to do an array multiplication of the dimension 10x10. How can I do that ion Python?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     final_answer = \u001b[43mprocess_with_rag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mn======= FINAL ANSWER =======\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    124\u001b[39m     \u001b[38;5;28mprint\u001b[39m(final_answer)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mprocess_with_rag\u001b[39m\u001b[34m(query)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Step 2: Load the documents from the web\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-> Loading documents from URLs...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m docs = [\u001b[43mWebBaseLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m urls]\n\u001b[32m     65\u001b[39m docs_list = [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m docs \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# Step 3: Split the documents into chunks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/packages/envs/genai25.06/lib/python3.12/site-packages/langchain_core/document_loaders/base.py:32\u001b[39m, in \u001b[36mBaseLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m     31\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/packages/envs/genai25.06/lib/python3.12/site-packages/langchain_community/document_loaders/web_base.py:375\u001b[39m, in \u001b[36mWebBaseLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Lazy load text from the url(s) in web_path.\"\"\"\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.web_paths:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m     soup = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_scrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbs_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m     text = soup.get_text(**\u001b[38;5;28mself\u001b[39m.bs_get_text_kwargs)\n\u001b[32m    377\u001b[39m     metadata = _build_metadata(soup, path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/packages/envs/genai25.06/lib/python3.12/site-packages/langchain_community/document_loaders/web_base.py:357\u001b[39m, in \u001b[36mWebBaseLoader._scrape\u001b[39m\u001b[34m(self, url, parser, bs_kwargs)\u001b[39m\n\u001b[32m    353\u001b[39m         parser = \u001b[38;5;28mself\u001b[39m.default_parser\n\u001b[32m    355\u001b[39m \u001b[38;5;28mself\u001b[39m._check_parser(parser)\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m html_doc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequests_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raise_for_status:\n\u001b[32m    359\u001b[39m     html_doc.raise_for_status()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/packages/envs/genai25.06/lib/python3.12/site-packages/requests/sessions.py:602\u001b[39m, in \u001b[36mSession.get\u001b[39m\u001b[34m(self, url, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[32m    595\u001b[39m \n\u001b[32m    596\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m    597\u001b[39m \u001b[33;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    601\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/packages/envs/genai25.06/lib/python3.12/site-packages/requests/sessions.py:575\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# Create the Request.\u001b[39;00m\n\u001b[32m    563\u001b[39m req = Request(\n\u001b[32m    564\u001b[39m     method=method.upper(),\n\u001b[32m    565\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    573\u001b[39m     hooks=hooks,\n\u001b[32m    574\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m prep = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    577\u001b[39m proxies = proxies \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m    579\u001b[39m settings = \u001b[38;5;28mself\u001b[39m.merge_environment_settings(\n\u001b[32m    580\u001b[39m     prep.url, proxies, stream, verify, cert\n\u001b[32m    581\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/packages/envs/genai25.06/lib/python3.12/site-packages/requests/sessions.py:484\u001b[39m, in \u001b[36mSession.prepare_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    481\u001b[39m     auth = get_netrc_auth(request.url)\n\u001b[32m    483\u001b[39m p = PreparedRequest()\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmerge_setting\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCaseInsensitiveDict\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmerge_setting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmerge_setting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcookies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmerged_cookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmerge_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/packages/envs/genai25.06/lib/python3.12/site-packages/requests/models.py:367\u001b[39m, in \u001b[36mPreparedRequest.prepare\u001b[39m\u001b[34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[39m\n\u001b[32m    364\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Prepares the entire request with the given parameters.\"\"\"\u001b[39;00m\n\u001b[32m    366\u001b[39m \u001b[38;5;28mself\u001b[39m.prepare_method(method)\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[38;5;28mself\u001b[39m.prepare_headers(headers)\n\u001b[32m    369\u001b[39m \u001b[38;5;28mself\u001b[39m.prepare_cookies(cookies)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/packages/envs/genai25.06/lib/python3.12/site-packages/requests/models.py:438\u001b[39m, in \u001b[36mPreparedRequest.prepare_url\u001b[39m\u001b[34m(self, url, params)\u001b[39m\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidURL(*e.args)\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scheme:\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MissingSchema(\n\u001b[32m    439\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m: No scheme supplied. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    440\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPerhaps you meant https://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    441\u001b[39m     )\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m host:\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidURL(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m: No host supplied\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mMissingSchema\u001b[39m: Invalid URL '/search?num=6': No scheme supplied. Perhaps you meant https:///search?num=6?"
     ]
    }
   ],
   "source": [
    "# ============== AGENT 2: INFORMATION RETRIEVAL ==============\n",
    "\n",
    "# --- Imports ---\n",
    "import os\n",
    "import socket\n",
    "from typing import List\n",
    "import subprocess\n",
    "import time\n",
    "import re\n",
    "\n",
    "# LangChain and related libraries\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Search library\n",
    "try:\n",
    "    from googlesearch import search\n",
    "except ImportError:\n",
    "    print(\"Error: 'googlesearch-python' is not installed. Please run 'pip install googlesearch-python'\")\n",
    "\n",
    "# It's good practice to set a user agent for web requests\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0\"\n",
    "\n",
    "# --- Function 1: Dynamic Search (Corrected Version 2.0) ---\n",
    "def dynamic_search(query: str, num_results: int = 4) -> list[str]:\n",
    "    \"\"\"\n",
    "    Searches the web for a given query, focusing on recent and relevant\n",
    "    technical blogs and tutorials related to NVIDIA GPU acceleration.\n",
    "    \"\"\"\n",
    "    print(f\"-> Starting dynamic search for: '{query}'\")\n",
    "    enhanced_query = f\"{query} site:developer.nvidia.com OR site:medium.com/rapids-ai OR site:medium.com/cupy-team\"\n",
    "    print(f\"-> Enhanced query: '{enhanced_query}'\")\n",
    "    try:\n",
    "        # THE FIX IS HERE: Removed the 'query=' keyword argument.\n",
    "        search_results = list(search(enhanced_query, num_results=num_results))\n",
    "        \n",
    "        print(f\"-> Found URLs: {search_results}\")\n",
    "        return search_results\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during search: {e}\")\n",
    "        # Fallback to a static list if the search fails\n",
    "        return [\n",
    "            \"https://medium.com/cupy-team/announcing-cupy-v13-66979ee7fab0\",\n",
    "            \"https://www.unum.cloud/blog/2022-01-26-cupy\",\n",
    "        ]\n",
    "\n",
    "# --- Function 2: Main RAG Pipeline ---\n",
    "def process_with_rag(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Processes a user query using a Retrieval-Augmented Generation (RAG) pipeline.\n",
    "    \"\"\"\n",
    "    print(\"--- Running RAG Pipeline ---\")\n",
    "    # Step 1: Get URLs from the dynamic search function\n",
    "    urls = dynamic_search(query)\n",
    "    if not urls:\n",
    "        return \"Sorry, I couldn't find any relevant articles online for your query.\"\n",
    "\n",
    "    # Step 2: Load the documents from the web\n",
    "    print(\"-> Loading documents from URLs...\")\n",
    "    docs = [WebBaseLoader(url).load() for url in urls]\n",
    "    docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "    # Step 3: Split the documents into chunks\n",
    "    print(\"-> Splitting documents into chunks...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=500, chunk_overlap=100\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "    # Step 4: Create embeddings and a vector store retriever\n",
    "    print(\"-> Creating embeddings and vector store...\")\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") #\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=doc_splits,\n",
    "        embedding=embedding_model,\n",
    "        collection_name=\"rag-chroma\"\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    # Step 5: Generate an answer using the LLM and the retrieved context\n",
    "    print(\"-> Retrieving context and generating answer...\")\n",
    "    GENERATE_PROMPT = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are an AI assistant for data scientists. Answer the user's question based ONLY on the context provided.\n",
    "        Focus on practical code examples and clear explanations from the context.\n",
    "        If the context doesn't contain enough information, just say that you couldn't find a specific answer in the provided documents.\n",
    "\n",
    "        CONTEXT:\n",
    "        {context}\n",
    "\n",
    "        QUESTION:\n",
    "        {question}\n",
    "\n",
    "        YOUR ANSWER:\n",
    "        \"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    context_text = \"\\\\n\\\\n---\\\\n\\\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    # Connect to the local LLM on Sol\n",
    "    host_node = socket.gethostname()\n",
    "    llm = OllamaLLM(model=\"qwen3:14b\", base_url=f\"http://kvinod@{host_node}:11434/\") # We'll need to update the user ID\n",
    "\n",
    "    rag_chain = GENERATE_PROMPT | llm\n",
    "    answer = rag_chain.invoke({\"context\": context_text, \"question\": query})\n",
    "\n",
    "    # Clean up the vector store\n",
    "    vectorstore.delete_collection()\n",
    "    \n",
    "    print(\"--- RAG Pipeline Complete ---\")\n",
    "    return answer\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == '__main__':\n",
    "    test_query = \"I want to do an array multiplication of the dimension 10x10. How can I do that ion Python?\"\n",
    "    final_answer = process_with_rag(test_query)\n",
    "    print(\"\\\\n======= FINAL ANSWER =======\\\\n\")\n",
    "    print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c84f58d-8501-4417-9765-ce486959b9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# AI TUTOR AGENT (DEFINITIVE, DYNAMIC & FULLY ABSTRACT FINAL)\n",
    "# This version contains ZERO hardcoded library/language names in the prompts.\n",
    "# =================================================================\n",
    "\n",
    "import os\n",
    "import socket\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "# LangChain and related libraries\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "# Search library\n",
    "try:\n",
    "    from googlesearch import search\n",
    "except ImportError:\n",
    "    print(\"Error: 'googlesearch-python' is not installed. Please run 'pip install googlesearch-python'\")\n",
    "\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0\"\n",
    "\n",
    "class SearchQueryGenerator(BaseModel):\n",
    "    queries: List[str] = Field(description=\"A list of targeted, keyword-focused search queries.\")\n",
    "\n",
    "# --- Part 1: Fully Abstracted Search Query Generation ---\n",
    "def generate_search_queries(query: str, llm) -> List[str]:\n",
    "    \"\"\"\n",
    "    Uses a dynamic prompt to generate language-and-library-specific search queries.\n",
    "    \"\"\"\n",
    "    print(\"-> Using LLM with FULLY ABSTRACTED prompt to generate search queries...\")\n",
    "    \n",
    "    # DYNAMIC PROMPT: The LLM now identifies the correct libraries and language on its own.\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are an expert at generating web search queries for a technical audience.\n",
    "        Analyze the user's question to identify the core technical task and the programming language.\n",
    "        Based on your knowledge, generate 5 concise, targeted search queries. Two queries should be for the standard, CPU-based library for that task/language. Three queries should be for potential GPU-accelerated libraries for that task/language, prioritizing NVIDIA-based solutions if they exist.\n",
    "\n",
    "        User Question: \"{question}\"\n",
    "        \n",
    "        Generate a JSON list of 5 search query strings.\n",
    "        \"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "    \n",
    "    query_generation_chain = prompt_template | llm.with_structured_output(SearchQueryGenerator)\n",
    "\n",
    "    try:\n",
    "        response_model = query_generation_chain.invoke({\"question\": query})\n",
    "        print(f\"-> Generated queries: {response_model.queries}\")\n",
    "        return response_model.queries\n",
    "    except Exception as e:\n",
    "        print(f\"-> LLM failed to generate structured output: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Part 2: The Main Search Function (Unchanged) ---\n",
    "def dynamic_search_agentic(queries: List[str]) -> list[str]:\n",
    "    print(\"-> Executing dynamic search...\")\n",
    "    all_urls = set()\n",
    "    for q in queries:\n",
    "        try:\n",
    "            enhanced_query = f\"{q} site:developer.nvidia.com OR site:medium.com/rapids-ai OR site:medium.com/cupy-team\"\n",
    "            search_results = list(search(enhanced_query, num_results=2))\n",
    "            for url in search_results:\n",
    "                all_urls.add(url)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during search for query '{q}': {e}\")\n",
    "            continue\n",
    "    final_urls = [url for url in list(all_urls) if url]\n",
    "    print(f\"-> Found {len(final_urls)} unique URLs: {final_urls}\")\n",
    "    return final_urls\n",
    "\n",
    "# --- Part 3: The Definitive Dynamic RAG Pipeline ---\n",
    "def process_with_rag(query: str) -> str:\n",
    "    print(\"--- Running FINAL DYNAMIC RAG Pipeline ---\")\n",
    "\n",
    "    host_node = socket.gethostname()\n",
    "    llm = ChatOllama(model=\"qwen3:14b\", base_url=f\"http://jgarc111@{host_node}:11434/\") # Placeholder ID\n",
    "\n",
    "    search_queries = generate_search_queries(query, llm)\n",
    "    urls = []\n",
    "    if search_queries:\n",
    "        urls = dynamic_search_agentic(search_queries)\n",
    "\n",
    "    context_text = \"\"\n",
    "    if urls:\n",
    "        print(\"-> Found documents. Loading and processing context...\")\n",
    "        docs = [WebBaseLoader(url).load() for url in urls]\n",
    "        docs_list = [item for sublist in docs for item in sublist]\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500, chunk_overlap=100)\n",
    "        doc_splits = text_splitter.split_documents(docs_list)\n",
    "        embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        vectorstore = Chroma.from_documents(documents=doc_splits, embedding=embedding_model, collection_name=\"rag-chroma\")\n",
    "        retriever = vectorstore.as_retriever()\n",
    "        retrieved_docs = retriever.invoke(query)\n",
    "        context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "        vectorstore.delete_collection()\n",
    "\n",
    "    # THE DEFINITIVE, FULLY ABSTRACTED \"NVIDIA-FIRST EDUCATIONAL PRIORITY\" PROMPT\n",
    "    final_prompt_template = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are a friendly and knowledgeable AI Tutor for a project focused on data science acceleration. Your primary mission is to educate users on leveraging NVIDIA-based GPU libraries.\n",
    "\n",
    "        First, analyze the user's QUESTION to identify the core task and programming language. Based on your knowledge, determine if a common NVIDIA-based GPU-accelerated library exists for that specific task and language.\n",
    "\n",
    "        Then, follow the appropriate path below to structure your conversational and helpful answer:\n",
    "\n",
    "        **PATH 1: An NVIDIA-based GPU-accelerated library EXISTS for this task.**\n",
    "        1.  Identify the standard CPU library and the NVIDIA GPU library for the user's language and task.\n",
    "        2.  Start with a friendly opening that explains you will show both the GPU-accelerated and standard methods.\n",
    "        3.  Provide a heading for the GPU solution, dynamically inserting the library name (e.g., `### Recommended GPU Solution (with [GPU Library Name])`).\n",
    "        4.  Write the code example for the GPU solution.\n",
    "        5.  Add a \"Performance Note\" section. Explain the benefits of the GPU approach (e.g., for large datasets) and the trade-offs (e.g., performance on small data vs. large data, data transfer overhead).\n",
    "        6.  Provide a heading for the CPU solution, dynamically inserting the library name (e.g., `### Standard CPU Solution (with [CPU Library Name])`).\n",
    "        7.  Write the CPU-based code for comparison.\n",
    "        8.  Do NOT add any disclaimer note at the end.\n",
    "\n",
    "        **PATH 2: An NVIDIA-based GPU-accelerated library DOES NOT EXIST for this task.**\n",
    "        1.  Identify the standard library for the user's specified language and task.\n",
    "        2.  Start with a friendly opening explaining the standard approach.\n",
    "        3.  Provide a heading for the standard solution, dynamically inserting the library name (e.g., `### Standard Solution (with [Library Name])`).\n",
    "        4.  Write the code example using the identified standard library.\n",
    "        5.  End your entire response with the exact sentence: \"Note: The provided solution is the standard method for this task, as a direct NVIDIA-based GPU library for it is not common.\"\n",
    "\n",
    "        Use the CONTEXT below to inform your answer if it is relevant, but your primary instruction is to follow the mission and logic paths described above.\n",
    "\n",
    "        CONTEXT:\n",
    "        {context}\n",
    "\n",
    "        QUESTION:\n",
    "        {question}\n",
    "\n",
    "        YOUR FINAL ANSWER:\n",
    "        \"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "    \n",
    "    final_chain = final_prompt_template | llm\n",
    "    answer = final_chain.invoke({\"context\": context_text, \"question\": query}).content\n",
    "\n",
    "    print(answer)\n",
    "\n",
    "    print(\"--- Pipeline Complete ---\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414516ff-c259-4898-bb4c-3492fdac9549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# GRADIO CHAT APPLICATION FOR AI TUTOR (FINAL)\n",
    "#\n",
    "# This version fixes the UserWarning by specifying the modern 'messages'\n",
    "# format for the chatbot component.\n",
    "# =================================================================\n",
    "\n",
    "import gradio as gr\n",
    "import re\n",
    "import time\n",
    "\n",
    "# This wrapper function connects our backend logic to the Gradio UI.\n",
    "# It assumes 'process_with_rag' is defined and available in the notebook environment.\n",
    "def tutor_chat_interface(user_message, history):\n",
    "    # The 'history' is now a list of dictionaries. Append the new user message.\n",
    "    history.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    # Show a \"thinking...\" message while processing\n",
    "    yield history + [{\"role\": \"assistant\", \"content\": \"Thinking...\"}], \"Thinking...\"\n",
    "\n",
    "    # Call our existing RAG pipeline function\n",
    "    full_response = process_with_rag(user_message)\n",
    "\n",
    "    # Use regex to separate the <think> block from the final answer\n",
    "    think_pattern = re.compile(r\"<think>(.*?)</think>\", re.DOTALL)\n",
    "    think_match = think_pattern.search(full_response)\n",
    "    \n",
    "    thought_process = \"No thought process was found in the response.\"\n",
    "    if think_match:\n",
    "        # Extract the thought process and clean it up\n",
    "        thought_process = think_match.group(1).strip()\n",
    "        # Remove the <think> block from the final answer shown to the user\n",
    "        final_answer = think_pattern.sub(\"\", full_response).strip()\n",
    "    else:\n",
    "        # If no <think> block, the whole response is the answer\n",
    "        final_answer = full_response\n",
    "\n",
    "    # Update the chatbot history with the AI's clean answer\n",
    "    history.append({\"role\": \"assistant\", \"content\": final_answer})\n",
    "    \n",
    "    # Return the final history and the extracted thought process\n",
    "    yield history, thought_process\n",
    "\n",
    "\n",
    "# Build the Gradio UI using Blocks for more control\n",
    "with gr.Blocks(theme=gr.themes.Soft(), css=\".gradio-container {background-color: #f5f5f5;}\") as demo:\n",
    "    gr.Markdown(\"# ðŸ¤– AI Accelerated Data Science Tutor\")\n",
    "    gr.Markdown(\"Ask a question about a data science task. The tutor will provide an explanation and code, prioritizing NVIDIA GPU-accelerated solutions where possible.\")\n",
    "\n",
    "    # THE FIX IS HERE: Added 'type=\"messages\"' to the chatbot component.\n",
    "    chatbot = gr.Chatbot(label=\"Conversation\", height=450, bubble_full_width=False, type=\"messages\")\n",
    "    \n",
    "    with gr.Accordion(\"ðŸ”Ž Show Agent's Thought Process\", open=False):\n",
    "        cot_output = gr.Markdown(\"The agent's reasoning will appear here after it responds.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        msg_textbox = gr.Textbox(\n",
    "            label=\"Your Question\",\n",
    "            placeholder=\"e.g., How do I multiply two 10x10 arrays in Python?\",\n",
    "            scale=4,\n",
    "            autofocus=True,\n",
    "            container=False # This makes the textbox look cleaner\n",
    "        )\n",
    "        submit_btn = gr.Button(\"Ask\", variant=\"primary\", scale=1, min_width=150)\n",
    "\n",
    "    # Main function to handle the chat logic\n",
    "    def handle_submit(user_message, chat_history):\n",
    "        response_generator = tutor_chat_interface(user_message, chat_history)\n",
    "        \n",
    "        # The generator now yields the full message history and the thought process\n",
    "        for history_state, thought_process in response_generator:\n",
    "             yield history_state, thought_process\n",
    "\n",
    "\n",
    "    # Connect the submit button and textbox to the handler function\n",
    "    submit_btn.click(\n",
    "        handle_submit, \n",
    "        [msg_textbox, chatbot], \n",
    "        [chatbot, cot_output]\n",
    "    ).then(lambda: gr.update(value=\"\"), None, [msg_textbox], queue=False)\n",
    "\n",
    "    msg_textbox.submit(\n",
    "        handle_submit, \n",
    "        [msg_textbox, chatbot], \n",
    "        [chatbot, cot_output]\n",
    "    ).then(lambda: gr.update(value=\"\"), None, [msg_textbox], queue=False)\n",
    "\n",
    "\n",
    "# Launch the application\n",
    "# Set share=True if you need a public link from the Sol jupyter notebook\n",
    "demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564167a0-3f71-4004-8c3f-0822950deb33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai25.06",
   "language": "python",
   "name": "genai25.06"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
