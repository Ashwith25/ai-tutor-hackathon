{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947bfc78-3d3d-4000-99b7-c82fb1119133",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============== AGENT 2: INFORMATION RETRIEVAL ==============\n",
    "\n",
    "# --- Imports ---\n",
    "import os\n",
    "import socket\n",
    "from typing import List\n",
    "\n",
    "# LangChain and related libraries\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Search library\n",
    "try:\n",
    "    from googlesearch import search\n",
    "except ImportError:\n",
    "    print(\"Error: 'googlesearch-python' is not installed. Please run 'pip install googlesearch-python'\")\n",
    "\n",
    "# It's good practice to set a user agent for web requests\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0\"\n",
    "\n",
    "# --- Function 1: Dynamic Search (Corrected Version 2.0) ---\n",
    "def dynamic_search(query: str, num_results: int = 4) -> list[str]:\n",
    "    \"\"\"\n",
    "    Searches the web for a given query, focusing on recent and relevant\n",
    "    technical blogs and tutorials related to NVIDIA GPU acceleration.\n",
    "    \"\"\"\n",
    "    print(f\"-> Starting dynamic search for: '{query}'\")\n",
    "    enhanced_query = f\"{query} site:developer.nvidia.com OR site:medium.com/rapids-ai OR site:medium.com/cupy-team\"\n",
    "    print(f\"-> Enhanced query: '{enhanced_query}'\")\n",
    "    try:\n",
    "        # THE FIX IS HERE: Removed the 'query=' keyword argument.\n",
    "        search_results = list(search(enhanced_query, num_results=num_results))\n",
    "        \n",
    "        print(f\"-> Found URLs: {search_results}\")\n",
    "        return search_results\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during search: {e}\")\n",
    "        # Fallback to a static list if the search fails\n",
    "        return [\n",
    "            \"https://medium.com/cupy-team/announcing-cupy-v13-66979ee7fab0\",\n",
    "            \"https://www.unum.cloud/blog/2022-01-26-cupy\",\n",
    "        ]\n",
    "\n",
    "# --- Function 2: Main RAG Pipeline ---\n",
    "def process_with_rag(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Processes a user query using a Retrieval-Augmented Generation (RAG) pipeline.\n",
    "    \"\"\"\n",
    "    print(\"--- Running RAG Pipeline ---\")\n",
    "    # Step 1: Get URLs from the dynamic search function\n",
    "    urls = dynamic_search(query)\n",
    "    if not urls:\n",
    "        return \"Sorry, I couldn't find any relevant articles online for your query.\"\n",
    "\n",
    "    # Step 2: Load the documents from the web\n",
    "    print(\"-> Loading documents from URLs...\")\n",
    "    docs = [WebBaseLoader(url).load() for url in urls]\n",
    "    docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "    # Step 3: Split the documents into chunks\n",
    "    print(\"-> Splitting documents into chunks...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=500, chunk_overlap=100\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "    # Step 4: Create embeddings and a vector store retriever\n",
    "    print(\"-> Creating embeddings and vector store...\")\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") #\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=doc_splits,\n",
    "        embedding=embedding_model,\n",
    "        collection_name=\"rag-chroma\"\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    # Step 5: Generate an answer using the LLM and the retrieved context\n",
    "    print(\"-> Retrieving context and generating answer...\")\n",
    "    GENERATE_PROMPT = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are an AI assistant for data scientists. Answer the user's question based ONLY on the context provided.\n",
    "        Focus on practical code examples and clear explanations from the context.\n",
    "        If the context doesn't contain enough information, just say that you couldn't find a specific answer in the provided documents.\n",
    "\n",
    "        CONTEXT:\n",
    "        {context}\n",
    "\n",
    "        QUESTION:\n",
    "        {question}\n",
    "\n",
    "        YOUR ANSWER:\n",
    "        \"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    context_text = \"\\\\n\\\\n---\\\\n\\\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    # Connect to the local LLM on Sol\n",
    "    host_node = socket.gethostname()\n",
    "    llm = OllamaLLM(model=\"qwen3:14b\", base_url=f\"http://apoojar4@{host_node}:11434/\") # We'll need to update the user ID\n",
    "\n",
    "    rag_chain = GENERATE_PROMPT | llm\n",
    "    answer = rag_chain.invoke({\"context\": context_text, \"question\": query})\n",
    "\n",
    "    # Clean up the vector store\n",
    "    vectorstore.delete_collection()\n",
    "    \n",
    "    print(\"--- RAG Pipeline Complete ---\")\n",
    "    return answer\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == '__main__':\n",
    "    test_query = \"I want to do an array multiplication of the dimension 10x10. How can I do that ion Python?\"\n",
    "    final_answer = process_with_rag(test_query)\n",
    "    print(\"\\\\n======= FINAL ANSWER =======\\\\n\")\n",
    "    print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c84f58d-8501-4417-9765-ce486959b9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'googlesearch-python' is not installed. Please run 'pip install googlesearch-python'\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# AI TUTOR AGENT (DEFINITIVE, DYNAMIC & FULLY ABSTRACT FINAL)\n",
    "# This version contains ZERO hardcoded library/language names in the prompts.\n",
    "# =================================================================\n",
    "\n",
    "import os\n",
    "import socket\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "# LangChain and related libraries\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "# Search library\n",
    "try:\n",
    "    from googlesearch import search\n",
    "except ImportError:\n",
    "    print(\"Error: 'googlesearch-python' is not installed. Please run 'pip install googlesearch-python'\")\n",
    "\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0\"\n",
    "\n",
    "class SearchQueryGenerator(BaseModel):\n",
    "    queries: List[str] = Field(description=\"A list of targeted, keyword-focused search queries.\")\n",
    "\n",
    "# --- Part 1: Fully Abstracted Search Query Generation ---\n",
    "def generate_search_queries(query: str, llm) -> List[str]:\n",
    "    \"\"\"\n",
    "    Uses a dynamic prompt to generate language-and-library-specific search queries.\n",
    "    \"\"\"\n",
    "    print(\"-> Using LLM with FULLY ABSTRACTED prompt to generate search queries...\")\n",
    "    \n",
    "    # DYNAMIC PROMPT: The LLM now identifies the correct libraries and language on its own.\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are an expert at generating web search queries for a technical audience.\n",
    "        Analyze the user's question to identify the core technical task and the programming language.\n",
    "        Based on your knowledge, generate 5 concise, targeted search queries. Two queries should be for the standard, CPU-based library for that task/language. Three queries should be for potential GPU-accelerated libraries for that task/language, prioritizing NVIDIA-based solutions if they exist.\n",
    "\n",
    "        User Question: \"{question}\"\n",
    "        \n",
    "        Generate a JSON list of 5 search query strings.\n",
    "        \"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "    \n",
    "    query_generation_chain = prompt_template | llm.with_structured_output(SearchQueryGenerator)\n",
    "\n",
    "    try:\n",
    "        response_model = query_generation_chain.invoke({\"question\": query})\n",
    "        print(f\"-> Generated queries: {response_model.queries}\")\n",
    "        return response_model.queries\n",
    "    except Exception as e:\n",
    "        print(f\"-> LLM failed to generate structured output: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Part 2: The Main Search Function (Unchanged) ---\n",
    "def dynamic_search_agentic(queries: List[str]) -> list[str]:\n",
    "    print(\"-> Executing dynamic search...\")\n",
    "    all_urls = set()\n",
    "    for q in queries:\n",
    "        try:\n",
    "            enhanced_query = f\"{q} site:developer.nvidia.com OR site:medium.com/rapids-ai OR site:medium.com/cupy-team\"\n",
    "            search_results = list(search(enhanced_query, num_results=2))\n",
    "            for url in search_results:\n",
    "                all_urls.add(url)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during search for query '{q}': {e}\")\n",
    "            continue\n",
    "    final_urls = [url for url in list(all_urls) if url]\n",
    "    print(f\"-> Found {len(final_urls)} unique URLs: {final_urls}\")\n",
    "    return final_urls\n",
    "\n",
    "# --- Part 3: The Definitive Dynamic RAG Pipeline ---\n",
    "def process_with_rag(query: str) -> str:\n",
    "    print(\"--- Running FINAL DYNAMIC RAG Pipeline ---\")\n",
    "\n",
    "    host_node = socket.gethostname()\n",
    "    llm = ChatOllama(model=\"qwen3:14b\", base_url=f\"http://jgarc111@{host_node}:11434/\") # Placeholder ID\n",
    "\n",
    "    search_queries = generate_search_queries(query, llm)\n",
    "    urls = []\n",
    "    if search_queries:\n",
    "        urls = dynamic_search_agentic(search_queries)\n",
    "\n",
    "    context_text = \"\"\n",
    "    if urls:\n",
    "        print(\"-> Found documents. Loading and processing context...\")\n",
    "        docs = [WebBaseLoader(url).load() for url in urls]\n",
    "        docs_list = [item for sublist in docs for item in sublist]\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500, chunk_overlap=100)\n",
    "        doc_splits = text_splitter.split_documents(docs_list)\n",
    "        embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        vectorstore = Chroma.from_documents(documents=doc_splits, embedding=embedding_model, collection_name=\"rag-chroma\")\n",
    "        retriever = vectorstore.as_retriever()\n",
    "        retrieved_docs = retriever.invoke(query)\n",
    "        context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "        vectorstore.delete_collection()\n",
    "\n",
    "    # THE DEFINITIVE, FULLY ABSTRACTED \"NVIDIA-FIRST EDUCATIONAL PRIORITY\" PROMPT\n",
    "    final_prompt_template = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are a friendly and knowledgeable AI Tutor for a project focused on data science acceleration. Your primary mission is to educate users on leveraging NVIDIA-based GPU libraries.\n",
    "\n",
    "        First, analyze the user's QUESTION to identify the core task and programming language. Based on your knowledge, determine if a common NVIDIA-based GPU-accelerated library exists for that specific task and language.\n",
    "\n",
    "        Then, follow the appropriate path below to structure your conversational and helpful answer:\n",
    "\n",
    "        **PATH 1: An NVIDIA-based GPU-accelerated library EXISTS for this task.**\n",
    "        1.  Identify the standard CPU library and the NVIDIA GPU library for the user's language and task.\n",
    "        2.  Start with a friendly opening that explains you will show both the GPU-accelerated and standard methods.\n",
    "        3.  Provide a heading for the GPU solution, dynamically inserting the library name (e.g., `### Recommended GPU Solution (with [GPU Library Name])`).\n",
    "        4.  Write the code example for the GPU solution.\n",
    "        5.  Add a \"Performance Note\" section. Explain the benefits of the GPU approach (e.g., for large datasets) and the trade-offs (e.g., performance on small data vs. large data, data transfer overhead).\n",
    "        6.  Provide a heading for the CPU solution, dynamically inserting the library name (e.g., `### Standard CPU Solution (with [CPU Library Name])`).\n",
    "        7.  Write the CPU-based code for comparison.\n",
    "        8.  Do NOT add any disclaimer note at the end.\n",
    "\n",
    "        **PATH 2: An NVIDIA-based GPU-accelerated library DOES NOT EXIST for this task.**\n",
    "        1.  Identify the standard library for the user's specified language and task.\n",
    "        2.  Start with a friendly opening explaining the standard approach.\n",
    "        3.  Provide a heading for the standard solution, dynamically inserting the library name (e.g., `### Standard Solution (with [Library Name])`).\n",
    "        4.  Write the code example using the identified standard library.\n",
    "        5.  End your entire response with the exact sentence: \"Note: The provided solution is the standard method for this task, as a direct NVIDIA-based GPU library for it is not common.\"\n",
    "\n",
    "        Use the CONTEXT below to inform your answer if it is relevant, but your primary instruction is to follow the mission and logic paths described above.\n",
    "\n",
    "        CONTEXT:\n",
    "        {context}\n",
    "\n",
    "        QUESTION:\n",
    "        {question}\n",
    "\n",
    "        YOUR FINAL ANSWER:\n",
    "        \"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "    \n",
    "    final_chain = final_prompt_template | llm\n",
    "    answer = final_chain.invoke({\"context\": context_text, \"question\": query}).content\n",
    "\n",
    "    print(\"--- Pipeline Complete ---\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414516ff-c259-4898-bb4c-3492fdac9549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1844222/790829847.py:51: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
      "  chatbot = gr.Chatbot(label=\"Conversation\", height=450, bubble_full_width=False, type=\"messages\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://ac502bfbe8ef153d4a.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ac502bfbe8ef153d4a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running FINAL DYNAMIC RAG Pipeline ---\n",
      "-> Using LLM with FULLY ABSTRACTED prompt to generate search queries...\n",
      "-> Generated queries: ['cuPy library documentation', 'cuPy vs NumPy performance comparison', 'cuPy GPU acceleration for linear algebra', 'cuPy installation and setup guide', 'cuPy equivalents for NumPy functions']\n",
      "-> Executing dynamic search...\n",
      "An error occurred during search for query 'cuPy library documentation': name 'search' is not defined\n",
      "An error occurred during search for query 'cuPy vs NumPy performance comparison': name 'search' is not defined\n",
      "An error occurred during search for query 'cuPy GPU acceleration for linear algebra': name 'search' is not defined\n",
      "An error occurred during search for query 'cuPy installation and setup guide': name 'search' is not defined\n",
      "An error occurred during search for query 'cuPy equivalents for NumPy functions': name 'search' is not defined\n",
      "-> Found 0 unique URLs: []\n",
      "--- Pipeline Complete ---\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# GRADIO CHAT APPLICATION FOR AI TUTOR (FINAL)\n",
    "#\n",
    "# This version fixes the UserWarning by specifying the modern 'messages'\n",
    "# format for the chatbot component.\n",
    "# =================================================================\n",
    "\n",
    "import gradio as gr\n",
    "import re\n",
    "import time\n",
    "\n",
    "# This wrapper function connects our backend logic to the Gradio UI.\n",
    "# It assumes 'process_with_rag' is defined and available in the notebook environment.\n",
    "def tutor_chat_interface(user_message, history):\n",
    "    # The 'history' is now a list of dictionaries. Append the new user message.\n",
    "    history.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    # Show a \"thinking...\" message while processing\n",
    "    yield history + [{\"role\": \"assistant\", \"content\": \"Thinking...\"}], \"Thinking...\"\n",
    "\n",
    "    # Call our existing RAG pipeline function\n",
    "    full_response = process_with_rag(user_message)\n",
    "\n",
    "    # Use regex to separate the <think> block from the final answer\n",
    "    think_pattern = re.compile(r\"<think>(.*?)</think>\", re.DOTALL)\n",
    "    think_match = think_pattern.search(full_response)\n",
    "    \n",
    "    thought_process = \"No thought process was found in the response.\"\n",
    "    if think_match:\n",
    "        # Extract the thought process and clean it up\n",
    "        thought_process = think_match.group(1).strip()\n",
    "        # Remove the <think> block from the final answer shown to the user\n",
    "        final_answer = think_pattern.sub(\"\", full_response).strip()\n",
    "    else:\n",
    "        # If no <think> block, the whole response is the answer\n",
    "        final_answer = full_response\n",
    "\n",
    "    # Update the chatbot history with the AI's clean answer\n",
    "    history.append({\"role\": \"assistant\", \"content\": final_answer})\n",
    "    \n",
    "    # Return the final history and the extracted thought process\n",
    "    yield history, thought_process\n",
    "\n",
    "\n",
    "# Build the Gradio UI using Blocks for more control\n",
    "with gr.Blocks(theme=gr.themes.Soft(), css=\".gradio-container {background-color: #f5f5f5;}\") as demo:\n",
    "    gr.Markdown(\"# ðŸ¤– AI Accelerated Data Science Tutor\")\n",
    "    gr.Markdown(\"Ask a question about a data science task. The tutor will provide an explanation and code, prioritizing NVIDIA GPU-accelerated solutions where possible.\")\n",
    "\n",
    "    # THE FIX IS HERE: Added 'type=\"messages\"' to the chatbot component.\n",
    "    chatbot = gr.Chatbot(label=\"Conversation\", height=450, bubble_full_width=False, type=\"messages\")\n",
    "    \n",
    "    with gr.Accordion(\"ðŸ”Ž Show Agent's Thought Process\", open=False):\n",
    "        cot_output = gr.Markdown(\"The agent's reasoning will appear here after it responds.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        msg_textbox = gr.Textbox(\n",
    "            label=\"Your Question\",\n",
    "            placeholder=\"e.g., How do I multiply two 10x10 arrays in Python?\",\n",
    "            scale=4,\n",
    "            autofocus=True,\n",
    "            container=False # This makes the textbox look cleaner\n",
    "        )\n",
    "        submit_btn = gr.Button(\"Ask\", variant=\"primary\", scale=1, min_width=150)\n",
    "\n",
    "    # Main function to handle the chat logic\n",
    "    def handle_submit(user_message, chat_history):\n",
    "        response_generator = tutor_chat_interface(user_message, chat_history)\n",
    "        \n",
    "        # The generator now yields the full message history and the thought process\n",
    "        for history_state, thought_process in response_generator:\n",
    "             yield history_state, thought_process\n",
    "\n",
    "\n",
    "    # Connect the submit button and textbox to the handler function\n",
    "    submit_btn.click(\n",
    "        handle_submit, \n",
    "        [msg_textbox, chatbot], \n",
    "        [chatbot, cot_output]\n",
    "    ).then(lambda: gr.update(value=\"\"), None, [msg_textbox], queue=False)\n",
    "\n",
    "    msg_textbox.submit(\n",
    "        handle_submit, \n",
    "        [msg_textbox, chatbot], \n",
    "        [chatbot, cot_output]\n",
    "    ).then(lambda: gr.update(value=\"\"), None, [msg_textbox], queue=False)\n",
    "\n",
    "\n",
    "# Launch the application\n",
    "# Set share=True if you need a public link from the Sol jupyter notebook\n",
    "demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564167a0-3f71-4004-8c3f-0822950deb33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai25.06",
   "language": "python",
   "name": "genai25.06"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
