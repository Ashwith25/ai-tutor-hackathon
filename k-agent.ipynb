{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a431c68-e602-4df4-b2ac-6de5c51a90e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully connected to Ollama LLM.\n",
      "ðŸ¤– Hello! I'm your AI Accelerated Data Science Tutor.\n"
     ]
    }
   ],
   "source": [
    "# ============== AGENT 1: LLM-POWERED USER PROFILER (V3 - Corrected) ==============\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import socket\n",
    "from typing import List\n",
    "\n",
    "# As per the deprecation warning, we should import from Pydantic V2.\n",
    "# This future-proofs our code.\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "# --- Pydantic model for structured LLM output ---\n",
    "class Questionnaire(BaseModel):\n",
    "    questions: List[str] = Field(description=\"A list of 4-5 questions for the user.\")\n",
    "\n",
    "class UserProfilerAgent_V3:\n",
    "    \"\"\"\n",
    "    Agent 1 (V3): Guarantees the user's name is collected first before\n",
    "    using an LLM to dynamically generate the rest of the questionnaire.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, profiles_dir: str = \"user_profiles\"):\n",
    "        self.profiles_dir = profiles_dir\n",
    "        if not os.path.exists(self.profiles_dir):\n",
    "            os.makedirs(self.profiles_dir)\n",
    "\n",
    "        try:\n",
    "            host_node = socket.gethostname()\n",
    "            # NOTE: Replace 'jgarc111' with the ASURITE ID of the user running the Ollama server\n",
    "            self.llm = ChatOllama(model=\"qwen3:14b\", base_url=f\"http://jgarc111@{host_node}:11434/\")\n",
    "            self.structured_llm = self.llm.with_structured_output(Questionnaire)\n",
    "            print(\"âœ… Successfully connected to Ollama LLM.\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error connecting to Ollama: {e}\")\n",
    "            self.llm = None\n",
    "\n",
    "    def _generate_questions_with_llm(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Uses an LLM to dynamically generate a user questionnaire.\n",
    "        **Correction:** The prompt now explicitly tells the LLM to NOT ask for a name.\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ¤– Generating a personalized questionnaire for you...\")\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            template=\"\"\"\n",
    "            You are a helpful assistant for an AI Data Science Tutor. Your goal is to create a short questionnaire (4-5 questions) to understand a user's knowledge level.\n",
    "            \n",
    "            The questions should gently probe their experience with:\n",
    "            1. The Python programming language.\n",
    "            2. Common CPU-based data science libraries (like NumPy, Pandas).\n",
    "            3. Their awareness of GPU computing and hardware acceleration.\n",
    "            4. Their familiarity with any NVIDIA-specific GPU libra_ries (like CuPy or RAPIDS).\n",
    "\n",
    "            IMPORTANT: Do NOT ask for the user's name, as it has already been collected.\n",
    "            Return the questions as a JSON list. Be conversational and friendly.\n",
    "            \"\"\",\n",
    "            input_variables=[],\n",
    "        )\n",
    "        \n",
    "        query_generation_chain = prompt | self.structured_llm\n",
    "        \n",
    "        try:\n",
    "            response_model = query_generation_chain.invoke({})\n",
    "            return response_model.questions\n",
    "        except Exception as e:\n",
    "            print(f\"-> LLM failed to generate questions, falling back to default. Error: {e}\")\n",
    "            return [\n",
    "                \"On a scale of 1-5, how comfortable are you with Python?\",\n",
    "                \"Which Python data science libraries (like Pandas or NumPy) have you used before?\",\n",
    "                \"Have you ever heard of using GPUs to speed up data analysis?\",\n",
    "                \"What's the first tool you'd reach for to do a large matrix multiplication in Python?\"\n",
    "            ]\n",
    "\n",
    "    def _ask_dynamic_questions(self, questions: List[str]) -> dict:\n",
    "        \"\"\"\n",
    "        Asks the dynamically generated questions and records the answers.\n",
    "        \"\"\"\n",
    "        answers = {}\n",
    "        for i, question in enumerate(questions, start=1):\n",
    "            answer = input(f\"{i+1}. {question} \") # Start numbering from 2\n",
    "            answers[question] = answer\n",
    "        return answers\n",
    "\n",
    "    def _generate_report_with_llm(self, user_name: str, answers: dict) -> str:\n",
    "        \"\"\"\n",
    "        Sends the user's answers to the LLM to generate a profile report.\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ¤– Analyzing your responses and creating a profile...\")\n",
    "        \n",
    "        answers_str = \"\\n\".join([f\"- {q}: {a}\" for q, a in answers.items()])\n",
    "\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"\"\"\n",
    "            You are an expert AI analyst. A user named {user_name} has answered a questionnaire about their data science skills.\n",
    "            Your task is to analyze their answers and generate a \"TUTORING STRATEGY\" report for our AI Tutor.\n",
    "\n",
    "            **User's Answers:**\n",
    "            {answers}\n",
    "\n",
    "            **Your Task:**\n",
    "            1.  Determine the user's knowledge level: 'Beginner', 'Intermediate', or 'Advanced'.\n",
    "            2.  Write a concise report following the correct strategy format below. This report will be given to another AI, so the instructions must be clear.\n",
    "\n",
    "            ---\n",
    "            **STRATEGY FORMATS (Choose ONE):**\n",
    "\n",
    "            **If 'Beginner':**\n",
    "            Start with `Knowledge Level: Beginner` on one line. On the next line, start with the exact phrase `TUTORING STRATEGY: The user is a beginner.` Then, explain that the tutor should use high-level concepts, explain the 'why' of GPU acceleration, and introduce NVIDIA libraries (like CuPy) as a simple, powerful alternative to what they already know.\n",
    "\n",
    "            **If 'Intermediate':**\n",
    "            Start with `Knowledge Level: Intermediate` on one line. On the next line, start with the exact phrase `TUTORING STRATEGY: The user is at an intermediate level.` Then, explain that the tutor should provide direct code comparisons (e.g., NumPy vs. CuPy), focus on performance benefits, and show clear benchmarking examples.\n",
    "\n",
    "            **If 'Advanced':**\n",
    "            Start with `Knowledge Level: Advanced` on one line. On the next line, start with the exact phrase `TUTORING STRATEGY: The user is advanced.` Then, explain that the tutor can provide nuanced advice, discuss the broader NVIDIA RAPIDS ecosystem, and cover specific benchmarking methodologies on the Sol supercomputer.\n",
    "            ---\n",
    "\n",
    "            Now, generate the complete report, including the \"Knowledge Level\" and the \"TUTORING STRATEGY\".\n",
    "            \"\"\",\n",
    "            input_variables=[\"user_name\", \"answers\"],\n",
    "        )\n",
    "\n",
    "        report_generation_chain = prompt | self.llm\n",
    "        response_message = report_generation_chain.invoke({\"user_name\": user_name, \"answers\": answers_str})\n",
    "        report_content = response_message.content\n",
    "        \n",
    "        return report_content\n",
    "\n",
    "    def _save_report(self, report: str, user_name: str):\n",
    "        \"\"\"\n",
    "        Saves the generated report to a text file.\n",
    "        \"\"\"\n",
    "        filename = \"_\".join(user_name.lower().split()) + \".txt\"\n",
    "        filepath = os.path.join(self.profiles_dir, filename)\n",
    "        \n",
    "        with open(filepath, \"w\") as f:\n",
    "            f.write(report)\n",
    "            \n",
    "        print(f\"\\nâœ… User profile report saved successfully to: {filepath}\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        The main method to run the agent's full workflow.\n",
    "        **Correction:** Asks for name first to ensure correct file naming.\n",
    "        \"\"\"\n",
    "        if not self.llm:\n",
    "            return\n",
    "\n",
    "        print(\"ðŸ¤– Hello! I'm your AI Accelerated Data Science Tutor.\")\n",
    "        user_name = input(\"1. To get started, what is your first and last name? \")\n",
    "\n",
    "        questions = self._generate_questions_with_llm()\n",
    "        answers = self._ask_dynamic_questions(questions)\n",
    "        report_content = self._generate_report_with_llm(user_name, answers)\n",
    "        \n",
    "        # Add the final header to the report\n",
    "        full_report = f\"--- User Profile for {user_name} ---\\n{report_content}\\n--- End of Profile ---\"\n",
    "        \n",
    "        print(\"\\n--- Generated Profile Report ---\")\n",
    "        print(full_report)\n",
    "        \n",
    "        self._save_report(full_report, user_name)\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == '__main__':\n",
    "    profiler_agent = UserProfilerAgent_V3()\n",
    "    profiler_agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7209a2-aec7-4348-8280-16f1770f6b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully connected to Ollama LLM.\n",
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://c9693d375e12c96772.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c9693d375e12c96772.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============== AGENT 1: GRADIO INTERFACE FOR USER PROFILER ==============\"\n",
    "# This script combines the LLM-powered agent with a user-friendly Gradio UI.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import socket\n",
    "import gradio as gr\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Pydantic and LangChain imports\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "# --- Pydantic Models for Structured LLM Output ---\n",
    "class Questionnaire(BaseModel):\n",
    "    \"\"\"A Pydantic model to structure the questionnaire from the LLM.\"\"\"\n",
    "    questions: List[str] = Field(description=\"A list of 4-5 questions for the user.\")\n",
    "\n",
    "class ProfileAnalysis(BaseModel):\n",
    "    \"\"\"A Pydantic model to structure the profile analysis from the LLM.\"\"\"\n",
    "    knowledge_level: str = Field(description=\"The user's determined knowledge level: 'Beginner', 'Intermediate', or 'Advanced'.\")\n",
    "    tutoring_strategy: str = Field(description=\"The detailed tutoring strategy based on the user's knowledge level.\")\n",
    "\n",
    "# --- The Backend Agent Logic ---# ============== AGENT 1: GRADIO INTERFACE FOR USER PROFILER ==============\"\n",
    "# This script combines the LLM-powered agent with a user-friendly Gradio UI.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import socket\n",
    "import gradio as gr\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Pydantic and LangChain imports\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "# --- Pydantic Models for Structured LLM Output ---\n",
    "class Questionnaire(BaseModel):\n",
    "    \"\"\"A Pydantic model to structure the questionnaire from the LLM.\"\"\"\n",
    "    questions: List[str] = Field(description=\"A list of 4-5 questions for the user.\")\n",
    "\n",
    "class ProfileAnalysis(BaseModel):\n",
    "    \"\"\"A Pydantic model to structure the profile analysis from the LLM.\"\"\"\n",
    "    knowledge_level: str = Field(description=\"The user's determined knowledge level: 'Beginner', 'Intermediate', or 'Advanced'.\")\n",
    "    tutoring_strategy: str = Field(description=\"The detailed tutoring strategy based on the user's knowledge level.\")\n",
    "\n",
    "# --- The Backend Agent Logic ---\n",
    "class UserProfilerAgent_Gradio:\n",
    "    \"\"\"\n",
    "    Agent 1 (Gradio Version): Handles the logic for profiling users via an LLM.\n",
    "    This class is designed to be called by the Gradio interface functions.\n",
    "    \"\"\"\n",
    "    def __init__(self, profiles_dir: str = \"user_profiles\"):\n",
    "        self.profiles_dir = profiles_dir\n",
    "        if not os.path.exists(self.profiles_dir):\n",
    "            os.makedirs(self.profiles_dir)\n",
    "\n",
    "        try:\n",
    "            host_node = socket.gethostname()\n",
    "            # IMPORTANT: Replace 'jgarc111' with the ASURITE ID of the user running the Ollama server on Sol.\n",
    "            self.llm = ChatOllama(model=\"qwen3:14b\", temperature=0, base_url=f\"http://jgarc111@{host_node}:11434/\")\n",
    "            print(\"âœ… Successfully connected to Ollama LLM.\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error connecting to Ollama: {e}\")\n",
    "            self.llm = None\n",
    "\n",
    "    def generate_questions(self) -> List[str]:\n",
    "        \"\"\"Uses an LLM to dynamically generate the questionnaire.\"\"\"\n",
    "        if not self.llm: return self._fallback_questions()\n",
    "\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            You are an assistant for an AI Data Science Tutor. Create a short questionnaire (4-5 questions)\n",
    "            to understand a user's knowledge of Python, data science libraries (NumPy, Pandas), and their\n",
    "            awareness of GPU computing (NVIDIA, CuPy, RAPIDS).\n",
    "            IMPORTANT: Do NOT ask for the user's name. Return a JSON list of questions.\n",
    "            \"\"\"\n",
    "        )\n",
    "        structured_llm = self.llm.with_structured_output(Questionnaire)\n",
    "        chain = prompt | structured_llm\n",
    "        try:\n",
    "            return chain.invoke({}).questions\n",
    "        except Exception:\n",
    "            return self._fallback_questions()\n",
    "\n",
    "    def _fallback_questions(self) -> List[str]:\n",
    "        \"\"\"Fallback questions if the LLM fails.\"\"\"\n",
    "        return [\n",
    "            \"On a scale of 1-5, how comfortable are you with Python?\",\n",
    "            \"Which Python data science libraries (like Pandas or NumPy) have you used before?\",\n",
    "            \"Have you ever heard of using GPUs to speed up data analysis?\",\n",
    "            \"What's the first tool you'd reach for to do a large matrix multiplication in Python?\"\n",
    "        ]\n",
    "\n",
    "    def generate_report(self, user_name: str, answers: Dict[str, str]) -> str:\n",
    "        \"\"\"Uses an LLM to analyze answers and generate a profile report.\"\"\"\n",
    "        if not self.llm: return \"Error: LLM not connected.\"\n",
    "\n",
    "        answers_str = \"\\n\".join([f\"- {q}: {a}\" for q, a in answers.items()])\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            You are an expert AI analyst. A user named {user_name} has answered a questionnaire.\n",
    "            Analyze their answers and generate a profile.\n",
    "\n",
    "            **User's Answers:**\n",
    "            {answers}\n",
    "\n",
    "            **Your Task:**\n",
    "            1.  Determine the user's knowledge level: 'Beginner', 'Intermediate', or 'Advanced'.\n",
    "            2.  Write a concise tutoring strategy for our AI Tutor based on this level.\n",
    "            \n",
    "            **Example for Intermediate:** Start with a strategy focusing on direct code comparisons (NumPy vs. CuPy) and performance benefits.\n",
    "            Provide your output as a JSON object with 'knowledge_level' and 'tutoring_strategy' fields.\n",
    "            \"\"\"\n",
    "        )\n",
    "        structured_llm = self.llm.with_structured_output(ProfileAnalysis)\n",
    "        chain = prompt | structured_llm\n",
    "        try:\n",
    "            analysis = chain.invoke({\"user_name\": user_name, \"answers\": answers_str})\n",
    "            report_content = f\"Knowledge Level: {analysis.knowledge_level}\\n\\nTUTORING STRATEGY: {analysis.tutoring_strategy}\"\n",
    "            full_report = f\"--- User Profile for {user_name} ---\\n{report_content}\\n--- End of Profile ---\"\n",
    "            return full_report\n",
    "        except Exception as e:\n",
    "            return f\"Error generating report: {e}\"\n",
    "\n",
    "    def save_report(self, report: str, user_name: str) -> str:\n",
    "        \"\"\"Saves the report to a file, overwriting if it exists.\"\"\"\n",
    "        filename = \"_\".join(user_name.lower().split()) + \".txt\"\n",
    "        filepath = os.path.join(self.profiles_dir, filename)\n",
    "        with open(filepath, \"w\") as f:\n",
    "            f.write(report)\n",
    "        return f\"âœ… Profile for {user_name} saved successfully to: {filepath}\"\n",
    "\n",
    "# --- Gradio UI Application ---\n",
    "\n",
    "# Instantiate the agent\n",
    "agent = UserProfilerAgent_Gradio()\n",
    "\n",
    "def start_workflow(mode: str):\n",
    "    \"\"\"Hides main buttons and shows the name input field.\"\"\"\n",
    "    initial_state = {\"mode\": mode, \"name\": \"\", \"questions\": [], \"answers\": {}, \"current_q_index\": -1}\n",
    "    return (\n",
    "        gr.update(visible=False), # Hide New User button\n",
    "        gr.update(visible=False), # Hide Update Profile button\n",
    "        gr.update(visible=True),  # Show name row\n",
    "        initial_state\n",
    "    )\n",
    "\n",
    "def process_name(name: str, state: Dict[str, Any]):\n",
    "    \"\"\"Processes the user's name and gets the questionnaire.\"\"\"\n",
    "    if not name.strip():\n",
    "        return gr.update(), gr.update(), gr.update(), gr.update(), state # No change if name is empty\n",
    "\n",
    "    state[\"name\"] = name\n",
    "    questions = agent.generate_questions()\n",
    "    state[\"questions\"] = questions\n",
    "    state[\"current_q_index\"] = 0\n",
    "    \n",
    "    return (\n",
    "        gr.update(visible=False), # Hide name row\n",
    "        gr.update(visible=True),  # Show question row\n",
    "        gr.update(label=questions[0]), # Update question label\n",
    "        gr.update(value=\"\"), # Clear answer textbox\n",
    "        state\n",
    "    )\n",
    "\n",
    "def process_answer(answer: str, state: Dict[str, Any]):\n",
    "    \"\"\"Processes an answer, shows the next question, or finishes the quiz.\"\"\"\n",
    "    # Save the last answer\n",
    "    current_question = state[\"questions\"][state[\"current_q_index\"]]\n",
    "    state[\"answers\"][current_question] = answer\n",
    "    \n",
    "    # Move to the next question\n",
    "    state[\"current_q_index\"] += 1\n",
    "    \n",
    "    if state[\"current_q_index\"] < len(state[\"questions\"]):\n",
    "        # Still more questions\n",
    "        next_question = state[\"questions\"][state[\"current_q_index\"]]\n",
    "        return (\n",
    "            gr.update(label=next_question),\n",
    "            gr.update(value=\"\"),\n",
    "            gr.update(), # No change to final report\n",
    "            gr.update(), # No change to buttons\n",
    "            gr.update(),\n",
    "            state\n",
    "        )\n",
    "    else:\n",
    "        # Finished questionnaire\n",
    "        report = agent.generate_report(state[\"name\"], state[\"answers\"])\n",
    "        save_status = agent.save_report(report, state[\"name\"])\n",
    "        final_display = f\"{report}\\n\\n{save_status}\"\n",
    "        \n",
    "        return (\n",
    "            gr.update(),\n",
    "            gr.update(),\n",
    "            gr.update(value=final_display, visible=True), # Show final report\n",
    "            gr.update(visible=False), # Hide question row\n",
    "            gr.update(visible=True),  # Show main buttons again\n",
    "            state\n",
    "        )\n",
    "\n",
    "def reset_ui():\n",
    "    \"\"\"Resets the UI to its initial state.\"\"\"\n",
    "    return (\n",
    "        gr.update(visible=True),  # Show New User\n",
    "        gr.update(visible=True),  # Show Update Profile\n",
    "        gr.update(visible=False), # Hide name row\n",
    "        gr.update(visible=False), # Hide question row\n",
    "        gr.update(visible=False, value=\"\") # Hide final report\n",
    "    )\n",
    "\n",
    "# --- Build the Gradio Interface using Blocks ---\n",
    "with gr.Blocks(theme=gr.themes.Soft(), css=\".gradio-container {background-color: #f5f5f5;}\") as demo:\n",
    "    gr.Markdown(\"# ðŸ¤– Agent 1: User Profiler\")\n",
    "    gr.Markdown(\"Create a new user profile or update an existing one. This helps the AI Tutor tailor its explanations to your skill level.\")\n",
    "    \n",
    "    # State object to hold session data\n",
    "    session_state = gr.State(value={})\n",
    "    \n",
    "    # --- UI Components ---\n",
    "    with gr.Row() as main_buttons_row:\n",
    "        new_user_btn = gr.Button(\"ðŸ‘‹ New User\", variant=\"primary\")\n",
    "        update_user_btn = gr.Button(\"ðŸ”„ Update Profile / Retake\", variant=\"secondary\")\n",
    "\n",
    "    with gr.Row(visible=False) as name_row:\n",
    "        name_input = gr.Textbox(label=\"What is your first and last name?\", placeholder=\"e.g., Jane Doe\")\n",
    "        name_submit_btn = gr.Button(\"Submit Name\")\n",
    "\n",
    "    with gr.Row(visible=False) as question_row:\n",
    "        answer_input = gr.Textbox(label=\"Your Answer\", placeholder=\"Type your answer here...\")\n",
    "        answer_submit_btn = gr.Button(\"Submit Answer\")\n",
    "        \n",
    "    final_report_display = gr.Markdown(visible=False)\n",
    "\n",
    "    # --- Event Handlers ---\n",
    "    new_user_btn.click(\n",
    "        fn=lambda: start_workflow(\"new\"),\n",
    "        outputs=[new_user_btn, update_user_btn, name_row, session_state]\n",
    "    )\n",
    "    update_user_btn.click(\n",
    "        fn=lambda: start_workflow(\"update\"),\n",
    "        outputs=[new_user_btn, update_user_btn, name_row, session_state]\n",
    "    )\n",
    "    \n",
    "    # Logic for when a name is submitted\n",
    "    name_submit_btn.click(\n",
    "        fn=process_name,\n",
    "        inputs=[name_input, session_state],\n",
    "        outputs=[name_row, question_row, answer_input, answer_input, session_state]\n",
    "    )\n",
    "    \n",
    "    # Logic for when an answer is submitted\n",
    "    answer_submit_btn.click(\n",
    "        fn=process_answer,\n",
    "        inputs=[answer_input, session_state],\n",
    "        outputs=[answer_input, answer_input, final_report_display, question_row, main_buttons_row, session_state]\n",
    "    ).then(\n",
    "        fn=reset_ui,\n",
    "        outputs=[new_user_btn, update_user_btn, name_row, question_row, final_report_display]\n",
    "    )\n",
    "\n",
    "# --- Launch the application ---\n",
    "# Set share=True to get a public link when running on Sol's Jupyter notebooks.\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True, debug=True)\n",
    "\n",
    "class UserProfilerAgent_Gradio:\n",
    "    \"\"\"\n",
    "    Agent 1 (Gradio Version): Handles the logic for profiling users via an LLM.\n",
    "    This class is designed to be called by the Gradio interface functions.\n",
    "    \"\"\"\n",
    "    def __init__(self, profiles_dir: str = \"user_profiles\"):\n",
    "        self.profiles_dir = profiles_dir\n",
    "        if not os.path.exists(self.profiles_dir):\n",
    "            os.makedirs(self.profiles_dir)\n",
    "\n",
    "        try:\n",
    "            host_node = socket.gethostname()\n",
    "            # IMPORTANT: Replace 'jgarc111' with the ASURITE ID of the user running the Ollama server on Sol.\n",
    "            self.llm = ChatOllama(model=\"qwen3:14b\", temperature=0, base_url=f\"http://jgarc111@{host_node}:11434/\")\n",
    "            print(\"âœ… Successfully connected to Ollama LLM.\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error connecting to Ollama: {e}\")\n",
    "            self.llm = None\n",
    "\n",
    "    def generate_questions(self) -> List[str]:\n",
    "        \"\"\"Uses an LLM to dynamically generate the questionnaire.\"\"\"\n",
    "        if not self.llm: return self._fallback_questions()\n",
    "\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            You are an assistant for an AI Data Science Tutor. Create a short questionnaire (4-5 questions)\n",
    "            to understand a user's knowledge of Python, data science libraries (NumPy, Pandas), and their\n",
    "            awareness of GPU computing (NVIDIA, CuPy, RAPIDS).\n",
    "            IMPORTANT: Do NOT ask for the user's name. Return a JSON list of questions.\n",
    "            \"\"\"\n",
    "        )\n",
    "        structured_llm = self.llm.with_structured_output(Questionnaire)\n",
    "        chain = prompt | structured_llm\n",
    "        try:\n",
    "            return chain.invoke({}).questions\n",
    "        except Exception:\n",
    "            return self._fallback_questions()\n",
    "\n",
    "    def _fallback_questions(self) -> List[str]:\n",
    "        \"\"\"Fallback questions if the LLM fails.\"\"\"\n",
    "        return [\n",
    "            \"On a scale of 1-5, how comfortable are you with Python?\",\n",
    "            \"Which Python data science libraries (like Pandas or NumPy) have you used before?\",\n",
    "            \"Have you ever heard of using GPUs to speed up data analysis?\",\n",
    "            \"What's the first tool you'd reach for to do a large matrix multiplication in Python?\"\n",
    "        ]\n",
    "\n",
    "    def generate_report(self, user_name: str, answers: Dict[str, str]) -> str:\n",
    "        \"\"\"Uses an LLM to analyze answers and generate a profile report.\"\"\"\n",
    "        if not self.llm: return \"Error: LLM not connected.\"\n",
    "\n",
    "        answers_str = \"\\n\".join([f\"- {q}: {a}\" for q, a in answers.items()])\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            You are an expert AI analyst. A user named {user_name} has answered a questionnaire.\n",
    "            Analyze their answers and generate a profile.\n",
    "\n",
    "            **User's Answers:**\n",
    "            {answers}\n",
    "\n",
    "            **Your Task:**\n",
    "            1.  Determine the user's knowledge level: 'Beginner', 'Intermediate', or 'Advanced'.\n",
    "            2.  Write a concise tutoring strategy for our AI Tutor based on this level.\n",
    "            \n",
    "            **Example for Intermediate:** Start with a strategy focusing on direct code comparisons (NumPy vs. CuPy) and performance benefits.\n",
    "            Provide your output as a JSON object with 'knowledge_level' and 'tutoring_strategy' fields.\n",
    "            \"\"\"\n",
    "        )\n",
    "        structured_llm = self.llm.with_structured_output(ProfileAnalysis)\n",
    "        chain = prompt | structured_llm\n",
    "        try:\n",
    "            analysis = chain.invoke({\"user_name\": user_name, \"answers\": answers_str})\n",
    "            report_content = f\"Knowledge Level: {analysis.knowledge_level}\\n\\nTUTORING STRATEGY: {analysis.tutoring_strategy}\"\n",
    "            full_report = f\"--- User Profile for {user_name} ---\\n{report_content}\\n--- End of Profile ---\"\n",
    "            return full_report\n",
    "        except Exception as e:\n",
    "            return f\"Error generating report: {e}\"\n",
    "\n",
    "    def save_report(self, report: str, user_name: str) -> str:\n",
    "        \"\"\"Saves the report to a file, overwriting if it exists.\"\"\"\n",
    "        filename = \"_\".join(user_name.lower().split()) + \".txt\"\n",
    "        filepath = os.path.join(self.profiles_dir, filename)\n",
    "        with open(filepath, \"w\") as f:\n",
    "            f.write(report)\n",
    "        return f\"âœ… Profile for {user_name} saved successfully to: {filepath}\"\n",
    "\n",
    "# --- Gradio UI Application ---\n",
    "\n",
    "# Instantiate the agent\n",
    "agent = UserProfilerAgent_Gradio()\n",
    "\n",
    "def start_workflow(mode: str):\n",
    "    \"\"\"Hides main buttons and shows the name input field.\"\"\"\n",
    "    initial_state = {\"mode\": mode, \"name\": \"\", \"questions\": [], \"answers\": {}, \"current_q_index\": -1}\n",
    "    return (\n",
    "        gr.update(visible=False), # Hide New User button\n",
    "        gr.update(visible=False), # Hide Update Profile button\n",
    "        gr.update(visible=True),  # Show name row\n",
    "        initial_state\n",
    "    )\n",
    "\n",
    "def process_name(name: str, state: Dict[str, Any]):\n",
    "    \"\"\"Processes the user's name and gets the questionnaire.\"\"\"\n",
    "    if not name.strip():\n",
    "        return gr.update(), gr.update(), gr.update(), gr.update(), state # No change if name is empty\n",
    "\n",
    "    state[\"name\"] = name\n",
    "    questions = agent.generate_questions()\n",
    "    state[\"questions\"] = questions\n",
    "    state[\"current_q_index\"] = 0\n",
    "    \n",
    "    return (\n",
    "        gr.update(visible=False), # Hide name row\n",
    "        gr.update(visible=True),  # Show question row\n",
    "        gr.update(label=questions[0]), # Update question label\n",
    "        gr.update(value=\"\"), # Clear answer textbox\n",
    "        state\n",
    "    )\n",
    "\n",
    "def process_answer(answer: str, state: Dict[str, Any]):\n",
    "    \"\"\"Processes an answer, shows the next question, or finishes the quiz.\"\"\"\n",
    "    # Save the last answer\n",
    "    current_question = state[\"questions\"][state[\"current_q_index\"]]\n",
    "    state[\"answers\"][current_question] = answer\n",
    "    \n",
    "    # Move to the next question\n",
    "    state[\"current_q_index\"] += 1\n",
    "    \n",
    "    if state[\"current_q_index\"] < len(state[\"questions\"]):\n",
    "        # Still more questions\n",
    "        next_question = state[\"questions\"][state[\"current_q_index\"]]\n",
    "        return (\n",
    "            gr.update(label=next_question),\n",
    "            gr.update(value=\"\"),\n",
    "            gr.update(), # No change to final report\n",
    "            gr.update(), # No change to buttons\n",
    "            gr.update(),\n",
    "            state\n",
    "        )\n",
    "    else:\n",
    "        # Finished questionnaire\n",
    "        report = agent.generate_report(state[\"name\"], state[\"answers\"])\n",
    "        save_status = agent.save_report(report, state[\"name\"])\n",
    "        final_display = f\"{report}\\n\\n{save_status}\"\n",
    "        \n",
    "        return (\n",
    "            gr.update(),\n",
    "            gr.update(),\n",
    "            gr.update(value=final_display, visible=True), # Show final report\n",
    "            gr.update(visible=False), # Hide question row\n",
    "            gr.update(visible=True),  # Show main buttons again\n",
    "            state\n",
    "        )\n",
    "\n",
    "def reset_ui():\n",
    "    \"\"\"Resets the UI to its initial state.\"\"\"\n",
    "    return (\n",
    "        gr.update(visible=True),  # Show New User\n",
    "        gr.update(visible=True),  # Show Update Profile\n",
    "        gr.update(visible=False), # Hide name row\n",
    "        gr.update(visible=False), # Hide question row\n",
    "        gr.update(visible=False, value=\"\") # Hide final report\n",
    "    )\n",
    "\n",
    "# --- Build the Gradio Interface using Blocks ---\n",
    "with gr.Blocks(theme=gr.themes.Soft(), css=\".gradio-container {background-color: #f5f5f5;}\") as demo:\n",
    "    gr.Markdown(\"# ðŸ¤– Agent 1: User Profiler\")\n",
    "    gr.Markdown(\"Create a new user profile or update an existing one. This helps the AI Tutor tailor its explanations to your skill level.\")\n",
    "    \n",
    "    # State object to hold session data\n",
    "    session_state = gr.State(value={})\n",
    "    \n",
    "    # --- UI Components ---\n",
    "    with gr.Row() as main_buttons_row:\n",
    "        new_user_btn = gr.Button(\"ðŸ‘‹ New User\", variant=\"primary\")\n",
    "        update_user_btn = gr.Button(\"ðŸ”„ Update Profile / Retake\", variant=\"secondary\")\n",
    "\n",
    "    with gr.Row(visible=False) as name_row:\n",
    "        name_input = gr.Textbox(label=\"What is your first and last name?\", placeholder=\"e.g., Jane Doe\")\n",
    "        name_submit_btn = gr.Button(\"Submit Name\")\n",
    "\n",
    "    with gr.Row(visible=False) as question_row:\n",
    "        answer_input = gr.Textbox(label=\"Your Answer\", placeholder=\"Type your answer here...\")\n",
    "        answer_submit_btn = gr.Button(\"Submit Answer\")\n",
    "        \n",
    "    final_report_display = gr.Markdown(visible=False)\n",
    "\n",
    "    # --- Event Handlers ---\n",
    "    new_user_btn.click(\n",
    "        fn=lambda: start_workflow(\"new\"),\n",
    "        outputs=[new_user_btn, update_user_btn, name_row, session_state]\n",
    "    )\n",
    "    update_user_btn.click(\n",
    "        fn=lambda: start_workflow(\"update\"),\n",
    "        outputs=[new_user_btn, update_user_btn, name_row, session_state]\n",
    "    )\n",
    "    \n",
    "    # Logic for when a name is submitted\n",
    "    name_submit_btn.click(\n",
    "        fn=process_name,\n",
    "        inputs=[name_input, session_state],\n",
    "        outputs=[name_row, question_row, answer_input, answer_input, session_state]\n",
    "    )\n",
    "    \n",
    "    # Logic for when an answer is submitted\n",
    "    answer_submit_btn.click(\n",
    "        fn=process_answer,\n",
    "        inputs=[answer_input, session_state],\n",
    "        outputs=[answer_input, answer_input, final_report_display, question_row, main_buttons_row, session_state]\n",
    "    ).then(\n",
    "        fn=reset_ui,\n",
    "        outputs=[new_user_btn, update_user_btn, name_row, question_row, final_report_display]\n",
    "    )\n",
    "\n",
    "# --- Launch the application ---\n",
    "# Set share=True to get a public link when running on Sol's Jupyter notebooks.\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9fe574-307a-4bea-88fc-fb74a729979e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai25.06",
   "language": "python",
   "name": "genai25.06"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
