{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45098534-76e6-4aa1-9131-b0cf1b5458af",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ff2cdb-7323-44f5-bec4-eafe098f0c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install googlesearch-python lxml deepspeed sentence-transformers \n",
    "# !pip install feedparser sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cb884c2-8e0d-459c-a2d2-07d56734e661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "#  \n",
    "# Cell 2: Imports\n",
    "#\n",
    "# Description: All required libraries for the application are imported here.\n",
    "#\n",
    "\n",
    "import os\n",
    "import json\n",
    "import socket\n",
    "from typing import List, Dict\n",
    "import subprocess\n",
    "import time\n",
    "import re\n",
    "import gradio as gr\n",
    "\n",
    "# LangChain and related libraries\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Search library\n",
    "try:\n",
    "    from googlesearch import search\n",
    "except ImportError:\n",
    "    print(\"Error: 'googlesearch-python' is not installed. Please run 'pip install googlesearch-python'\")\n",
    "\n",
    "# Set a user agent to avoid being blocked by Google search\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4351c5cb-e018-44cf-a752-b6fc9919a648",
   "metadata": {},
   "source": [
    "## Agent 1 - Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cdcd502-f05b-4322-b9c7-6d13ed1c802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Cell 3: Agent 1 - User Profiler\n",
    "#\n",
    "# Description: This agent is responsible for understanding the user's knowledge level.\n",
    "# It dynamically generates a questionnaire, analyzes the answers, and creates a\n",
    "# profile that will be used by Agent 3 to tailor its responses.\n",
    "#\n",
    "\n",
    "class Questionnaire(BaseModel):\n",
    "    questions: List[str] = Field(description=\"A list of 4-5 questions for the user.\")\n",
    "\n",
    "class UserProfilerAgent_V3:\n",
    "    \"\"\"\n",
    "    Agent 1 (V3): Guarantees the user's name is collected first before\n",
    "    using an LLM to dynamically generate the rest of the questionnaire.\n",
    "    **ADAPTED FOR MODULAR, GRADIO-FRIENDLY USE.**\n",
    "    \"\"\"\n",
    "    def __init__(self, profiles_dir: str = \"user_profiles\"):\n",
    "        self.profiles_dir = profiles_dir\n",
    "        if not os.path.exists(self.profiles_dir):\n",
    "            os.makedirs(self.profiles_dir)\n",
    "\n",
    "        try:\n",
    "            host_node = socket.gethostname()\n",
    "            # NOTE: The ASURITE ID should be that of the user running the Ollama server.\n",
    "            # This is specified as a hackathon resource.\n",
    "            asurite_id = \"apoojar4\"\n",
    "            self.llm = ChatOllama(model=\"qwen3:14b\", base_url=f\"http://{asurite_id}@{host_node}:11434/\")\n",
    "            self.structured_llm = self.llm.with_structured_output(Questionnaire)\n",
    "            print(\"✅ [Agent 1] Successfully connected to Ollama LLM.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ [Agent 1] Error connecting to Ollama: {e}\")\n",
    "            self.llm = None\n",
    "\n",
    "    def get_user_list(self) -> List[str]:\n",
    "        \"\"\"Scans the profiles directory and returns a list of user names.\"\"\"\n",
    "        if not os.path.exists(self.profiles_dir):\n",
    "            return []\n",
    "        files = [f for f in os.listdir(self.profiles_dir) if f.endswith('.txt')]\n",
    "        # Convert 'first_last.txt' to 'First Last'\n",
    "        names = [\" \".join(f.replace('.txt', '').split('_')).title() for f in files]\n",
    "        return names\n",
    "\n",
    "    def _generate_questions_with_llm(self) -> List[str]:\n",
    "        \"\"\"Uses an LLM to dynamically generate a user questionnaire.\"\"\"\n",
    "        if not self.llm: # Fallback if LLM is not available\n",
    "            return [\n",
    "                \"On a scale of 1-5, how comfortable are you with Python?\",\n",
    "                \"Which Python data science libraries (like Pandas or NumPy) have you used before?\",\n",
    "                \"Have you ever heard of using GPUs to speed up data analysis?\",\n",
    "                \"What's the first tool you'd reach for to do a large matrix multiplication in Python?\"\n",
    "            ]\n",
    "            \n",
    "        print(\"\\n🤖 [Agent 1] Generating a personalized questionnaire...\")\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"\"\"\n",
    "            You are a helpful assistant for an AI Data Science Tutor. Your goal is to create a short questionnaire (4-5 questions) to understand a user's knowledge level.\n",
    "            The questions should gently probe their experience with:\n",
    "            1. The Python programming language.\n",
    "            2. Common CPU-based data science libraries (like NumPy, Pandas).\n",
    "            3. Their awareness of GPU computing and hardware acceleration.\n",
    "            4. Their familiarity with any NVIDIA-specific GPU libraries (like CuPy or RAPIDS).\n",
    "            IMPORTANT: Do NOT ask for the user's name, as it will be collected separately.\n",
    "            Return the questions as a JSON list. Be conversational and friendly.\n",
    "            \"\"\",\n",
    "            input_variables=[],\n",
    "        )\n",
    "        query_generation_chain = prompt | self.structured_llm\n",
    "        try:\n",
    "            response_model = query_generation_chain.invoke({})\n",
    "            return response_model.questions\n",
    "        except Exception as e:\n",
    "            print(f\"-> [Agent 1] LLM failed to generate questions, falling back to default. Error: {e}\")\n",
    "            return [\n",
    "                \"On a scale of 1-5, how comfortable are you with Python?\",\n",
    "                \"Which Python data science libraries (like Pandas or NumPy) have you used before?\",\n",
    "                \"Have you ever heard of using GPUs to speed up data analysis?\",\n",
    "                \"What's the first tool you'd reach for to do a large matrix multiplication in Python?\"\n",
    "            ]\n",
    "\n",
    "    def generate_and_save_report(self, user_name: str, answers_dict: dict) -> str:\n",
    "        \"\"\"\n",
    "        Takes a user name and a dictionary of answers, generates a report with an LLM,\n",
    "        and saves it to a file. Returns the path to the saved file.\n",
    "        \"\"\"\n",
    "        if not self.llm:\n",
    "            return \"Error: LLM not connected.\"\n",
    "\n",
    "        print(f\"\\n🤖 [Agent 1] Analyzing responses for {user_name} and creating a profile...\")\n",
    "        answers_str = \"\\n\".join([f\"- {q}: {a}\" for q, a in answers_dict.items()])\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"\"\"\n",
    "            You are an expert AI analyst. A user named {user_name} has answered a questionnaire about their data science skills.\n",
    "            Your task is to analyze their answers and generate a \"TUTORING STRATEGY\" report for our AI Tutor.\n",
    "            **User's Answers:**\n",
    "            {answers}\n",
    "            **Your Task:**\n",
    "            1.  Determine the user's knowledge level: 'Beginner', 'Intermediate', or 'Advanced'.\n",
    "            2.  Write a concise report following the correct strategy format below. This report will be given to another AI, so the instructions must be clear.\n",
    "            ---\n",
    "            **STRATEGY FORMATS (Choose ONE):**\n",
    "            **If 'Beginner':**\n",
    "            Start with `Knowledge Level: Beginner`. On the next line, start with `TUTORING STRATEGY: The user is a beginner.` Then, explain that the tutor should use high-level concepts, explain the 'why' of GPU acceleration, and introduce NVIDIA libraries (like CuPy) as a simple, powerful alternative.\n",
    "            **If 'Intermediate':**\n",
    "            Start with `Knowledge Level: Intermediate`. On the next line, start with `TUTORING STRATEGY: The user is at an intermediate level.` Then, explain that the tutor should provide direct code comparisons (e.g., NumPy vs. CuPy), focus on performance benefits, and show clear benchmarking examples.\n",
    "            **If 'Advanced':**\n",
    "            Start with `Knowledge Level: Advanced`. On the next line, start with `TUTORING STRATEGY: The user is advanced.` Then, explain that the tutor can provide nuanced advice, discuss the broader NVIDIA RAPIDS ecosystem, and cover specific benchmarking methodologies on the Sol supercomputer.\n",
    "            ---\n",
    "            Now, generate the complete report.\n",
    "            \"\"\",\n",
    "            input_variables=[\"user_name\", \"answers\"],\n",
    "        )\n",
    "        report_generation_chain = prompt | self.llm\n",
    "        response_message = report_generation_chain.invoke({\"user_name\": user_name, \"answers\": answers_str})\n",
    "        report_content = response_message.content\n",
    "        full_report = f\"--- User Profile for {user_name} ---\\n{report_content}\\n--- End of Profile ---\"\n",
    "\n",
    "        # Save the report to a text file\n",
    "        filename = \"_\".join(user_name.lower().split()) + \".txt\"\n",
    "        filepath = os.path.join(self.profiles_dir, filename)\n",
    "        with open(filepath, \"w\") as f:\n",
    "            f.write(full_report)\n",
    "        print(f\"\\n✅ [Agent 1] User profile report saved successfully to: {filepath}\")\n",
    "        return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eadc4b8-19c6-401e-ab23-ff3977c29f9b",
   "metadata": {},
   "source": [
    "## Agent 4 - Benchmarker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d122e961-c4af-4a01-9299-0e419a6b1412",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolBenchmarker:\n",
    "    def __init__(self, user: str, python_env: str = \"rapids25.02\"):\n",
    "        if not user or user == \"YOUR_ASURITE_ID\":\n",
    "            raise ValueError(\"A valid ASURITE username is required for SolBenchmarker.\")\n",
    "        self.user = user\n",
    "        self.python_env = python_env\n",
    "\n",
    "    def _generate_sbatch_script(self, script_dir: str, script_name: str, job_type: str) -> str:\n",
    "        # This function now generates a specific script for either a CPU or GPU job\n",
    "        \n",
    "        # if job_type == \"gpu\":\n",
    "        #     # The GPU job requests a GPU resource\n",
    "        #     resources = \"#SBATCH -G 1\"\n",
    "        # else: # cpu\n",
    "        #     # The CPU job requests a standard node without a GPU\n",
    "        #     resources = \"#SBATCH --nodes=1\" \n",
    "\n",
    "        return f\"\"\"#!/bin/bash\n",
    "#SBATCH -p general\n",
    "#SBATCH -q public\n",
    "#SBATCH -G 1\n",
    "#SBATCH -A grp_hackathon2025\n",
    "#SBATCH --reservation=hackathon2025\n",
    "#SBATCH -t 0-00:10:00\n",
    "#SBATCH -c 1\n",
    "#SBATCH -o {script_dir}/slurm-{job_type}-%j.out\n",
    "#SBATCH -e {script_dir}/slurm-{job_type}-%j.err\n",
    "\n",
    "module load mamba/latest\n",
    "source activate {self.python_env}\n",
    "\n",
    "echo \"--- STARTING {job_type.upper()} BENCHMARK ---\"\n",
    "/usr/bin/time -p python3 {script_dir}/{script_name} 2>&1\n",
    "echo \"--- FINISHED {job_type.upper()} BENCHMARK ---\"\n",
    "\"\"\"\n",
    "\n",
    "    def _submit_job(self, code: str, job_type: str, benchmark_dir: str) -> str:\n",
    "        # Helper to submit a single job and return its ID\n",
    "        script_name = f\"{job_type}_benchmark.py\"\n",
    "        script_path = os.path.join(benchmark_dir, script_name)\n",
    "        sbatch_path = os.path.join(benchmark_dir, f\"{job_type}_job.sh\")\n",
    "        \n",
    "        with open(script_path, \"w\") as f: f.write(code)\n",
    "        sbatch_content = self._generate_sbatch_script(benchmark_dir, script_name, job_type)\n",
    "        with open(sbatch_path, \"w\") as f: f.write(sbatch_content)\n",
    "\n",
    "        process = subprocess.run(f\"sbatch {sbatch_path}\", shell=True, capture_output=True, text=True)\n",
    "        if process.returncode != 0: raise RuntimeError(f\"sbatch submission for {job_type} failed: {process.stderr}\")\n",
    "        \n",
    "        job_id_match = re.search(r\"Submitted batch job (\\d+)\", process.stdout.strip())\n",
    "        if not job_id_match: raise RuntimeError(f\"Could not parse Job ID for {job_type}: {process.stdout}\")\n",
    "        \n",
    "        job_id = job_id_match.group(1)\n",
    "        print(f\"--> [Agent 4] Submitted {job_type.upper()} job with ID: {job_id}\")\n",
    "        return job_id\n",
    "\n",
    "    # REPLACE THIS FUNCTION INSIDE THE SolBenchmarker CLASS IN CELL 4\n",
    "\n",
    "    def _check_job_completion(self, job_id: str, job_type: str, benchmark_dir: str) -> dict:\n",
    "        \"\"\"\n",
    "        Helper to check if a job is done using the robust 'sacct' command.\n",
    "        This avoids the race condition present with 'squeue'.\n",
    "        \"\"\"\n",
    "        # Command to get the state of the specific job, with no header.\n",
    "        command = f\"sacct -j {job_id} -o State --noheader\"\n",
    "        process = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "        \n",
    "        # The output can have multiple lines (e.g., for each step), so we check the first.\n",
    "        status_lines = [line.strip() for line in process.stdout.splitlines() if line.strip()]\n",
    "        \n",
    "        if not status_lines:\n",
    "            # If sacct returns nothing, the job might still be initializing. Treat as running.\n",
    "            return {\"status\": \"running\"}\n",
    "\n",
    "        # Get the primary status, removing \"(See...\" if present\n",
    "        primary_status = status_lines[0].split()[0]\n",
    "        \n",
    "        if primary_status in [\"PENDING\", \"RUNNING\", \"CONFIGURING\"]:\n",
    "            return {\"status\": \"running\"}\n",
    "        \n",
    "        # If the status is anything else, the job is considered finished.\n",
    "        print(f\"--> [Agent 4] {job_type.upper()} Job {job_id} finished with status: {primary_status}\")\n",
    "        \n",
    "        if primary_status == \"COMPLETED\":\n",
    "            # Add a small delay to ensure the output file has been fully written by the filesystem\n",
    "            time.sleep(2)\n",
    "            output_file_path = os.path.join(benchmark_dir, f\"slurm-{job_type}-{job_id}.out\")\n",
    "            if os.path.exists(output_file_path):\n",
    "                with open(output_file_path, \"r\") as f:\n",
    "                    output_content = f.read()\n",
    "                real_time_match = re.search(r\"real\\s+([\\d.]+)\", output_content)\n",
    "                time_val = float(real_time_match.group(1)) if real_time_match else None\n",
    "                return {\"status\": \"complete\", \"job_type\": job_type, \"time\": time_val}\n",
    "            else:\n",
    "                return {\"status\": \"error\", \"job_type\": job_type, \"message\": \"Job COMPLETED but output file not found.\"}\n",
    "        else: # FAILED, CANCELLED, TIMEOUT, etc.\n",
    "            return {\"status\": \"error\", \"job_type\": job_type, \"message\": f\"Job finished with non-COMPLETED status: {primary_status}\"}\n",
    "\n",
    "#         --> [Agent 4] GPU Job 28620141 completed.\n",
    "# --> [Agent 4] CPU Job 28620142 completed.\n",
    "\n",
    "    def run_benchmark_parallel(self, cpu_code: str, gpu_code: str):\n",
    "        # The main method, now a generator that yields results as they complete.\n",
    "        benchmark_dir = os.path.join(os.getcwd(), \"benchmark_files\")\n",
    "        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            gpu_job_id = self._submit_job(gpu_code, \"gpu\", benchmark_dir)\n",
    "            cpu_job_id = self._submit_job(cpu_code, \"cpu\", benchmark_dir)\n",
    "\n",
    "            active_jobs = {\"gpu\": gpu_job_id, \"cpu\": cpu_job_id}\n",
    "\n",
    "            while active_jobs:\n",
    "                # Check the status of each active job\n",
    "                for job_type, job_id in list(active_jobs.items()):\n",
    "                    result = self._check_job_completion(job_id, job_type, benchmark_dir)\n",
    "                    if result[\"status\"] != \"running\":\n",
    "                        yield result # Yield the result as soon as a job finishes\n",
    "                        del active_jobs[job_type] # Remove from active monitoring\n",
    "                \n",
    "                if active_jobs:\n",
    "                    time.sleep(5) # Wait before polling again\n",
    "\n",
    "        except Exception as e:\n",
    "            yield {\"status\": \"error\", \"message\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67cd2448-7084-4dec-a1ae-645cafad0657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# # Cell 4: Agent 4 - Sol Benchmarker\n",
    "# #\n",
    "# # Description: This agent is responsible for executing code on the Sol supercomputer\n",
    "# # to benchmark the performance difference between CPU and GPU implementations. It writes\n",
    "# # and submits a SLURM batch script.\n",
    "# #\n",
    "\n",
    "# class SolBenchmarker:\n",
    "#     def __init__(self, user: str, python_env: str = \"rapids25.02\"): # Note: Kernel name from docs\n",
    "#         if not user or user == \"YOUR_ASURITE_ID\":\n",
    "#             raise ValueError(\"A valid ASURITE username is required for SolBenchmarker.\")\n",
    "#         self.user = user\n",
    "#         self.python_env = python_env\n",
    "\n",
    "#     def _generate_sbatch_script(self, script_dir: str, cpu_script_name: str, gpu_script_name: str) -> str:\n",
    "#         # This SLURM script is configured according to the hackathon's resources \n",
    "#         return f\"\"\"#!/bin/bash\n",
    "# #SBATCH -p general\n",
    "# #SBATCH -q public\n",
    "# #SBATCH -G 1\n",
    "# #SBATCH -A grp_hackathon2025\n",
    "# #SBATCH --reservation=hackathon2025\n",
    "# #SBATCH -t 0-00:10:00\n",
    "# #SBATCH -c 1\n",
    "# #SBATCH -o {script_dir}/slurm-%j.out\n",
    "# #SBATCH -e {script_dir}/slurm-%j.err\n",
    "\n",
    "# module load mamba/latest\n",
    "# source activate {self.python_env}\n",
    "\n",
    "# echo \"--- STARTING CPU BENCHMARK ---\"\n",
    "# /usr/bin/time -p python3 {script_dir}/{cpu_script_name} 2>&1\n",
    "# echo \"--- FINISHED CPU BENCHMARK ---\"\n",
    "\n",
    "# echo \"\"\n",
    "# echo \"--- STARTING GPU BENCHMARK ---\"\n",
    "# /usr/bin/time -p python3 {script_dir}/{gpu_script_name} 2>&1\n",
    "# echo \"--- FINISHED GPU BENCHMARK ---\"\n",
    "# \"\"\"\n",
    "#     def _parse_output(self, output_content: str) -> dict:\n",
    "#         try:\n",
    "#             # --- FIX 1: Removed erroneous backslash before the string literal ---\n",
    "#             # This regex finds the execution time from the '/usr/bin/time' command output.\n",
    "#             real_times = re.findall(r\"real\\s+([\\d.]+)\", output_content)\n",
    "            \n",
    "#             cpu_time = float(real_times[0]) if len(real_times) > 0 else None\n",
    "#             gpu_time = float(real_times[1]) if len(real_times) > 1 else None\n",
    "#             print(\"cpu_time_seconds\", cpu_time, \"gpu_time_seconds\", gpu_time)\n",
    "#             return {\"status\": \"success\", \"cpu_time_seconds\": cpu_time, \"gpu_time_seconds\": gpu_time}\n",
    "#         except (IndexError, ValueError) as e:\n",
    "#             return {\"status\": \"error\", \"message\": f\"Failed to parse benchmark times. Error: {e}\", \"raw_log\": output_content}\n",
    "\n",
    "#     def run_benchmark(self, cpu_code: str, gpu_code: str) -> dict:\n",
    "#         benchmark_dir = os.path.join(os.getcwd(), \"benchmark_files\")\n",
    "#         os.makedirs(benchmark_dir, exist_ok=True)\n",
    "        \n",
    "#         cpu_script_path = os.path.join(benchmark_dir, \"cpu_benchmark.py\")\n",
    "#         gpu_script_path = os.path.join(benchmark_dir, \"gpu_benchmark.py\")\n",
    "#         sbatch_path = os.path.join(benchmark_dir, \"benchmark_job.sh\")\n",
    "\n",
    "#         try:\n",
    "#             with open(cpu_script_path, \"w\") as f: f.write(cpu_code)\n",
    "#             with open(gpu_script_path, \"w\") as f: f.write(gpu_code)\n",
    "#             sbatch_script = self._generate_sbatch_script(benchmark_dir, \"cpu_benchmark.py\", \"gpu_benchmark.py\")\n",
    "#             with open(sbatch_path, \"w\") as f: f.write(sbatch_script)\n",
    "\n",
    "#             process = subprocess.run(f\"sbatch {sbatch_path}\", shell=True, capture_output=True, text=True)\n",
    "#             if process.returncode != 0: raise RuntimeError(f\"sbatch submission failed: {process.stderr}\")\n",
    "\n",
    "#             # --- FIX 2: Removed erroneous backslash before the string literal ---\n",
    "#             # This regex finds the Job ID from the sbatch submission output.\n",
    "#             job_id_match = re.search(r\"Submitted batch job (\\d+)\", process.stdout.strip())\n",
    "\n",
    "#             if not job_id_match: raise RuntimeError(f\"Could not parse Job ID from sbatch output: {process.stdout}\")\n",
    "#             job_id = job_id_match.group(1)\n",
    "#             print(f\"--> [Agent 4] Submitted benchmark job to SLURM with ID: {job_id}\")\n",
    "\n",
    "#             print(\"--> [Agent 4] Waiting for job to complete...\")\n",
    "#             while True:\n",
    "#                 queue_process = subprocess.run(f\"squeue -u {self.user} -j {job_id}\", shell=True, capture_output=True, text=True)\n",
    "#                 if job_id not in queue_process.stdout: break\n",
    "#                 time.sleep(10)\n",
    "\n",
    "#             print(f\"--> [Agent 4] Job {job_id} completed.\")\n",
    "#             output_file_path = os.path.join(benchmark_dir, f\"slurm-{job_id}.out\")\n",
    "            \n",
    "#             if not os.path.exists(output_file_path):\n",
    "#                  err_file_path = os.path.join(benchmark_dir, f\"slurm-{job_id}.err\")\n",
    "#                  if os.path.exists(err_file_path):\n",
    "#                      with open(err_file_path, \"r\") as f: error_content = f.read()\n",
    "#                      return {\"status\": \"error\", \"message\": f\"Job failed. See error log: {error_content}\"}\n",
    "#                  return {\"status\": \"error\", \"message\": f\"Output file not found.\"}\n",
    "\n",
    "#             with open(output_file_path, \"r\") as f: output_content = f.read()\n",
    "#             return self._parse_output(output_content)\n",
    "#         except Exception as e:\n",
    "#             return {\"status\": \"error\", \"message\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dd130d8-4e78-48ce-9ce5-6bd897dbd2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEW: Code Upscaler Function with Strict Constraints ---\n",
    "def _upscale_code_agentic(original_code: str, llm, library_string: str) -> str:\n",
    "    print(\"--> [Agent 3] Invoking LLM with STRICT CONSTRAINTS to upscale code...\")\n",
    "    \n",
    "    # This class is used to ensure the LLM returns only a code string.\n",
    "    class CodeOutput(BaseModel):\n",
    "        code: str = Field(description=\"The complete, upscaled Python code.\")\n",
    "        \n",
    "    # THE NEW, MORE SPECIFIC PROMPT\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are a benchmark preparation assistant. Your task is to analyze the following Python script and make it suitable for a meaningful performance benchmark.\n",
    "\n",
    "        **Your instructions are very strict:**\n",
    "        1.  **DO NOT** change any of the `import` statements.\n",
    "        2.  **ONLY** find the variable that defines the primary data size (e.g., `size = 10`, `n_rows = 1000`).\n",
    "        3.  Change the value of that single variable to a much larger number to ensure the benchmark is meaningful. For example, change matrix dimensions to at least 8000 or DataFrame rows to over 5 million.\n",
    "        4.  Return the complete, modified Python script and nothing else.\n",
    "\n",
    "        **Constraint:** The final code must only use libraries from this list: {available_libraries}\n",
    "\n",
    "        Original Code:\n",
    "        ```python\n",
    "        {original_code}\n",
    "        ```\n",
    "\n",
    "        Upscaled Code:\n",
    "        \"\"\",\n",
    "        input_variables=[\"original_code\", \"available_libraries\"],\n",
    "    )\n",
    "    \n",
    "    # The chain now uses the new, stricter prompt\n",
    "    structured_chain = prompt_template | llm.with_structured_output(CodeOutput)\n",
    "    \n",
    "    try:\n",
    "        response_model = structured_chain.invoke({\n",
    "            \"original_code\": original_code,\n",
    "            \"available_libraries\": library_string\n",
    "        })\n",
    "        upscaled_code = response_model.code\n",
    "        print(\"--> [Agent 3] Code successfully upscaled.\")\n",
    "        return upscaled_code\n",
    "    except Exception as e:\n",
    "        print(f\"--> [Agent 3] Could not upscale code, using original. Error: {e}\")\n",
    "        # Fallback to original code if upscaling fails\n",
    "        return original_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d282d80-7f9b-4ed0-9714-2430235e8602",
   "metadata": {},
   "source": [
    "## Agent 2 - Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e908618-cce9-4711-b02f-daba7f1fdbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# AGENTS 2 & 3 HELPER FUNCTIONS\n",
    "# This cell contains all the supporting logic for retrieval and code processing.\n",
    "# =================================================================\n",
    "from sentence_transformers import util\n",
    "import feedparser\n",
    "\n",
    "class SearchQueryGenerator(BaseModel):\n",
    "    queries: List[str] = Field(description=\"A list of targeted, keyword-focused search queries.\")\n",
    "\n",
    "def generate_search_queries(query: str, llm) -> List[str]:\n",
    "    print(\"-> [Agent 2] Using LLM to generate search queries...\")\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are an expert at generating web search queries. Analyze the user's question to identify the core technical task and programming language.\n",
    "        Generate 3 concise, targeted search queries. One query should be for the standard, CPU-based approach. Two queries should be for GPU-accelerated approaches, prioritizing NVIDIA-based solutions if they exist.\n",
    "        User Question: \"{question}\"\n",
    "        Generate a JSON list of 3 search query strings.\n",
    "        \"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "    query_generation_chain = prompt_template | llm.with_structured_output(SearchQueryGenerator)\n",
    "    try:\n",
    "        response_model = query_generation_chain.invoke({\"question\": query})\n",
    "        print(f\"-> [Agent 2] Generated queries: {response_model.queries}\")\n",
    "        return response_model.queries\n",
    "    except Exception as e:\n",
    "        print(f\"-> [Agent 2] LLM failed to generate structured output: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- NEW: Hybrid Retriever Functions (Tier 1, 2, 3) ---\n",
    "\n",
    "def fetch_articles_from_rss(query: str, embedding_model) -> list[str]:\n",
    "    print(\"-> [Tier 1] Trying Semantic Search on RSS feeds...\")\n",
    "    RSS_FEEDS = {\n",
    "        \"NVIDIA Developer Blog\": \"https://developer.nvidia.com/blog/feed/\",\n",
    "        \"RAPIDS AI (Medium)\": \"https://medium.com/feed/rapids-ai\",\n",
    "        \"CuPy (Medium)\": \"https://medium.com/feed/cupy-team\"\n",
    "    }\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    found_articles = []\n",
    "    for name, url in RSS_FEEDS.items():\n",
    "        feed = feedparser.parse(url)\n",
    "        for entry in feed.entries:\n",
    "            entry_text = entry.title + \". \" + entry.summary\n",
    "            entry_embedding = embedding_model.embed_query(entry_text)\n",
    "            similarity = util.pytorch_cos_sim(query_embedding, entry_embedding)\n",
    "            if similarity.item() > 0.35: # Keep a minimal threshold\n",
    "                found_articles.append({'link': entry.link, 'similarity': similarity.item()})\n",
    "    \n",
    "    found_articles.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    final_urls = [article['link'] for article in found_articles[:5]]\n",
    "    print(f\"-> [Tier 1] Found {len(final_urls)} semantically relevant URLs from RSS.\")\n",
    "    return final_urls\n",
    "\n",
    "def dynamic_web_search(queries: List[str]) -> list[str]:\n",
    "    print(\"-> [Tier 2] Falling back to polite Web Search...\")\n",
    "    all_urls = set()\n",
    "    for q in queries:\n",
    "        try:\n",
    "            search_results = list(search(q, num_results=2))\n",
    "            all_urls.update(url for url in search_results if url)\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during web search: {e}\")\n",
    "            continue\n",
    "    return list(all_urls)\n",
    "\n",
    "def hybrid_retriever(query: str, llm, embedding_model) -> list[str]:\n",
    "    # 1. First, try the high-quality RSS feeds\n",
    "    urls = fetch_articles_from_rss(query, embedding_model)\n",
    "\n",
    "    # 2. If RSS finds nothing, fallback to a general web search\n",
    "    if not urls:\n",
    "        print(\"--> RSS search found no relevant articles. Falling back to web search.\")\n",
    "        search_queries = generate_search_queries(query, llm)\n",
    "        if search_queries:\n",
    "            urls = dynamic_web_search(search_queries)\n",
    "\n",
    "    # 3. If both searches fail, use a final safety net of default links\n",
    "    if not urls:\n",
    "        print(\"--> All searches failed. Using default fallback links.\")\n",
    "        urls = [\n",
    "            \"https://developer.nvidia.com/blog/icymi-leveraging-the-power-of-gpus-with-cupy-in-python/\",\n",
    "            \"https://medium.com/rapids-ai/rapids-23-08-release-23db51c255f0\"\n",
    "        ]\n",
    "        \n",
    "    print(f\"-> [Hybrid Retriever] Found {len(urls)} final URLs to use for context.\")\n",
    "    return urls\n",
    "\n",
    "# --- Other Helper Functions ---\n",
    "def _get_available_libraries(filepath: str) -> List[str]:\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"--> Library file not found at {filepath}. Using a default list.\")\n",
    "        return ['numpy', 'cupy', 'pandas', 'cudf', 'cuml', 'scipy', 'sklearn']\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    libraries = []\n",
    "    for line in lines:\n",
    "        if not line.startswith('#'):\n",
    "            package_name = line.split()[0]\n",
    "            libraries.append(package_name)\n",
    "    print(f\"--> Successfully loaded {len(libraries)} available libraries from file.\")\n",
    "    return libraries\n",
    "\n",
    "# DEFINITIVE, \"PROCESS OF ELIMINATION\" CODE EXTRACTOR\n",
    "def _extract_python_code(markdown_text: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    The definitive helper function to parse CPU and GPU code blocks. It uses\n",
    "    a \"process of elimination\" to robustly identify the CPU and GPU code.\n",
    "    \"\"\"\n",
    "    print(\"--> [Agent 3] Extracting code blocks using definitive 'Process of Elimination' parser...\")\n",
    "    \n",
    "    cpu_code = \"\"\n",
    "    gpu_code = \"\"\n",
    "    \n",
    "    # This pattern finds all sections that have a \"Solution\" heading and a python code block\n",
    "    pattern = re.compile(\n",
    "        r\"(###?\\s*.*?Solution.*?)\\n*```python\\n(.*?)\\n```\", \n",
    "        re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    matches = pattern.findall(markdown_text)\n",
    "    \n",
    "    # This list will hold any code that isn't explicitly a GPU solution\n",
    "    other_code_blocks = []\n",
    "    \n",
    "    for heading, code in matches:\n",
    "        # First, positively identify the GPU block\n",
    "        if \"gpu\" in heading.lower():\n",
    "            gpu_code = code.strip()\n",
    "        else:\n",
    "            # If it's not a GPU block, add it to a temporary list\n",
    "            other_code_blocks.append(code.strip())\n",
    "\n",
    "    # If we found a GPU block AND there's another block left over,\n",
    "    # that other block must be the CPU solution.\n",
    "    if gpu_code and other_code_blocks:\n",
    "        cpu_code = other_code_blocks[0]\n",
    "    # If we only found one block and it wasn't the GPU one, it must be the CPU one.\n",
    "    elif not gpu_code and other_code_blocks:\n",
    "        cpu_code = other_code_blocks[0]\n",
    "            \n",
    "    # Your helpful debugging prints\n",
    "    print(\"################################################\")\n",
    "    print(\"GPU Code Found:\\n\", gpu_code)\n",
    "    print(\"################################################\")\n",
    "    print(\"CPU Code Found:\\n\", cpu_code)\n",
    "    print(\"################################################\")\n",
    "\n",
    "    return {\"cpu_code\": cpu_code, \"gpu_code\": gpu_code}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a993605-224d-4f4d-a280-2e16bc70a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resp = \"\"\"### Standard Solution (Using NumPy)\n",
    "# To multiply three 100x100 matrices in Python, you can use **NumPy**, a foundational library for numerical computing. Here's how:\n",
    "\n",
    "# ```python\n",
    "# import numpy as np\n",
    "\n",
    "# # Create three 100x100 matrices with random values\n",
    "# A = np.random.rand(100, 100)\n",
    "# B = np.random.rand(100, 100)\n",
    "# C = np.random.rand(100, 100)\n",
    "\n",
    "# # Multiply them sequentially\n",
    "# result = A @ B @ C  # Equivalent to np.dot(np.dot(A, B), C)\n",
    "# ```\n",
    "\n",
    "# **Key Notes:**\n",
    "# - Matrix multiplication is **associative** but **not commutative**, so the order of operations matters.\n",
    "# - NumPy ensures efficient computation using optimized C-based routines under the hood.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### Recommended GPU Solution (Using CuPy)\n",
    "# If you have access to an NVIDIA GPU, **CuPy** provides a GPU-accelerated alternative to NumPy, drastically improving performance for large-scale matrix operations:\n",
    "\n",
    "# ```python\n",
    "# import cupy as cp\n",
    "\n",
    "# # Create GPU matrices\n",
    "# A = cp.random.rand(100, 100)\n",
    "# B = cp.random.rand(100, 100)\n",
    "# C = cp.random.rand(100, 100)\n",
    "\n",
    "# # Multiply them on the GPU\n",
    "# result = A @ B @ C  # GPU-accelerated computation\n",
    "# ```\n",
    "\n",
    "# **Performance Note:**\n",
    "# - CuPy leverages CUDA to perform computations on the GPU, which can be **orders of magnitude faster** than CPU-based NumPy for large matrices.\n",
    "# - This is especially impactful for deep learning, scientific computing, or applications requiring repeated matrix operations.\n",
    "\n",
    "# ---\"\"\"\n",
    "# extracted_code = _extract_python_code(resp)\n",
    "# cpu_code, gpu_code = extracted_code[\"cpu_code\"], extracted_code[\"gpu_code\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8345a9-6913-4473-b7ec-e9bf3cf0320e",
   "metadata": {},
   "source": [
    "## Agent N - Router Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a9221fe-6a41-41ed-a530-6eef03c49553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Cell 8, REPLACE the RouterAgent class\n",
    "\n",
    "class RouterAgent:\n",
    "    \"\"\"A more precise agent to classify user intent and choose an execution path.\"\"\"\n",
    "    \n",
    "    class Path(BaseModel):\n",
    "        path_name: str = Field(description=\"The chosen execution path. Must be one of: 'CONVERSATIONAL', 'FOLLOW_UP', or 'COMPLEX_QUERY'.\")\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        self.prompt = PromptTemplate(\n",
    "            template=\"\"\"\n",
    "            You are a routing agent. Your job is to classify the user's latest message by following these rules in order:\n",
    "\n",
    "            1.  First, check for simple \"chit-chat\". If the message is a greeting (hi, hello), a thank you, a goodbye, or asks about you (\"who are you?\", \"what can you do?\"), choose the path 'CONVERSATIONAL'.\n",
    "\n",
    "            2.  If it's not chit-chat, analyze the CONVERSATION HISTORY. If the latest message is short (e.g., \"why?\", \"can you explain that differently?\") AND it directly refers to the AI's immediately preceding answer, choose the path 'FOLLOW_UP'.\n",
    "\n",
    "            3.  If it's none of the above, it is a new technical question. Choose the path 'COMPLEX_QUERY'.\n",
    "\n",
    "            CONVERSATION HISTORY:\n",
    "            {chat_history}\n",
    "\n",
    "            USER'S LATEST MESSAGE:\n",
    "            {question}\n",
    "\n",
    "            Chosen Path:\n",
    "            \"\"\",\n",
    "            input_variables=[\"chat_history\", \"question\"],\n",
    "        )\n",
    "        self.chain = self.prompt | llm.with_structured_output(self.Path)\n",
    "\n",
    "    def choose_path(self, query: str, chat_history: List[Dict]) -> str:\n",
    "        # This function's logic remains the same\n",
    "        history_str = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
    "        try:\n",
    "            result = self.chain.invoke({\"chat_history\": history_str, \"question\": query})\n",
    "            print(f\"--> [Router Agent] Chose path: {result.path_name}\")\n",
    "            return result.path_name\n",
    "        except Exception as e:\n",
    "            print(f\"--> [Router Agent] Error, defaulting to complex query. {e}\")\n",
    "            return \"COMPLEX_QUERY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b07ac0d-0378-4122-9bbe-969f6bf7e83b",
   "metadata": {},
   "source": [
    "## Agent 3 - Synthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8efa7e56-1550-45bd-9594-b9e60c84ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE process_with_rag IN CELL 8 WITH THESE TWO FUNCTIONS\n",
    "# ADD THIS NEW FUNCTION TO CELL 8\n",
    "\n",
    "# In Cell 8, ADD this new function\n",
    "\n",
    "# In Cell 8, REPLACE the run_conversational_response function\n",
    "\n",
    "def run_conversational_response(query: str, llm):\n",
    "    \"\"\"\n",
    "    Handles non-technical questions by referencing a persona document.\n",
    "    \"\"\"\n",
    "    print(\"--> [Conversational Path] Generating a persona-driven response.\")\n",
    "    \n",
    "    AGENT_PERSONA = \"\"\"\n",
    "    Your Identity: You are a helpful and enthusiastic AI Data Science Tutor.\n",
    "    Your Purpose: You were created by a team of developers for the AI Accelerated Spark Challenge, a hackathon hosted by ASU and sponsored by NVIDIA. Your primary mission is to help users learn how to speed up their data science work using NVIDIA GPU acceleration.\n",
    "    Your Capabilities:\n",
    "    - You can answer theoretical questions about data science and GPU computing.\n",
    "    - You can provide code examples for procedural tasks, always prioritizing NVIDIA-based libraries (like CuPy and RAPIDS).\n",
    "    - For procedural questions, you can even run live performance benchmarks on the ASU Sol supercomputer.\n",
    "    - You can personalize your explanations based on the user's knowledge level.\n",
    "    Your Limitations: You cannot answer questions outside of data science and technology. You do not have personal opinions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # THE FIX IS HERE: Added a negative constraint to the prompt.\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are a helpful and enthusiastic AI Tutor. Your identity and capabilities are defined below.\n",
    "        Based on this persona and the user's message, provide a short, friendly, and helpful response.\n",
    "\n",
    "        **IMPORTANT**: For this simple conversational reply, you MUST NOT include any `<think>` or `</think>` tags in your output. Respond directly.\n",
    "\n",
    "        ---\n",
    "        AGENT PERSONA DOCUMENT:\n",
    "        {persona}\n",
    "        ---\n",
    "\n",
    "        USER'S MESSAGE:\n",
    "        {question}\n",
    "\n",
    "        YOUR CONVERSATIONAL RESPONSE:\n",
    "        \"\"\",\n",
    "        input_variables=[\"persona\", \"question\"]\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"persona\": AGENT_PERSONA, \"question\": query})\n",
    "    print(response)\n",
    "    yield {\"status\": \"llm_complete\", \"content\": response.content}\n",
    "\n",
    "def run_simple_follow_up(query: str, chat_history, llm):\n",
    "    \"\"\"Handles simple follow-up questions without search or benchmarking.\"\"\"\n",
    "    print(\"--> [Simple Path] Handling simple follow-up.\")\n",
    "    history_str = \"\\n\".join([f\"{msg[\"role\"]}: {msg[\"content\"]}\" for msg in chat_history])\n",
    "    \n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "    You are a helpful AI Tutor. Answer the user's follow-up question based on the provided conversation history. Be concise and direct.\n",
    "\n",
    "    CONVERSATION HISTORY:\n",
    "    {chat_history}\n",
    "    \n",
    "    USER'S FOLLOW-UP QUESTION:\n",
    "    {question}\n",
    "\n",
    "    YOUR ANSWER:\n",
    "    \"\"\")\n",
    "    \n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"chat_history\": history_str, \"question\": query})\n",
    "    yield {\"status\": \"llm_complete\", \"content\": response.content}\n",
    "\n",
    "\n",
    "def run_full_pipeline(query: str, chat_history, user_profile_path: str = None):\n",
    "    \"\"\"The new main orchestrator that uses the Router Agent.\"\"\"\n",
    "    \n",
    "    host_node = socket.gethostname()\n",
    "    asurite_id = \"apoojar4\"\n",
    "    llm = ChatOllama(model=\"qwen3:14b\", base_url=f\"http://{asurite_id}@{host_node}:11434/\")\n",
    "    \n",
    "    # 1. First, call the Router Agent to choose a path\n",
    "    router = RouterAgent(llm)\n",
    "    chosen_path = router.choose_path(query, chat_history)\n",
    "    \n",
    "    # 2. Execute the chosen path\n",
    "    if chosen_path == \"CONVERSATIONAL\":\n",
    "        # yield {\"status\": \"llm_complete\", \"content\": \"Hello! How can I help you with data science and GPU acceleration today?\"}\n",
    "        yield from run_conversational_response(query, llm)\n",
    "    \n",
    "    elif chosen_path == \"FOLLOW_UP\":\n",
    "        # Yield the results from the simple follow-up function\n",
    "        yield from run_simple_follow_up(query, chat_history, llm)\n",
    "        \n",
    "    elif chosen_path == \"COMPLEX_QUERY\":\n",
    "        # Yield the results from our existing full pipeline\n",
    "        # (This is our old process_with_rag function, now just for complex queries)\n",
    "        yield from process_with_rag(query, user_profile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93850901-0361-4fb9-838d-2eed9257268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_rag(query: str, user_profile_path: str = None) -> str:\n",
    "    print(\"\\n--- Running FINAL Integrated RAG Pipeline ---\")\n",
    "    host_node = socket.gethostname()\n",
    "    asurite_id = \"apoojar4\"\n",
    "    llm = ChatOllama(model=\"qwen3:14b\", base_url=f\"http://{asurite_id}@{host_node}:11434/\")\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Load available libraries from file\n",
    "    available_libs = _get_available_libraries(\"installed_libraries.txt\")\n",
    "    library_string = \", \".join(available_libs)\n",
    "\n",
    "    # Call the hybrid retriever to get URLs\n",
    "    # Note: hybrid_retriever is assumed to be defined in the cell above (Cell 5)\n",
    "    urls = hybrid_retriever(query, llm, embedding_model)\n",
    "\n",
    "    context_text = \"\"\n",
    "    if urls:\n",
    "        print(\"-> Found documents. Loading and processing context...\")\n",
    "        docs = [WebBaseLoader(url).load() for url in urls]\n",
    "        docs_list = [item for sublist in docs for item in sublist]\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500, chunk_overlap=100)\n",
    "        doc_splits = text_splitter.split_documents(docs_list)\n",
    "        vectorstore = Chroma.from_documents(documents=doc_splits, embedding=embedding_model, collection_name=\"rag-chroma\")\n",
    "        retriever = vectorstore.as_retriever()\n",
    "        retrieved_docs = retriever.invoke(query)\n",
    "        context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "        vectorstore.delete_collection()\n",
    "\n",
    "    user_profile_content = \"Knowledge Level: Intermediate\\nTUTORING STRATEGY: The user is at an intermediate level. Provide direct code comparisons (e.g., NumPy vs. CuPy), focus on performance benefits, and show clear benchmarking examples.\"\n",
    "    if user_profile_path and os.path.exists(user_profile_path):\n",
    "        print(\"\\n--- Loading the user data:\", user_profile_path)\n",
    "        with open(user_profile_path, 'r') as f:\n",
    "            user_profile_content = f.read()\n",
    "\n",
    "    # THE DEFINITIVE PROMPT, COMBINING ALL LOGIC: USER PROFILE, 3-PATH REASONING, AND LIBRARY CONSTRAINTS\n",
    "    # In Cell 6, replace the final_prompt_template string with this:\n",
    "\n",
    "    # THE ULTIMATE, BALANCED PROMPT (LOGIC + PERSONA)\n",
    "    final_prompt_template = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are an expert AI Tutor and an enthusiastic advocate for NVIDIA technology. Your primary mission is to provide an encouraging, insightful, and clear answer that is perfectly tailored to the user's knowledge level. Your tone should be that of a patient expert who is excited to help users discover the power of GPU acceleration.\n",
    "\n",
    "        **STEP 1: ANALYZE YOUR INSTRUCTIONS**\n",
    "        - First, read the **TUTORING STRATEGY DOCUMENT**. This is your most important instruction. You MUST tailor the tone, depth, and complexity of your final answer to this level.\n",
    "        - Second, read the user's **QUESTION**. Determine if its intent is **theoretical** (what/why) or **procedural** (how-to).\n",
    "        - Third, note the **CODE GENERATION CONSTRAINT**.\n",
    "\n",
    "        **TUTORING STRATEGY DOCUMENT:**\n",
    "        {user_profile}\n",
    "\n",
    "        **CODE GENERATION CONSTRAINT:**\n",
    "        You MUST only use libraries from the following list: `{available_libraries}`.\n",
    "\n",
    "        **STEP 2: CHOOSE YOUR RESPONSE PATH AND APPLY THE TUTOR PERSONA**\n",
    "        Based on the user's intent, follow ONE of the three paths below.\n",
    "\n",
    "        ---\n",
    "        **PATH 1: The question is THEORETICAL.**\n",
    "        - Provide a clear, insightful, and conversational explanation.\n",
    "        - **TAILOR YOUR TONE:** For a Beginner, use simple analogies and focus on the 'wow factor' of the technology. For an Intermediate user, be technically precise. For an Advanced user, discuss deeper architectural concepts and trade-offs.\n",
    "        - You may include ONE concise code snippet ONLY if it is essential to illustrate a key concept. Do not provide comparative code blocks.\n",
    "\n",
    "        ---\n",
    "        **PATH 2: The question is PROCEDURAL and a common NVIDIA-based GPU library EXISTS for the task.**\n",
    "        - Your answer MUST frame the GPU solution as the modern, high-performance standard.\n",
    "        - **TAILOR YOUR TONE:** For a Beginner, start with an encouraging tone, explaining what the code is doing in simple terms and focusing on how accessible GPU power can be. For an Intermediate user, focus on the performance benefits with clear code comparisons. For an Advanced user, provide more optimized code and add notes about advanced features like CUDA streams or memory management.\n",
    "        - Present the `### Recommended GPU Solution` first, followed by the `### Standard CPU Solution` clearly labeled as a \"baseline for comparison.\"\n",
    "        - Always add a \"Performance Note\" or \"Key Benefits\" section to explain the trade-offs and advantages.\n",
    "\n",
    "        ---\n",
    "        **PATH 3: The question is PROCEDURAL and a common NVIDIA-based GPU library DOES NOT EXIST for the task.**\n",
    "        - **TAILOR YOUR TONE:** Explain the standard solution at a level appropriate for the user.\n",
    "        - Provide the `### Standard Solution` with a clear code example.\n",
    "        - After the solution, add a `### GPU Acceleration Note`. In this section, explain *why* this specific task is not typically GPU-accelerated. Then, proactively pivot to an encouraging and helpful suggestion. Frame it as an exciting next step, explaining where the user *could* apply GPU acceleration in a related part of their workflow.\n",
    "        ---\n",
    "\n",
    "        **STEP 3: GENERATE THE FINAL, PERSONALIZED ANSWER**\n",
    "        Use the **RELEVANT CONTEXT** below to find supporting facts or code if helpful, but always follow your primary logic and persona from STEP 1 and STEP 2.\n",
    "\n",
    "        **RELEVANT CONTEXT FROM WEB SEARCH:**\n",
    "        {context}\n",
    "\n",
    "        **USER'S QUESTION:**\n",
    "        {question}\n",
    "\n",
    "        YOUR FINAL, TAILORED ANSWER MUST START AFTER THIS:\n",
    "        \"\"\",\n",
    "        input_variables=[\"user_profile\", \"question\", \"context\", \"available_libraries\"],\n",
    "    )\n",
    "    \n",
    "    final_chain = final_prompt_template | llm\n",
    "    llm_response_text = final_chain.invoke({\n",
    "        \"user_profile\": user_profile_content,\n",
    "        \"question\": query,\n",
    "        \"context\": context_text,\n",
    "        \"available_libraries\": library_string\n",
    "    }).content\n",
    "    \n",
    "    print(\"--> [Agent 3] Generated conversational answer.\")\n",
    "    print(llm_response_text)\n",
    "\n",
    "    yield {\"status\": \"llm_complete\", \"content\": llm_response_text}\n",
    "\n",
    "    # Code extraction, upscaling, and benchmarking logic remains the same\n",
    "    extracted_code = _extract_python_code(llm_response_text)\n",
    "    cpu_code, gpu_code = extracted_code[\"cpu_code\"], extracted_code[\"gpu_code\"]\n",
    "\n",
    "    if cpu_code and gpu_code:\n",
    "        print(\"--> [Agent 3] Both CPU and GPU code found. Proceeding to upscale and benchmark in parallel.\")\n",
    "        \n",
    "        upscaled_cpu_code = _upscale_code_agentic(cpu_code, llm, library_string)\n",
    "        upscaled_gpu_code = _upscale_code_agentic(gpu_code, llm, library_string)\n",
    "        \n",
    "        yield {\"status\": \"benchmark_running\", \"content\": \"\\n\\n---\\n*⏳ Submitting parallel benchmark jobs to Sol...*\"}\n",
    "\n",
    "        try:\n",
    "            benchmarker = SolBenchmarker(user=asurite_id)\n",
    "            # Loop through the results as they come in from the parallel jobs\n",
    "            for result in benchmarker.run_benchmark_parallel(upscaled_cpu_code, upscaled_gpu_code):\n",
    "                yield result # Pass the result directly up to the UI handler\n",
    "\n",
    "        except ValueError as e:\n",
    "            yield {\"status\": \"error\", \"message\": f\"Benchmark configuration error: {e}\"}\n",
    "    else:\n",
    "        print(\"--> [Agent 3] Did not find both code types. Skipping benchmark.\")\n",
    "\n",
    "    print(\"--- Pipeline Complete ---\")\n",
    "    # return llm_response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24215d5-6697-4a43-94cb-f2477d06e7b0",
   "metadata": {},
   "source": [
    "# Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b727ad29-8c12-4802-947b-83eddff3741b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [Agent 1] Successfully connected to Ollama LLM.\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://6abb6ab7bfb6de5970.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://6abb6ab7bfb6de5970.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'metadata': {}, 'content': 'How do I multiply two matrices of size 50x50 in Python?', 'options': []}, {'role': 'assistant', 'metadata': {}, 'content': '🧠 Thinking...', 'options': []}]\n",
      "--> [Router Agent] Chose path: COMPLEX_QUERY\n",
      "\n",
      "--- Running FINAL Integrated RAG Pipeline ---\n",
      "--> Successfully loaded 575 available libraries from file.\n",
      "-> [Tier 1] Trying Semantic Search on RSS feeds...\n",
      "-> [Tier 1] Found 0 semantically relevant URLs from RSS.\n",
      "--> RSS search found no relevant articles. Falling back to web search.\n",
      "-> [Agent 2] Using LLM to generate search queries...\n",
      "-> [Agent 2] Generated queries: ['python matrix multiplication 50x50', 'python gpu matrix multiplication 50x50', 'python nvidia gpu matrix multiplication 50x50']\n",
      "-> [Tier 2] Falling back to polite Web Search...\n",
      "-> [Hybrid Retriever] Found 5 final URLs to use for context.\n",
      "-> Found documents. Loading and processing context...\n",
      "\n",
      "--- Loading the user data: user_profiles/ashwith.txt\n",
      "--> [Agent 3] Generated conversational answer.\n",
      "<think>\n",
      "Okay, the user is asking how to multiply two 50x50 matrices in Python. Let me think about the best way to approach this.\n",
      "\n",
      "First, I need to check if there's a common NVIDIA GPU library for matrix multiplication. The user provided a long list of libraries, including cuBLAS and others. Since cuBLAS is a GPU-accelerated library for linear algebra, that's probably the recommended solution here. But the user might not have CUDA installed, so I should also provide a standard CPU solution using NumPy.\n",
      "\n",
      "Wait, the user's question is about multiplying two matrices, which is a standard operation. The PATH 2 would be appropriate if there's a GPU solution. But the user might be a beginner, so I need to explain both the GPU and CPU methods.\n",
      "\n",
      "Let me structure the answer. Start with the GPU solution using cuBLAS via SciPy, then the CPU solution with NumPy. Also, add a performance note comparing the two. But wait, the user might not have SciPy or CUDA installed. Should I mention that? Maybe, but the answer should still present the GPU solution as the modern standard, even if it requires additional setup.\n",
      "\n",
      "Also, the code example for the GPU solution might need to use scipy.linalg.blas, but I need to check the correct function. The example in the context used gpuarray, which is from PyCUDA. Alternatively, using cuBLAS through SciPy's interface would be more straightforward for many users.\n",
      "\n",
      "Wait, the user's example in the context had a code snippet that used PyCUDA's gpuarray, but that's a bit more involved. Maybe using NumPy's matmul is simpler for the CPU solution, and for the GPU, using cuBLAS via SciPy's get_blas_funcs.\n",
      "\n",
      "But I should make sure the code is correct. Let me think: for the GPU solution, using SciPy's interface to cuBLAS. The code would be something like:\n",
      "\n",
      "from scipy.linalg import get_blas_funcs\n",
      "import numpy as np\n",
      "\n",
      "a = np.random.rand(50,50).astype(np.float32)\n",
      "b = np.random.rand(50,50).astype(np.float32)\n",
      "gemm = get_blas_funcs('gemm', [a, b])\n",
      "c = gemm(a, b)\n",
      "\n",
      "But wait, does get_blas_funcs use the GPU? No, that's the CPU BLAS. Oh, right, SciPy's BLAS functions are for the CPU. So maybe the user needs to use a different library for GPU acceleration, like cuBLAS directly via PyCUDA or another wrapper.\n",
      "\n",
      "Hmm, maybe I should correct that. The original context example used PyCUDA's gpuarray, which is a way to use CUDA in Python. So perhaps the GPU solution would involve using PyCUDA or another CUDA wrapper.\n",
      "\n",
      "But the user might not have those libraries installed. So maybe the answer should present the standard NumPy solution and then mention that for GPU acceleration, using cuBLAS via PyCUDA or similar libraries is possible, but requires additional setup.\n",
      "\n",
      "Wait, the user's question is about multiplying two matrices. The standard solution is NumPy's matmul or @ operator. The GPU solution would be using a library like cuBLAS, but the user might not have that set up. However, the answer should still follow PATH 2 if a GPU solution exists.\n",
      "\n",
      "But in the context, the user had a code example that used PyCUDA's gpuarray. So maybe the answer should use that. However, PyCUDA is not as commonly used as other libraries. Alternatively, using CuPy, which is a NumPy-like library for GPU computing.\n",
      "\n",
      "But the user's provided list includes CuPy? Let me check the list. The user's list includes 'cupy' in the list? Wait, looking back, the list provided by the user is a long list of packages, but I don't see 'cupy' there. The list includes 'cudnn', 'cublas', etc., but maybe not CuPy. However, the user might have CuPy installed.\n",
      "\n",
      "Alternatively, the answer could use NumPy for the CPU solution and CuPy for the GPU solution. But since the user hasn't mentioned having CuPy, maybe it's better to use PyCUDA's gpuarray as in the context.\n",
      "\n",
      "But this is getting complicated. Let me simplify. The standard solution is NumPy's matmul. For GPU acceleration, using a library like CuPy would be straightforward. However, if the user doesn't have CuPy, they might need to use PyCUDA or another library.\n",
      "\n",
      "Alternatively, the answer could present the NumPy solution and then mention that for GPU acceleration, using CuPy would be a good approach, providing an example with CuPy if it's available.\n",
      "\n",
      "But the user's question is about multiplying two matrices, and the answer should follow PATH 2 if there's a GPU solution. Since cuBLAS is a standard library for this, even if the code example uses PyCUDA, the answer should present the GPU solution first.\n",
      "\n",
      "Alternatively, maybe the user is looking for a simple answer, and the GPU part is optional. But according to the instructions, if a GPU solution exists, it should be presented as the recommended solution.\n",
      "\n",
      "So, putting it all together: the standard solution is with NumPy, and the recommended GPU solution is using CuPy or PyCUDA's gpuarray. However, the code examples need to be correct.\n",
      "\n",
      "Wait, in the context, the user had a code snippet that used gpuarray.to_gpu and then performed the multiplication. But that code might not be complete. The example in the context had a code block with some timing, but the user's question is about multiplying two matrices.\n",
      "\n",
      "So, for the GPU solution, using PyCUDA's gpuarray:\n",
      "\n",
      "import pyopencl as cl\n",
      "import pyopencl.array as cl_array\n",
      "import numpy as np\n",
      "\n",
      "# Initialize OpenCL context and command queue\n",
      "ctx = cl.create_some_context()\n",
      "queue = cl.CommandQueue(ctx)\n",
      "\n",
      "# Create matrices on the host\n",
      "a = np.random.rand(50, 50).astype(np.float32)\n",
      "b = np.random.rand(50, 50).astype(np.float32)\n",
      "\n",
      "# Transfer to GPU\n",
      "a_gpu = cl_array.to_array(queue, a)\n",
      "b_gpu = cl_array.to_array(queue, b)\n",
      "\n",
      "# Perform matrix multiplication on the GPU\n",
      "c_gpu = a_gpu.dot(b_gpu)\n",
      "\n",
      "# Transfer result back to host\n",
      "c = c_gpu.get()\n",
      "\n",
      "But this uses PyOpenCL, not CUDA. However, the user's context mentioned CUDA, so maybe PyCUDA is the right path.\n",
      "\n",
      "Alternatively, using PyCUDA's gpuarray:\n",
      "\n",
      "import pycuda.autoinit\n",
      "import pycuda.gpuarray as gpuarray\n",
      "import numpy as np\n",
      "\n",
      "a = np.random.rand(50, 50).astype(np.float32)\n",
      "b = np.random.rand(50, 50).astype(np.float32)\n",
      "\n",
      "a_gpu = gpuarray.to_gpu(a)\n",
      "b_gpu = gpuarray.to_gpu(b)\n",
      "\n",
      "# Perform matrix multiplication using cuBLAS\n",
      "from pycuda import cublas\n",
      "handle = cublas.cublasCreate()\n",
      "c_gpu = gpuarray.empty((50, 50), np.float32)\n",
      "cublas.cublasSgemm(handle, 'n', 'n', 50, 50, 50, 1.0, a_gpu, 50, b_gpu, 50, 0.0, c_gpu, 50)\n",
      "cublas.cublasDestroy(handle)\n",
      "\n",
      "c = c_gpu.get()\n",
      "\n",
      "But this requires the user to have PyCUDA and cuBLAS installed, which might be a bit involved.\n",
      "\n",
      "Alternatively, using CuPy:\n",
      "\n",
      "import cupy as cp\n",
      "\n",
      "a = cp.random.rand(50, 50).astype(cp.float32)\n",
      "b = cp.random.rand(50, 50).astype(cp.float32)\n",
      "c = a @ b\n",
      "result = c.get()\n",
      "\n",
      "This is much simpler, but the user needs to have CuPy installed.\n",
      "\n",
      "Given that, perhaps the answer should present the NumPy solution and then the CuPy solution as the GPU-accelerated version, since it's more straightforward for users.\n",
      "\n",
      "But the user's question is about multiplying two matrices, which is a standard operation. The answer should follow PATH 2, which requires presenting the GPU solution first, then the CPU solution.\n",
      "\n",
      "So, the answer would be:\n",
      "\n",
      "### Recommended GPU Solution\n",
      "Using CuPy for GPU acceleration:\n",
      "\n",
      "import cupy as cp\n",
      "\n",
      "a = cp.random.rand(50, 50).astype(cp.float32)\n",
      "b = cp.random.rand(50, 50).astype(cp.float32)\n",
      "c = a @ b\n",
      "result = c.get()\n",
      "\n",
      "### Standard CPU Solution\n",
      "Using NumPy:\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "a = np.random.rand(50, 50).astype(np.float32)\n",
      "b = np.random.rand(50, 50).astype(np.float32)\n",
      "c = np.dot(a, b)\n",
      "\n",
      "Performance Note: CuPy leverages GPU parallelism for significantly faster matrix multiplications, especially for large matrices. However, it requires CUDA-compatible hardware and the CuPy library installed.\n",
      "</think>\n",
      "\n",
      "### Recommended GPU Solution\n",
      "Using **CuPy**, a NumPy-like library for GPU computing, provides a seamless way to accelerate matrix multiplication on NVIDIA GPUs:\n",
      "\n",
      "```python\n",
      "import cupy as cp\n",
      "\n",
      "# Create matrices on the GPU\n",
      "a_gpu = cp.random.rand(50, 50).astype(cp.float32)\n",
      "b_gpu = cp.random.rand(50, 50).astype(cp.float32)\n",
      "\n",
      "# Perform matrix multiplication on the GPU\n",
      "c_gpu = a_gpu @ b_gpu  # Uses CuPy's GPU-accelerated operator\n",
      "\n",
      "# Transfer result back to the CPU (if needed)\n",
      "result = c_gpu.get()\n",
      "```\n",
      "\n",
      "### Standard CPU Solution\n",
      "For CPU-based computation, **NumPy** is the standard tool:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "# Create matrices on the CPU\n",
      "a_cpu = np.random.rand(50, 50).astype(np.float32)\n",
      "b_cpu = np.random.rand(50, 50).astype(np.float32)\n",
      "\n",
      "# Perform matrix multiplication on the CPU\n",
      "c_cpu = np.dot(a_cpu, b_cpu)\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Performance Note\n",
      "- **GPU Solution (CuPy)**: Leverages parallelism on NVIDIA GPUs, offering significantly faster execution for large matrices (e.g., 1000x1000+). Ideal for HPC, deep learning, or data science workflows where performance is critical.\n",
      "- **CPU Solution (NumPy)**: Efficient for small to moderate matrices (e.g., 50x50) and requires no additional hardware or libraries. Suitable for prototyping or environments without GPU access.\n",
      "\n",
      "**Note**: CuPy requires a CUDA-compatible GPU and the `cupy` library installed (`pip install cupy`). For most users, the NumPy solution is sufficient unless GPU acceleration is explicitly needed.\n",
      "--> [Agent 3] Extracting code blocks using definitive 'Process of Elimination' parser...\n",
      "################################################\n",
      "GPU Code Found:\n",
      " import cupy as cp\n",
      "\n",
      "# Create matrices on the GPU\n",
      "a_gpu = cp.random.rand(50, 50).astype(cp.float32)\n",
      "b_gpu = cp.random.rand(50, 50).astype(cp.float32)\n",
      "\n",
      "# Perform matrix multiplication on the GPU\n",
      "c_gpu = a_gpu @ b_gpu  # Uses CuPy's GPU-accelerated operator\n",
      "\n",
      "# Transfer result back to the CPU (if needed)\n",
      "result = c_gpu.get()\n",
      "################################################\n",
      "CPU Code Found:\n",
      " import numpy as np\n",
      "\n",
      "# Create matrices on the CPU\n",
      "a_cpu = np.random.rand(50, 50).astype(np.float32)\n",
      "b_cpu = np.random.rand(50, 50).astype(np.float32)\n",
      "\n",
      "# Perform matrix multiplication on the CPU\n",
      "c_cpu = np.dot(a_cpu, b_cpu)\n",
      "################################################\n",
      "--> [Agent 3] Both CPU and GPU code found. Proceeding to upscale and benchmark in parallel.\n",
      "--> [Agent 3] Invoking LLM with STRICT CONSTRAINTS to upscale code...\n",
      "--> [Agent 3] Code successfully upscaled.\n",
      "--> [Agent 3] Invoking LLM with STRICT CONSTRAINTS to upscale code...\n",
      "--> [Agent 3] Code successfully upscaled.\n",
      "--> [Agent 4] Submitted GPU job with ID: 28623504\n",
      "--> [Agent 4] Submitted CPU job with ID: 28623505\n",
      "--> [Agent 4] GPU Job 28623504 finished with status: COMPLETED\n",
      "--> [Agent 4] CPU Job 28623505 finished with status: COMPLETED\n",
      "--- Pipeline Complete ---\n",
      "[{'role': 'user', 'metadata': {}, 'content': 'How do I multiply two matrices of size 50x50 in Python?', 'options': []}, {'role': 'assistant', 'metadata': {}, 'content': \"### Recommended GPU Solution\\nUsing **CuPy**, a NumPy-like library for GPU computing, provides a seamless way to accelerate matrix multiplication on NVIDIA GPUs:\\n\\n```python\\nimport cupy as cp\\n\\n# Create matrices on the GPU\\na_gpu = cp.random.rand(50, 50).astype(cp.float32)\\nb_gpu = cp.random.rand(50, 50).astype(cp.float32)\\n\\n# Perform matrix multiplication on the GPU\\nc_gpu = a_gpu @ b_gpu  # Uses CuPy's GPU-accelerated operator\\n\\n# Transfer result back to the CPU (if needed)\\nresult = c_gpu.get()\\n```\\n\\n### Standard CPU Solution\\nFor CPU-based computation, **NumPy** is the standard tool:\\n\\n```python\\nimport numpy as np\\n\\n# Create matrices on the CPU\\na_cpu = np.random.rand(50, 50).astype(np.float32)\\nb_cpu = np.random.rand(50, 50).astype(np.float32)\\n\\n# Perform matrix multiplication on the CPU\\nc_cpu = np.dot(a_cpu, b_cpu)\\n```\\n\\n---\\n\\n### Performance Note\\n- **GPU Solution (CuPy)**: Leverages parallelism on NVIDIA GPUs, offering significantly faster execution for large matrices (e.g., 1000x1000+). Ideal for HPC, deep learning, or data science workflows where performance is critical.\\n- **CPU Solution (NumPy)**: Efficient for small to moderate matrices (e.g., 50x50) and requires no additional hardware or libraries. Suitable for prototyping or environments without GPU access.\\n\\n**Note**: CuPy requires a CUDA-compatible GPU and the `cupy` library installed (`pip install cupy`). For most users, the NumPy solution is sufficient unless GPU acceleration is explicitly needed.\", 'options': []}, {'role': 'assistant', 'metadata': {'title': '📊 Live Benchmark Results'}, 'content': '| Metric | Result |\\n|---|---|\\n| GPU Time | 2.1000s |\\n| CPU Time | 11.0100s |\\n| **Speedup** | **5.24x faster on GPU!** |', 'options': []}, {'role': 'user', 'metadata': {}, 'content': 'now multiply 5 arrays of dimension 1 million each and convert it to a tensor of your choice', 'options': []}, {'role': 'assistant', 'metadata': {}, 'content': '🧠 Thinking...', 'options': []}]\n",
      "--> [Router Agent] Chose path: COMPLEX_QUERY\n",
      "\n",
      "--- Running FINAL Integrated RAG Pipeline ---\n",
      "--> Successfully loaded 575 available libraries from file.\n",
      "-> [Tier 1] Trying Semantic Search on RSS feeds...\n",
      "-> [Tier 1] Found 2 semantically relevant URLs from RSS.\n",
      "-> [Hybrid Retriever] Found 2 final URLs to use for context.\n",
      "-> Found documents. Loading and processing context...\n",
      "\n",
      "--- Loading the user data: user_profiles/ashwith.txt\n",
      "--> [Agent 3] Generated conversational answer.\n",
      "<think>\n",
      "Okay, let's see. The user wants to multiply five arrays each with a million elements and convert the result into a tensor. Hmm, first, I need to figure out the best way to approach this. The user mentioned \"convert it to a tensor of your choice,\" so maybe they want it in a specific framework like PyTorch or TensorFlow. But since the question is procedural, I should check if there's a GPU library that can handle this efficiently.\n",
      "\n",
      "Looking at the provided list of libraries, there's PyTorch, TensorFlow, and others like NumPy. But the user might be interested in GPU acceleration. The relevant context talks about NVIDIA TensorRT for RTX, which is optimized for inference. However, TensorRT is more for deployment rather than basic array operations. The user's task is more about array multiplication, which is a common operation in machine learning.\n",
      "\n",
      "Wait, the user's question is about multiplying arrays, which is a general computation. The libraries listed include NumPy, which is CPU-based, and PyTorch/TensorFlow, which can use GPUs. Since there's no specific mention of a GPU library for array multiplication beyond what's in the context, maybe I should go with Path 3. Because the task isn't specific to NVIDIA's libraries for this kind of operation, even though PyTorch and TensorFlow can use GPUs.\n",
      "\n",
      "But the user might be looking for a way to do this with GPU acceleration. However, the standard solution would be using NumPy or a similar library. Then, in the GPU acceleration note, I can mention that while the multiplication itself is straightforward, if they're working within a deep learning framework, using GPU-accelerated libraries like PyTorch or TensorFlow would be beneficial. But since the task is just array multiplication, the standard solution is sufficient, and the GPU note can explain that for larger-scale operations or within a machine learning pipeline, using GPU libraries would be more efficient.\n",
      "\n",
      "So, Path 3 seems appropriate here. The answer would first present the standard solution using NumPy, then note that for more complex tasks, GPU acceleration is possible. I should make sure to explain that multiplying arrays is a basic operation, and while GPU can speed it up, it's not typically a bottleneck unless dealing with very large datasets or repeated operations.\n",
      "</think>\n",
      "\n",
      "### Standard Solution\n",
      "To multiply five arrays of 1 million elements each and convert the result into a tensor, you can use **NumPy** for efficient array operations and **PyTorch** for tensor conversion. Here's a concise example:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import torch\n",
      "\n",
      "# Create 5 arrays of 1 million elements each\n",
      "arrays = [np.random.rand(1_000_000) for _ in range(5)]\n",
      "\n",
      "# Multiply all arrays element-wise\n",
      "result_array = np.prod(arrays, axis=0)\n",
      "\n",
      "# Convert to PyTorch tensor\n",
      "result_tensor = torch.from_numpy(result_array)\n",
      "print(result_tensor)\n",
      "```\n",
      "\n",
      "### GPU Acceleration Note\n",
      "While this solution uses CPU-based NumPy, **GPU acceleration is not typically required for basic array multiplication** due to the simplicity of the operation. However, if this task is part of a larger machine learning pipeline (e.g., preprocessing for a neural network), consider using **PyTorch or TensorFlow** directly. These frameworks leverage GPU acceleration for tensor operations, which can significantly speed up workflows involving repeated or large-scale computations. For example, replacing `np.prod` with `torch.prod` on GPU tensors would automatically utilize CUDA for faster execution.\n",
      "--> [Agent 3] Extracting code blocks using definitive 'Process of Elimination' parser...\n",
      "################################################\n",
      "GPU Code Found:\n",
      " \n",
      "################################################\n",
      "CPU Code Found:\n",
      " import numpy as np\n",
      "import torch\n",
      "\n",
      "# Create 5 arrays of 1 million elements each\n",
      "arrays = [np.random.rand(1_000_000) for _ in range(5)]\n",
      "\n",
      "# Multiply all arrays element-wise\n",
      "result_array = np.prod(arrays, axis=0)\n",
      "\n",
      "# Convert to PyTorch tensor\n",
      "result_tensor = torch.from_numpy(result_array)\n",
      "print(result_tensor)\n",
      "################################################\n",
      "--> [Agent 3] Did not find both code types. Skipping benchmark.\n",
      "--- Pipeline Complete ---\n",
      "[{'role': 'user', 'metadata': {}, 'content': 'How do I multiply two matrices of size 50x50 in Python?', 'options': []}, {'role': 'assistant', 'metadata': {}, 'content': \"### Recommended GPU Solution\\nUsing **CuPy**, a NumPy-like library for GPU computing, provides a seamless way to accelerate matrix multiplication on NVIDIA GPUs:\\n\\n```python\\nimport cupy as cp\\n\\n# Create matrices on the GPU\\na_gpu = cp.random.rand(50, 50).astype(cp.float32)\\nb_gpu = cp.random.rand(50, 50).astype(cp.float32)\\n\\n# Perform matrix multiplication on the GPU\\nc_gpu = a_gpu @ b_gpu  # Uses CuPy's GPU-accelerated operator\\n\\n# Transfer result back to the CPU (if needed)\\nresult = c_gpu.get()\\n```\\n\\n### Standard CPU Solution\\nFor CPU-based computation, **NumPy** is the standard tool:\\n\\n```python\\nimport numpy as np\\n\\n# Create matrices on the CPU\\na_cpu = np.random.rand(50, 50).astype(np.float32)\\nb_cpu = np.random.rand(50, 50).astype(np.float32)\\n\\n# Perform matrix multiplication on the CPU\\nc_cpu = np.dot(a_cpu, b_cpu)\\n```\\n\\n---\\n\\n### Performance Note\\n- **GPU Solution (CuPy)**: Leverages parallelism on NVIDIA GPUs, offering significantly faster execution for large matrices (e.g., 1000x1000+). Ideal for HPC, deep learning, or data science workflows where performance is critical.\\n- **CPU Solution (NumPy)**: Efficient for small to moderate matrices (e.g., 50x50) and requires no additional hardware or libraries. Suitable for prototyping or environments without GPU access.\\n\\n**Note**: CuPy requires a CUDA-compatible GPU and the `cupy` library installed (`pip install cupy`). For most users, the NumPy solution is sufficient unless GPU acceleration is explicitly needed.\", 'options': []}, {'role': 'assistant', 'metadata': {'title': '📊 Live Benchmark Results'}, 'content': '| Metric | Result |\\n|---|---|\\n| GPU Time | 2.1000s |\\n| CPU Time | 11.0100s |\\n| **Speedup** | **5.24x faster on GPU!** |', 'options': []}, {'role': 'user', 'metadata': {}, 'content': 'now multiply 5 arrays of dimension 1 million each and convert it to a tensor of your choice', 'options': []}, {'role': 'assistant', 'metadata': {}, 'content': \"### Standard Solution\\nTo multiply five arrays of 1 million elements each and convert the result into a tensor, you can use **NumPy** for efficient array operations and **PyTorch** for tensor conversion. Here's a concise example:\\n\\n```python\\nimport numpy as np\\nimport torch\\n\\n# Create 5 arrays of 1 million elements each\\narrays = [np.random.rand(1_000_000) for _ in range(5)]\\n\\n# Multiply all arrays element-wise\\nresult_array = np.prod(arrays, axis=0)\\n\\n# Convert to PyTorch tensor\\nresult_tensor = torch.from_numpy(result_array)\\nprint(result_tensor)\\n```\\n\\n### GPU Acceleration Note\\nWhile this solution uses CPU-based NumPy, **GPU acceleration is not typically required for basic array multiplication** due to the simplicity of the operation. However, if this task is part of a larger machine learning pipeline (e.g., preprocessing for a neural network), consider using **PyTorch or TensorFlow** directly. These frameworks leverage GPU acceleration for tensor operations, which can significantly speed up workflows involving repeated or large-scale computations. For example, replacing `np.prod` with `torch.prod` on GPU tensors would automatically utilize CUDA for faster execution.\", 'options': []}, {'role': 'user', 'metadata': {}, 'content': 'multiply 5 dataframes each having 10 million rows and 15 features.', 'options': []}, {'role': 'assistant', 'metadata': {}, 'content': '🧠 Thinking...', 'options': []}]\n",
      "--> [Router Agent] Chose path: COMPLEX_QUERY\n",
      "\n",
      "--- Running FINAL Integrated RAG Pipeline ---\n",
      "--> Successfully loaded 575 available libraries from file.\n",
      "-> [Tier 1] Trying Semantic Search on RSS feeds...\n",
      "-> [Tier 1] Found 0 semantically relevant URLs from RSS.\n",
      "--> RSS search found no relevant articles. Falling back to web search.\n",
      "-> [Agent 2] Using LLM to generate search queries...\n",
      "-> [Agent 2] Generated queries: ['pandas multiply 5 dataframes 10 million rows 15 columns CPU', 'pandas multiply 5 dataframes 10 million rows 15 columns GPU', 'pycuda multiply 5 dataframes 10 million rows 15 columns GPU']\n",
      "-> [Tier 2] Falling back to polite Web Search...\n",
      "-> [Hybrid Retriever] Found 5 final URLs to use for context.\n",
      "-> Found documents. Loading and processing context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/packages/envs/genai25.07/lib/python3.12/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.07/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.07/lib/python3.12/site-packages/gradio/blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.07/lib/python3.12/site-packages/gradio/blocks.py\", line 1743, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.07/lib/python3.12/site-packages/gradio/utils.py\", line 749, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.07/lib/python3.12/site-packages/gradio/utils.py\", line 743, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.07/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.07/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.07/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.07/lib/python3.12/site-packages/gradio/utils.py\", line 726, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.07/lib/python3.12/site-packages/gradio/utils.py\", line 887, in gen_wrapper\n",
      "    response = next(iterator)\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2420827/329455975.py\", line 104, in get_bot_response\n",
      "    for update in response_generator:\n",
      "                  ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2420827/416021333.py\", line 96, in run_full_pipeline\n",
      "    yield from process_with_rag(query, user_profile_path)\n",
      "  File \"/tmp/ipykernel_2420827/1652360303.py\", line 19, in process_with_rag\n",
      "    docs = [WebBaseLoader(url).load() for url in urls]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.07/lib/python3.12/site-packages/langchain_core/document_loaders/base.py\", line 32, in load\n",
      "    return list(self.lazy_load())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.07/lib/python3.12/site-packages/langchain_community/document_loaders/web_base.py\", line 375, in lazy_load\n",
      "    soup = self._scrape(path, bs_kwargs=self.bs_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.07/lib/python3.12/site-packages/langchain_community/document_loaders/web_base.py\", line 357, in _scrape\n",
      "    html_doc = self.session.get(url, **self.requests_kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.07/lib/python3.12/site-packages/requests/sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.07/lib/python3.12/site-packages/requests/sessions.py\", line 575, in request\n",
      "    prep = self.prepare_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/packages/envs/genai25.07/lib/python3.12/site-packages/requests/sessions.py\", line 484, in prepare_request\n",
      "    p.prepare(\n",
      "  File \"/packages/envs/genai25.07/lib/python3.12/site-packages/requests/models.py\", line 367, in prepare\n",
      "    self.prepare_url(url, params)\n",
      "  File \"/packages/envs/genai25.07/lib/python3.12/site-packages/requests/models.py\", line 438, in prepare_url\n",
      "    raise MissingSchema(\n",
      "requests.exceptions.MissingSchema: Invalid URL '/search?num=4': No scheme supplied. Perhaps you meant https:///search?num=4?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'metadata': {}, 'content': 'How do I multiply two matrices of size 50x50 in Python?', 'options': []}, {'role': 'assistant', 'metadata': {}, 'content': \"### Recommended GPU Solution\\nUsing **CuPy**, a NumPy-like library for GPU computing, provides a seamless way to accelerate matrix multiplication on NVIDIA GPUs:\\n\\n```python\\nimport cupy as cp\\n\\n# Create matrices on the GPU\\na_gpu = cp.random.rand(50, 50).astype(cp.float32)\\nb_gpu = cp.random.rand(50, 50).astype(cp.float32)\\n\\n# Perform matrix multiplication on the GPU\\nc_gpu = a_gpu @ b_gpu  # Uses CuPy's GPU-accelerated operator\\n\\n# Transfer result back to the CPU (if needed)\\nresult = c_gpu.get()\\n```\\n\\n### Standard CPU Solution\\nFor CPU-based computation, **NumPy** is the standard tool:\\n\\n```python\\nimport numpy as np\\n\\n# Create matrices on the CPU\\na_cpu = np.random.rand(50, 50).astype(np.float32)\\nb_cpu = np.random.rand(50, 50).astype(np.float32)\\n\\n# Perform matrix multiplication on the CPU\\nc_cpu = np.dot(a_cpu, b_cpu)\\n```\\n\\n---\\n\\n### Performance Note\\n- **GPU Solution (CuPy)**: Leverages parallelism on NVIDIA GPUs, offering significantly faster execution for large matrices (e.g., 1000x1000+). Ideal for HPC, deep learning, or data science workflows where performance is critical.\\n- **CPU Solution (NumPy)**: Efficient for small to moderate matrices (e.g., 50x50) and requires no additional hardware or libraries. Suitable for prototyping or environments without GPU access.\\n\\n**Note**: CuPy requires a CUDA-compatible GPU and the `cupy` library installed (`pip install cupy`). For most users, the NumPy solution is sufficient unless GPU acceleration is explicitly needed.\", 'options': []}, {'role': 'assistant', 'metadata': {'title': '📊 Live Benchmark Results'}, 'content': '| Metric | Result |\\n|---|---|\\n| GPU Time | 2.1000s |\\n| CPU Time | 11.0100s |\\n| **Speedup** | **5.24x faster on GPU!** |', 'options': []}, {'role': 'user', 'metadata': {}, 'content': 'now multiply 5 arrays of dimension 1 million each and convert it to a tensor of your choice', 'options': []}, {'role': 'assistant', 'metadata': {}, 'content': \"### Standard Solution\\nTo multiply five arrays of 1 million elements each and convert the result into a tensor, you can use **NumPy** for efficient array operations and **PyTorch** for tensor conversion. Here's a concise example:\\n\\n```python\\nimport numpy as np\\nimport torch\\n\\n# Create 5 arrays of 1 million elements each\\narrays = [np.random.rand(1_000_000) for _ in range(5)]\\n\\n# Multiply all arrays element-wise\\nresult_array = np.prod(arrays, axis=0)\\n\\n# Convert to PyTorch tensor\\nresult_tensor = torch.from_numpy(result_array)\\nprint(result_tensor)\\n```\\n\\n### GPU Acceleration Note\\nWhile this solution uses CPU-based NumPy, **GPU acceleration is not typically required for basic array multiplication** due to the simplicity of the operation. However, if this task is part of a larger machine learning pipeline (e.g., preprocessing for a neural network), consider using **PyTorch or TensorFlow** directly. These frameworks leverage GPU acceleration for tensor operations, which can significantly speed up workflows involving repeated or large-scale computations. For example, replacing `np.prod` with `torch.prod` on GPU tensors would automatically utilize CUDA for faster execution.\", 'options': []}, {'role': 'user', 'metadata': {}, 'content': 'multiply 5 dataframes each having 10 million rows and 15 features.', 'options': []}, {'role': 'assistant', 'metadata': {}, 'content': '🧠 Thinking...', 'options': []}, {'role': 'user', 'metadata': {}, 'content': 'multiply 2 dataframes each having 10 million rows and 15 features', 'options': []}, {'role': 'assistant', 'metadata': {}, 'content': '🧠 Thinking...', 'options': []}]\n",
      "--> [Router Agent] Chose path: COMPLEX_QUERY\n",
      "\n",
      "--- Running FINAL Integrated RAG Pipeline ---\n",
      "--> Successfully loaded 575 available libraries from file.\n",
      "-> [Tier 1] Trying Semantic Search on RSS feeds...\n",
      "-> [Tier 1] Found 0 semantically relevant URLs from RSS.\n",
      "--> RSS search found no relevant articles. Falling back to web search.\n",
      "-> [Agent 2] Using LLM to generate search queries...\n",
      "-> [Agent 2] Generated queries: ['pandas multiply two dataframes 10 million rows 15 columns cpu', 'pandas multiply two dataframes 10 million rows 15 columns gpu nvidia', 'pytorch multiply two dataframes 10 million rows 15 columns gpu nvidia']\n",
      "-> [Tier 2] Falling back to polite Web Search...\n",
      "-> [Hybrid Retriever] Found 6 final URLs to use for context.\n",
      "-> Found documents. Loading and processing context...\n",
      "\n",
      "--- Loading the user data: user_profiles/ashwith.txt\n",
      "--> [Agent 3] Generated conversational answer.\n",
      "<think>\n",
      "Okay, let's tackle the user's question. They want to multiply two dataframes, each with 10 million rows and 15 features. Hmm, first, I need to figure out the best approach here.\n",
      "\n",
      "The user mentioned the list of packages they have, which includes things like pandas, numpy, and some GPU-related libraries. But looking at the context provided, there's a discussion about scaling with pandas and using chunking. The example given was about reducing memory usage by converting data types and using categorical variables. However, multiplying two large dataframes isn't something that's typically done with standard pandas operations because of memory constraints.\n",
      "\n",
      "Wait, the user might be thinking of element-wise multiplication or matrix multiplication. But with 10 million rows and 15 columns each, even element-wise multiplication would be computationally intensive. If it's matrix multiplication, that's a different story, but the question says \"multiply 2 dataframes,\" which is a bit ambiguous. Assuming element-wise multiplication, each dataframe has 150 million elements, so the result would be another 150 million elements. That's a lot of data.\n",
      "\n",
      "Given the context, the user might be facing memory issues. The example in the context shows how to reduce memory by downcasting data types. So maybe the first step is to optimize the data types of the dataframes. For example, using float32 instead of float64 if possible. That would cut the memory usage in half for each dataframe. But even then, with 10 million rows and 15 columns, each dataframe is around 1.5 GB (assuming float64), so two of them would be 3 GB. But if they're using float32, that's 750 MB each, totaling 1.5 GB. That's manageable, but the multiplication could still be slow on the CPU.\n",
      "\n",
      "The user might not have a GPU solution for this specific task. The context mentions PyTorch forums discussing multi-GPU calculations, but the question is about multiplying dataframes, which is more of a data processing task than a machine learning one. So maybe there's no direct GPU library for this. However, using libraries like NumPy with GPU acceleration (like CuPy) could help. But if the user is using pandas, they might need to convert the dataframes to NumPy arrays first.\n",
      "\n",
      "Another approach is to use Dask, which is a parallel computing library that can handle larger-than-memory datasets by breaking them into chunks. Dask can distribute the computation across multiple cores or even multiple machines. That might be a good fit here. Alternatively, using Spark for distributed processing could be an option if the data is too large for a single machine.\n",
      "\n",
      "Wait, the user's list includes 'distributed' from the context, which is part of Dask. So maybe suggesting Dask as a solution would be appropriate. But the user might not be familiar with it. Also, the question is about multiplying two dataframes, which in Dask would be straightforward if they convert their pandas dataframes to Dask dataframes.\n",
      "\n",
      "Alternatively, if the user is open to using NumPy, they could convert the dataframes to NumPy arrays and perform the multiplication there. However, with 10 million rows, even that might be memory-heavy. But using CuPy (a GPU-accelerated NumPy) could be a way to leverage the GPUs if available.\n",
      "\n",
      "But the user's question might not have a direct GPU solution, so following Path 3. The standard solution would be using pandas with optimized data types, but since that's memory-intensive, chunking or using Dask would be better. Then, in the GPU acceleration note, mention that while direct multiplication might not be GPU-accelerated, using CuPy or Dask with GPU support could help.\n",
      "\n",
      "Wait, but the user has 4 GPUs. So maybe using Dask with GPU workers. However, that might be more complex. Alternatively, using PyTorch or TensorFlow for tensor operations, but that would require converting the dataframes to tensors.\n",
      "\n",
      "Alternatively, using the 'dask.dataframe' module to handle the multiplication in chunks. That would allow parallel processing on the CPU, and if the user has access to GPUs, maybe using Dask with CUDA support.\n",
      "\n",
      "But I need to check if the user's question is about element-wise multiplication or matrix multiplication. If it's matrix multiplication, that's a different scenario. For example, multiplying two matrices of size (10M, 15) would require that the inner dimensions match, which they don't. So that's not possible. Therefore, it's more likely element-wise multiplication.\n",
      "\n",
      "So, the standard solution would be to use pandas with optimized data types, but given the size, that might not be feasible. Therefore, using Dask to process the data in chunks. Then, in the GPU acceleration note, suggest using CuPy for element-wise operations on the GPU.\n",
      "\n",
      "Alternatively, if the user is using PyTorch, they could convert the dataframes to tensors and use GPU acceleration. But the initial data is in pandas, so that would require some conversion steps.\n",
      "\n",
      "In the end, the best approach is to use Dask for handling the large dataframes and then perform the multiplication in chunks. If GPU acceleration is needed, mention CuPy or Dask with GPU support as a possible next step.\n",
      "</think>\n",
      "\n",
      "### Standard Solution  \n",
      "To multiply two large dataframes efficiently, start by optimizing memory usage and leveraging parallel processing. Here's how to proceed:  \n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Optimize data types (e.g., float32 instead of float64)\n",
      "df1 = pd.read_pickle('df1.pkl').astype(np.float32)\n",
      "df2 = pd.read_pickle('df2.pkl').astype(np.float32)\n",
      "\n",
      "# Perform element-wise multiplication\n",
      "result = df1 * df2\n",
      "```\n",
      "\n",
      "**Key Steps:**  \n",
      "1. **Data Type Optimization:** Convert columns to smaller data types (e.g., `float32`) to reduce memory usage.  \n",
      "2. **Chunking:** If the data exceeds memory, use `chunksize` in `pd.read_pickle` or `dask.dataframe` to process data in smaller batches.  \n",
      "3. **Parallel Processing:** Use libraries like **Dask** to distribute computation across multiple cores or nodes.  \n",
      "\n",
      "---\n",
      "\n",
      "### GPU Acceleration Note  \n",
      "While direct dataframe multiplication isn’t typically GPU-accelerated in libraries like PyTorch or CuPy, you can:  \n",
      "1. **Convert to GPU Tensors:** Use **CuPy** (a NumPy-like library for GPUs) to perform element-wise operations on the GPU:  \n",
      "   ```python\n",
      "   import cupy as cp\n",
      "   df1_gpu = cp.array(df1.values)\n",
      "   df2_gpu = cp.array(df2.values)\n",
      "   result_gpu = df1_gpu * df2_gpu  # GPU-accelerated multiplication\n",
      "   ```  \n",
      "2. **Leverage Dask with GPU Support:** Use **Dask-CUDA** to parallelize operations across GPUs for massive datasets.  \n",
      "\n",
      "**Why This Works:**  \n",
      "- **CuPy** mirrors NumPy's API but runs on the GPU, offering significant speedups for numerical operations.  \n",
      "- **Dask** scales computation across clusters, making it ideal for datasets larger than memory.  \n",
      "\n",
      "**Next Step:** If your workflow involves repeated numerical operations (e.g., matrix math, aggregations), consider migrating to GPU-native libraries like CuPy or PyTorch for end-to-end acceleration.\n",
      "--> [Agent 3] Extracting code blocks using definitive 'Process of Elimination' parser...\n",
      "################################################\n",
      "GPU Code Found:\n",
      " \n",
      "################################################\n",
      "CPU Code Found:\n",
      " import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Optimize data types (e.g., float32 instead of float64)\n",
      "df1 = pd.read_pickle('df1.pkl').astype(np.float32)\n",
      "df2 = pd.read_pickle('df2.pkl').astype(np.float32)\n",
      "\n",
      "# Perform element-wise multiplication\n",
      "result = df1 * df2\n",
      "################################################\n",
      "--> [Agent 3] Did not find both code types. Skipping benchmark.\n",
      "--- Pipeline Complete ---\n",
      "[{'role': 'user', 'metadata': {}, 'content': 'How do I multiply two matrices of size 50x50 in Python?', 'options': []}, {'role': 'assistant', 'metadata': {}, 'content': \"### Recommended GPU Solution\\nUsing **CuPy**, a NumPy-like library for GPU computing, provides a seamless way to accelerate matrix multiplication on NVIDIA GPUs:\\n\\n```python\\nimport cupy as cp\\n\\n# Create matrices on the GPU\\na_gpu = cp.random.rand(50, 50).astype(cp.float32)\\nb_gpu = cp.random.rand(50, 50).astype(cp.float32)\\n\\n# Perform matrix multiplication on the GPU\\nc_gpu = a_gpu @ b_gpu  # Uses CuPy's GPU-accelerated operator\\n\\n# Transfer result back to the CPU (if needed)\\nresult = c_gpu.get()\\n```\\n\\n### Standard CPU Solution\\nFor CPU-based computation, **NumPy** is the standard tool:\\n\\n```python\\nimport numpy as np\\n\\n# Create matrices on the CPU\\na_cpu = np.random.rand(50, 50).astype(np.float32)\\nb_cpu = np.random.rand(50, 50).astype(np.float32)\\n\\n# Perform matrix multiplication on the CPU\\nc_cpu = np.dot(a_cpu, b_cpu)\\n```\\n\\n---\\n\\n### Performance Note\\n- **GPU Solution (CuPy)**: Leverages parallelism on NVIDIA GPUs, offering significantly faster execution for large matrices (e.g., 1000x1000+). Ideal for HPC, deep learning, or data science workflows where performance is critical.\\n- **CPU Solution (NumPy)**: Efficient for small to moderate matrices (e.g., 50x50) and requires no additional hardware or libraries. Suitable for prototyping or environments without GPU access.\\n\\n**Note**: CuPy requires a CUDA-compatible GPU and the `cupy` library installed (`pip install cupy`). For most users, the NumPy solution is sufficient unless GPU acceleration is explicitly needed.\", 'options': []}, {'role': 'assistant', 'metadata': {'title': '📊 Live Benchmark Results'}, 'content': '| Metric | Result |\\n|---|---|\\n| GPU Time | 2.1000s |\\n| CPU Time | 11.0100s |\\n| **Speedup** | **5.24x faster on GPU!** |', 'options': []}, {'role': 'user', 'metadata': {}, 'content': 'now multiply 5 arrays of dimension 1 million each and convert it to a tensor of your choice', 'options': []}, {'role': 'assistant', 'metadata': {}, 'content': \"### Standard Solution\\nTo multiply five arrays of 1 million elements each and convert the result into a tensor, you can use **NumPy** for efficient array operations and **PyTorch** for tensor conversion. Here's a concise example:\\n\\n```python\\nimport numpy as np\\nimport torch\\n\\n# Create 5 arrays of 1 million elements each\\narrays = [np.random.rand(1_000_000) for _ in range(5)]\\n\\n# Multiply all arrays element-wise\\nresult_array = np.prod(arrays, axis=0)\\n\\n# Convert to PyTorch tensor\\nresult_tensor = torch.from_numpy(result_array)\\nprint(result_tensor)\\n```\\n\\n### GPU Acceleration Note\\nWhile this solution uses CPU-based NumPy, **GPU acceleration is not typically required for basic array multiplication** due to the simplicity of the operation. However, if this task is part of a larger machine learning pipeline (e.g., preprocessing for a neural network), consider using **PyTorch or TensorFlow** directly. These frameworks leverage GPU acceleration for tensor operations, which can significantly speed up workflows involving repeated or large-scale computations. For example, replacing `np.prod` with `torch.prod` on GPU tensors would automatically utilize CUDA for faster execution.\", 'options': []}, {'role': 'user', 'metadata': {}, 'content': 'multiply 5 dataframes each having 10 million rows and 15 features.', 'options': []}, {'role': 'assistant', 'metadata': {}, 'content': '🧠 Thinking...', 'options': []}, {'role': 'user', 'metadata': {}, 'content': 'multiply 2 dataframes each having 10 million rows and 15 features', 'options': []}, {'role': 'assistant', 'metadata': {}, 'content': \"### Standard Solution  \\nTo multiply two large dataframes efficiently, start by optimizing memory usage and leveraging parallel processing. Here's how to proceed:  \\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\n# Optimize data types (e.g., float32 instead of float64)\\ndf1 = pd.read_pickle('df1.pkl').astype(np.float32)\\ndf2 = pd.read_pickle('df2.pkl').astype(np.float32)\\n\\n# Perform element-wise multiplication\\nresult = df1 * df2\\n```\\n\\n**Key Steps:**  \\n1. **Data Type Optimization:** Convert columns to smaller data types (e.g., `float32`) to reduce memory usage.  \\n2. **Chunking:** If the data exceeds memory, use `chunksize` in `pd.read_pickle` or `dask.dataframe` to process data in smaller batches.  \\n3. **Parallel Processing:** Use libraries like **Dask** to distribute computation across multiple cores or nodes.  \\n\\n---\\n\\n### GPU Acceleration Note  \\nWhile direct dataframe multiplication isn’t typically GPU-accelerated in libraries like PyTorch or CuPy, you can:  \\n1. **Convert to GPU Tensors:** Use **CuPy** (a NumPy-like library for GPUs) to perform element-wise operations on the GPU:  \\n   ```python\\n   import cupy as cp\\n   df1_gpu = cp.array(df1.values)\\n   df2_gpu = cp.array(df2.values)\\n   result_gpu = df1_gpu * df2_gpu  # GPU-accelerated multiplication\\n   ```  \\n2. **Leverage Dask with GPU Support:** Use **Dask-CUDA** to parallelize operations across GPUs for massive datasets.  \\n\\n**Why This Works:**  \\n- **CuPy** mirrors NumPy's API but runs on the GPU, offering significant speedups for numerical operations.  \\n- **Dask** scales computation across clusters, making it ideal for datasets larger than memory.  \\n\\n**Next Step:** If your workflow involves repeated numerical operations (e.g., matrix math, aggregations), consider migrating to GPU-native libraries like CuPy or PyTorch for end-to-end acceleration.\", 'options': []}, {'role': 'user', 'metadata': {}, 'content': 'How does this work?', 'options': []}, {'role': 'assistant', 'metadata': {}, 'content': '🧠 Thinking...', 'options': []}]\n",
      "--> [Router Agent] Chose path: FOLLOW_UP\n",
      "--> [Simple Path] Handling simple follow-up.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Cell 6: Gradio User Interface\n",
    "#\n",
    "# Description: This cell builds the complete Gradio app, integrating all agents.\n",
    "# It includes controls for creating and selecting user profiles, a chat interface for\n",
    "# posing questions to the tutor, and logic to connect the selected user profile to\n",
    "# the RAG pipeline.\n",
    "#\n",
    "\n",
    "# --- Instantiate Agent 1 for use in the UI ---\n",
    "profiler_agent = UserProfilerAgent_V3()\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft(), css=\".gradio-container {background-color: #f5f5f5;}\") as demo:\n",
    "    \n",
    "    # --- State Management for user profiles ---\n",
    "    user_state = gr.State({\n",
    "        \"all_users\": profiler_agent.get_user_list(),\n",
    "        \"current_user\": None\n",
    "    })\n",
    "\n",
    "    gr.Markdown(\"# 🤖 AI Accelerated Data Science Tutor\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            gr.Markdown(\"Ask a question about a data science task. For a personalized response, create or select a user profile.\")\n",
    "        with gr.Column(scale=2, min_width=300):\n",
    "            # --- User Management UI ---\n",
    "            user_dropdown = gr.Dropdown(\n",
    "                label=\"Current User\",\n",
    "                choices=user_state.value[\"all_users\"],\n",
    "                interactive=True\n",
    "            )\n",
    "            with gr.Row():\n",
    "                new_user_btn = gr.Button(\"✚ New / Update User\")\n",
    "\n",
    "    gr.Markdown(\"---\")\n",
    "\n",
    "    # --- Main Chat UI ---\n",
    "    chatbot = gr.Chatbot(label=\"Conversation\", height=500, type=\"messages\")\n",
    "\n",
    "    with gr.Accordion(\"🔎 Show Agent's Thought Process\", open=False):\n",
    "        cot_output = gr.Markdown(\"The agent's reasoning will appear here after it responds.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        msg_textbox = gr.Textbox(\n",
    "            label=\"Your Question\",\n",
    "            placeholder=\"e.g., How do I multiply two 10x10 arrays in Python?\",\n",
    "            scale=4,\n",
    "            lines=2,\n",
    "            container=False\n",
    "        )\n",
    "        with gr.Column():\n",
    "            submit_btn = gr.Button(\"Ask Tutor\", variant=\"primary\", scale=1, min_width=150)\n",
    "            clear_btn = gr.Button(\"✚ Clear Conversation\")\n",
    "\n",
    "    # --- Questionnaire Modal (Hidden by default) ---\n",
    "    with gr.Group(visible=False) as profiler_ui_group:\n",
    "        with gr.Blocks() as profiler_modal:\n",
    "            gr.Markdown(\"### User Profile Questionnaire\")\n",
    "            gr.Markdown(\"Please answer the following questions to help me tailor my explanations to your knowledge level.\")\n",
    "            profiler_name_input = gr.Textbox(label=\"First and Last Name\")\n",
    "            \n",
    "            q_inputs = [gr.Textbox(label=f\"Question {i+1}\", visible=False) for i in range(5)]\n",
    "\n",
    "            submit_profile_btn = gr.Button(\"Submit Profile\")\n",
    "            cancel_profile_btn = gr.Button(\"Cancel\")\n",
    "\n",
    "    # ==================================\n",
    "    # GRADIO HANDLER FUNCTIONS\n",
    "    # ==================================\n",
    "\n",
    "    # REPLACE BOTH HANDLER FUNCTIONS IN YOUR GRADIO UI CELL WITH THESE\n",
    "\n",
    "    # --- NEW: Function 1 - Instant UI Update (using ChatMessage) ---\n",
    "    def on_user_submit(user_message, chat_history):\n",
    "        chat_history = chat_history or []\n",
    "        # Append the user's message as a ChatMessage object\n",
    "        chat_history.append(gr.ChatMessage(role=\"user\", content=user_message))\n",
    "        # Append the \"Thinking...\" placeholder as a ChatMessage object\n",
    "        chat_history.append(gr.ChatMessage(role=\"assistant\", content=\"🧠 Thinking...\"))\n",
    "        # This function returns instantly, guaranteeing the UI updates\n",
    "        return gr.update(value=\"\"), chat_history\n",
    "\n",
    "    # --- NEW: Function 2 - Backend Processing (using ChatMessage for streaming) ---\n",
    "    def get_bot_response(chat_history, selected_user):\n",
    "        print(chat_history)\n",
    "        user_message = chat_history[-2][\"content\"] # Get content from the ChatMessage object\n",
    "\n",
    "        # --- Initial check for selected user ---\n",
    "        user_profile_path = os.path.join(profiler_agent.profiles_dir, \"_\".join(selected_user.lower().split()) + \".txt\") if selected_user else None\n",
    "        if not user_profile_path or not os.path.exists(user_profile_path):\n",
    "             chat_history[-1][\"content\"] = \"⚠️ **Warning:** Could not find the selected user profile.\"\n",
    "             yield chat_history, \"User profile not found.\"\n",
    "             return\n",
    "\n",
    "        # --- Call the backend generator ---\n",
    "        response_generator = run_full_pipeline(user_message, chat_history, user_profile_path)\n",
    "        \n",
    "        benchmark_data = {\"cpu_time\": None, \"gpu_time\": None}\n",
    "        llm_answer_content = \"\"\n",
    "        benchmark_message_index = -1\n",
    "        thought_process = \"Thinking...\"\n",
    "\n",
    "        for update in response_generator:\n",
    "            status = update.get(\"status\")\n",
    "            content = update.get(\"content\", \"\")\n",
    "            \n",
    "            think_match = re.search(r\"<think>(.*?)</think>\", content, re.DOTALL)\n",
    "            if think_match:\n",
    "                thought_process = think_match.group(1).strip()\n",
    "            \n",
    "            clean_content = re.sub(r\"<think>.*?</think>\", \"\", content, flags=re.DOTALL).strip()\n",
    "\n",
    "            if status == \"llm_complete\":\n",
    "                # Update the \"Thinking...\" bubble with the final text answer\n",
    "                chat_history[-1][\"content\"] = clean_content\n",
    "                yield chat_history, thought_process\n",
    "            \n",
    "            elif status == \"benchmark_running\":\n",
    "                # Append a new, titled message bubble for the benchmark status\n",
    "                chat_history.append(gr.ChatMessage(\n",
    "                    role=\"assistant\",\n",
    "                    content=clean_content,\n",
    "                    metadata={\"title\": \"⏳ Benchmark in Progress\"}\n",
    "                ))\n",
    "                benchmark_message_index = len(chat_history) - 1\n",
    "                yield chat_history, thought_process\n",
    "            \n",
    "            elif status == \"complete\" or status == \"error\":\n",
    "                if status == \"complete\":\n",
    "                    job_type, time_val = update.get(\"job_type\"), update.get(\"time\")\n",
    "                    if job_type == \"gpu\": benchmark_data[\"gpu_time\"] = time_val\n",
    "                    elif job_type == \"cpu\": benchmark_data[\"cpu_time\"] = time_val\n",
    "                else:\n",
    "                    benchmark_data[\"error\"] = update.get('message')\n",
    "\n",
    "                # Re-create the results table\n",
    "                gpu_time_str = f\"{benchmark_data['gpu_time']:.4f}s\" if benchmark_data[\"gpu_time\"] is not None else \"In Progress...\"\n",
    "                cpu_time_str = f\"{benchmark_data['cpu_time']:.4f}s\" if benchmark_data[\"cpu_time\"] is not None else \"In Progress...\"\n",
    "                \n",
    "                benchmark_md = f\"\"\"\n",
    "| Metric | Result |\n",
    "|---|---|\n",
    "| GPU Time | {gpu_time_str} |\n",
    "| CPU Time | {cpu_time_str} |\n",
    "\"\"\"\n",
    "                if benchmark_data.get(\"cpu_time\") and benchmark_data.get(\"gpu_time\"):\n",
    "                    speedup = benchmark_data[\"cpu_time\"] / benchmark_data[\"gpu_time\"]\n",
    "                    benchmark_md += f\"| **Speedup** | **{speedup:.2f}x faster on GPU!** |\\n\"\n",
    "                \n",
    "                if benchmark_data.get(\"error\"):\n",
    "                    benchmark_md = f\"⚠️ Benchmark Error: {benchmark_data['error']}\"\n",
    "\n",
    "                # Update the benchmark status bubble in-place with the live table\n",
    "                if benchmark_message_index != -1:\n",
    "                    chat_history[benchmark_message_index].content = benchmark_md\n",
    "                    # Optionally, update the title when complete\n",
    "                    chat_history[benchmark_message_index].metadata = {\"title\": \"📊 Live Benchmark Results\"}\n",
    "\n",
    "                yield chat_history, thought_process\n",
    "\n",
    "    def clear_chat():\n",
    "        return None, \"\"\n",
    "\n",
    "    # --- User Profile Logic ---\n",
    "    def start_profiling_flow(current_user):\n",
    "        \"\"\"Called when 'New / Update User' is clicked.\"\"\"\n",
    "        questions = profiler_agent._generate_questions_with_llm()\n",
    "        \n",
    "        updates = [gr.update(visible=True)]\n",
    "        updates.append(gr.update(value=current_user if current_user else \"\", visible=True))\n",
    "        \n",
    "        for i in range(5):\n",
    "            if i < len(questions):\n",
    "                updates.append(gr.update(label=questions[i], value=\"\", visible=True))\n",
    "            else:\n",
    "                updates.append(gr.update(visible=False))\n",
    "        \n",
    "        return *updates, questions\n",
    "\n",
    "    # --- FIX 1: Add 'questions' to the function signature to receive it from the state component. ---\n",
    "    def process_profile_submission(user_name, state, questions, *answers):\n",
    "        \"\"\"Called when 'Submit Profile' is clicked.\"\"\"\n",
    "        \n",
    "        if not user_name:\n",
    "            gr.Warning(\"User name cannot be empty!\")\n",
    "            return state, gr.update(choices=state[\"all_users\"], value=state[\"current_user\"]), gr.update(visible=True)\n",
    "\n",
    "        answers_dict = {q: a for q, a in zip(questions, answers) if q and a}\n",
    "        profiler_agent.generate_and_save_report(user_name, answers_dict)\n",
    "        \n",
    "        gr.Info(f\"Profile for {user_name} has been saved!\")\n",
    "\n",
    "        updated_users = profiler_agent.get_user_list()\n",
    "        state[\"all_users\"] = updated_users\n",
    "        state[\"current_user\"] = user_name\n",
    "        \n",
    "        return state, gr.update(choices=updated_users, value=user_name), gr.update(visible=False)\n",
    "\n",
    "    def cancel_profiling():\n",
    "        return gr.update(visible=False)\n",
    "\n",
    "    def update_current_user(selected_user, state):\n",
    "        state[\"current_user\"] = selected_user\n",
    "        gr.Info(f\"Switched to user: {selected_user}\")\n",
    "        return state\n",
    "\n",
    "    # ==================================\n",
    "    # WIRING UP THE UI EVENTS\n",
    "    # ==================================\n",
    "    \n",
    "    question_state = gr.State([])\n",
    "\n",
    "    clear_btn.click(clear_chat, outputs=[chatbot, cot_output], queue=False)\n",
    "\n",
    "    submit_btn.click(\n",
    "        on_user_submit,\n",
    "        [msg_textbox, chatbot],\n",
    "        [msg_textbox, chatbot]\n",
    "    ).then(\n",
    "        get_bot_response,\n",
    "        [chatbot, user_dropdown],\n",
    "        [chatbot, cot_output],\n",
    "        show_progress=\"hidden\"\n",
    "    )\n",
    "\n",
    "    msg_textbox.submit(\n",
    "        on_user_submit,\n",
    "        [msg_textbox, chatbot],\n",
    "        [msg_textbox, chatbot]\n",
    "    ).then(\n",
    "        get_bot_response,\n",
    "        [chatbot, user_dropdown],\n",
    "        [chatbot, cot_output],\n",
    "        show_progress=\"hidden\"\n",
    "    )\n",
    "\n",
    "    new_user_btn.click(\n",
    "        start_profiling_flow,\n",
    "        inputs=[user_dropdown],\n",
    "        outputs=[profiler_ui_group, profiler_name_input] + q_inputs + [question_state]\n",
    "    )\n",
    "    \n",
    "    submit_profile_btn.click(\n",
    "        process_profile_submission,\n",
    "        # --- FIX 2: Pass the 'question_state' component as an input. ---\n",
    "        inputs=[profiler_name_input, user_state, question_state, *q_inputs],\n",
    "        outputs=[user_state, user_dropdown, profiler_ui_group]\n",
    "    )\n",
    "\n",
    "    cancel_profile_btn.click(cancel_profiling, outputs=[profiler_ui_group])\n",
    "\n",
    "    user_dropdown.change(\n",
    "        update_current_user,\n",
    "        inputs=[user_dropdown, user_state],\n",
    "        outputs=[user_state]\n",
    "    )\n",
    "\n",
    "# --- Launch the Application ---\n",
    "demo.queue().launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c952e1-e1a5-4ac2-bb51-e9d19f8c03d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"numpy<2\"\n",
    "# !pip install --upgrade numba"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai25.07",
   "language": "python",
   "name": "genai25.07"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
