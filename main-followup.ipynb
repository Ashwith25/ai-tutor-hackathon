{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45098534-76e6-4aa1-9131-b0cf1b5458af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ff2cdb-7323-44f5-bec4-eafe098f0c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install googlesearch-python lxml deepspeed sentence-transformers \n",
    "# !pip install feedparser sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cb884c2-8e0d-459c-a2d2-07d56734e661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "#  \n",
    "# Cell 2: Imports\n",
    "#\n",
    "# Description: All required libraries for the application are imported here.\n",
    "#\n",
    "\n",
    "import os\n",
    "import json\n",
    "import socket\n",
    "from typing import List, Dict\n",
    "import subprocess\n",
    "import time\n",
    "import re\n",
    "import gradio as gr\n",
    "\n",
    "# LangChain and related libraries\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Search library\n",
    "try:\n",
    "    from googlesearch import search\n",
    "except ImportError:\n",
    "    print(\"Error: 'googlesearch-python' is not installed. Please run 'pip install googlesearch-python'\")\n",
    "\n",
    "# Set a user agent to avoid being blocked by Google search\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4351c5cb-e018-44cf-a752-b6fc9919a648",
   "metadata": {},
   "source": [
    "## Agent 1 - Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cdcd502-f05b-4322-b9c7-6d13ed1c802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Cell 3: Agent 1 - User Profiler\n",
    "#\n",
    "# Description: This agent is responsible for understanding the user's knowledge level.\n",
    "# It dynamically generates a questionnaire, analyzes the answers, and creates a\n",
    "# profile that will be used by Agent 3 to tailor its responses.\n",
    "#\n",
    "\n",
    "class Questionnaire(BaseModel):\n",
    "    questions: List[str] = Field(description=\"A list of 4-5 questions for the user.\")\n",
    "\n",
    "class UserProfilerAgent_V3:\n",
    "    \"\"\"\n",
    "    Agent 1 (V3): Guarantees the user's name is collected first before\n",
    "    using an LLM to dynamically generate the rest of the questionnaire.\n",
    "    **ADAPTED FOR MODULAR, GRADIO-FRIENDLY USE.**\n",
    "    \"\"\"\n",
    "    def __init__(self, profiles_dir: str = \"user_profiles\"):\n",
    "        self.profiles_dir = profiles_dir\n",
    "        if not os.path.exists(self.profiles_dir):\n",
    "            os.makedirs(self.profiles_dir)\n",
    "\n",
    "        try:\n",
    "            host_node = socket.gethostname()\n",
    "            # NOTE: The ASURITE ID should be that of the user running the Ollama server.\n",
    "            # This is specified as a hackathon resource.\n",
    "            asurite_id = \"apoojar4\"\n",
    "            self.llm = ChatOllama(model=\"qwen3:14b\", base_url=f\"http://{asurite_id}@{host_node}:11434/\")\n",
    "            self.structured_llm = self.llm.with_structured_output(Questionnaire)\n",
    "            print(\"✅ [Agent 1] Successfully connected to Ollama LLM.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ [Agent 1] Error connecting to Ollama: {e}\")\n",
    "            self.llm = None\n",
    "\n",
    "    def get_user_list(self) -> List[str]:\n",
    "        \"\"\"Scans the profiles directory and returns a list of user names.\"\"\"\n",
    "        if not os.path.exists(self.profiles_dir):\n",
    "            return []\n",
    "        files = [f for f in os.listdir(self.profiles_dir) if f.endswith('.txt')]\n",
    "        # Convert 'first_last.txt' to 'First Last'\n",
    "        names = [\" \".join(f.replace('.txt', '').split('_')).title() for f in files]\n",
    "        return names\n",
    "\n",
    "    def _generate_questions_with_llm(self) -> List[str]:\n",
    "        \"\"\"Uses an LLM to dynamically generate a user questionnaire.\"\"\"\n",
    "        if not self.llm: # Fallback if LLM is not available\n",
    "            return [\n",
    "                \"On a scale of 1-5, how comfortable are you with Python?\",\n",
    "                \"Which Python data science libraries (like Pandas or NumPy) have you used before?\",\n",
    "                \"Have you ever heard of using GPUs to speed up data analysis?\",\n",
    "                \"What's the first tool you'd reach for to do a large matrix multiplication in Python?\"\n",
    "            ]\n",
    "            \n",
    "        print(\"\\n🤖 [Agent 1] Generating a personalized questionnaire...\")\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"\"\"\n",
    "            You are a helpful assistant for an AI Data Science Tutor. Your goal is to create a short questionnaire (4-5 questions) to understand a user's knowledge level.\n",
    "            The questions should gently probe their experience with:\n",
    "            1. The Python programming language.\n",
    "            2. Common CPU-based data science libraries (like NumPy, Pandas).\n",
    "            3. Their awareness of GPU computing and hardware acceleration.\n",
    "            4. Their familiarity with any NVIDIA-specific GPU libraries (like CuPy or RAPIDS).\n",
    "            IMPORTANT: Do NOT ask for the user's name, as it will be collected separately.\n",
    "            Return the questions as a JSON list. Be conversational and friendly.\n",
    "            \"\"\",\n",
    "            input_variables=[],\n",
    "        )\n",
    "        query_generation_chain = prompt | self.structured_llm\n",
    "        try:\n",
    "            response_model = query_generation_chain.invoke({})\n",
    "            return response_model.questions\n",
    "        except Exception as e:\n",
    "            print(f\"-> [Agent 1] LLM failed to generate questions, falling back to default. Error: {e}\")\n",
    "            return [\n",
    "                \"On a scale of 1-5, how comfortable are you with Python?\",\n",
    "                \"Which Python data science libraries (like Pandas or NumPy) have you used before?\",\n",
    "                \"Have you ever heard of using GPUs to speed up data analysis?\",\n",
    "                \"What's the first tool you'd reach for to do a large matrix multiplication in Python?\"\n",
    "            ]\n",
    "\n",
    "    def generate_and_save_report(self, user_name: str, answers_dict: dict) -> str:\n",
    "        \"\"\"\n",
    "        Takes a user name and a dictionary of answers, generates a report with an LLM,\n",
    "        and saves it to a file. Returns the path to the saved file.\n",
    "        \"\"\"\n",
    "        if not self.llm:\n",
    "            return \"Error: LLM not connected.\"\n",
    "\n",
    "        print(f\"\\n🤖 [Agent 1] Analyzing responses for {user_name} and creating a profile...\")\n",
    "        answers_str = \"\\n\".join([f\"- {q}: {a}\" for q, a in answers_dict.items()])\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"\"\"\n",
    "            You are an expert AI analyst. A user named {user_name} has answered a questionnaire about their data science skills.\n",
    "            Your task is to analyze their answers and generate a \"TUTORING STRATEGY\" report for our AI Tutor.\n",
    "            **User's Answers:**\n",
    "            {answers}\n",
    "            **Your Task:**\n",
    "            1.  Determine the user's knowledge level: 'Beginner', 'Intermediate', or 'Advanced'.\n",
    "            2.  Write a concise report following the correct strategy format below. This report will be given to another AI, so the instructions must be clear.\n",
    "            ---\n",
    "            **STRATEGY FORMATS (Choose ONE):**\n",
    "            **If 'Beginner':**\n",
    "            Start with `Knowledge Level: Beginner`. On the next line, start with `TUTORING STRATEGY: The user is a beginner.` Then, explain that the tutor should use high-level concepts, explain the 'why' of GPU acceleration, and introduce NVIDIA libraries (like CuPy) as a simple, powerful alternative.\n",
    "            **If 'Intermediate':**\n",
    "            Start with `Knowledge Level: Intermediate`. On the next line, start with `TUTORING STRATEGY: The user is at an intermediate level.` Then, explain that the tutor should provide direct code comparisons (e.g., NumPy vs. CuPy), focus on performance benefits, and show clear benchmarking examples.\n",
    "            **If 'Advanced':**\n",
    "            Start with `Knowledge Level: Advanced`. On the next line, start with `TUTORING STRATEGY: The user is advanced.` Then, explain that the tutor can provide nuanced advice, discuss the broader NVIDIA RAPIDS ecosystem, and cover specific benchmarking methodologies on the Sol supercomputer.\n",
    "            ---\n",
    "            Now, generate the complete report.\n",
    "            \"\"\",\n",
    "            input_variables=[\"user_name\", \"answers\"],\n",
    "        )\n",
    "        report_generation_chain = prompt | self.llm\n",
    "        response_message = report_generation_chain.invoke({\"user_name\": user_name, \"answers\": answers_str})\n",
    "        report_content = response_message.content\n",
    "        full_report = f\"--- User Profile for {user_name} ---\\n{report_content}\\n--- End of Profile ---\"\n",
    "\n",
    "        # Save the report to a text file\n",
    "        filename = \"_\".join(user_name.lower().split()) + \".txt\"\n",
    "        filepath = os.path.join(self.profiles_dir, filename)\n",
    "        with open(filepath, \"w\") as f:\n",
    "            f.write(full_report)\n",
    "        print(f\"\\n✅ [Agent 1] User profile report saved successfully to: {filepath}\")\n",
    "        return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eadc4b8-19c6-401e-ab23-ff3977c29f9b",
   "metadata": {},
   "source": [
    "## Agent 4 - Benchmarker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d122e961-c4af-4a01-9299-0e419a6b1412",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolBenchmarker:\n",
    "    def __init__(self, user: str, python_env: str = \"rapids25.02\"):\n",
    "        if not user or user == \"YOUR_ASURITE_ID\":\n",
    "            raise ValueError(\"A valid ASURITE username is required for SolBenchmarker.\")\n",
    "        self.user = user\n",
    "        self.python_env = python_env\n",
    "\n",
    "    def _generate_sbatch_script(self, script_dir: str, script_name: str, job_type: str) -> str:\n",
    "        # This function now generates a specific script for either a CPU or GPU job\n",
    "        if job_type == \"gpu\":\n",
    "            # The GPU job requests a GPU resource\n",
    "            resources = \"#SBATCH -G 1\"\n",
    "        else: # cpu\n",
    "            # The CPU job requests a standard node without a GPU\n",
    "            resources = \"#SBATCH --nodes=1\" \n",
    "\n",
    "        return f\"\"\"#!/bin/bash\n",
    "#SBATCH -p general\n",
    "#SBATCH -q public\n",
    "{resources}\n",
    "#SBATCH -A grp_hackathon2025\n",
    "#SBATCH --reservation=hackathon2025\n",
    "#SBATCH -t 0-00:10:00\n",
    "#SBATCH -c 1\n",
    "#SBATCH -o {script_dir}/slurm-{job_type}-%j.out\n",
    "#SBATCH -e {script_dir}/slurm-{job_type}-%j.err\n",
    "\n",
    "module load mamba/latest\n",
    "source activate {self.python_env}\n",
    "\n",
    "echo \"--- STARTING {job_type.upper()} BENCHMARK ---\"\n",
    "/usr/bin/time -p python3 {script_dir}/{script_name} 2>&1\n",
    "echo \"--- FINISHED {job_type.upper()} BENCHMARK ---\"\n",
    "\"\"\"\n",
    "\n",
    "    def _submit_job(self, code: str, job_type: str, benchmark_dir: str) -> str:\n",
    "        # Helper to submit a single job and return its ID\n",
    "        script_name = f\"{job_type}_benchmark.py\"\n",
    "        script_path = os.path.join(benchmark_dir, script_name)\n",
    "        sbatch_path = os.path.join(benchmark_dir, f\"{job_type}_job.sh\")\n",
    "        \n",
    "        with open(script_path, \"w\") as f: f.write(code)\n",
    "        sbatch_content = self._generate_sbatch_script(benchmark_dir, script_name, job_type)\n",
    "        with open(sbatch_path, \"w\") as f: f.write(sbatch_content)\n",
    "\n",
    "        process = subprocess.run(f\"sbatch {sbatch_path}\", shell=True, capture_output=True, text=True)\n",
    "        if process.returncode != 0: raise RuntimeError(f\"sbatch submission for {job_type} failed: {process.stderr}\")\n",
    "        \n",
    "        job_id_match = re.search(r\"Submitted batch job (\\\\d+)\", process.stdout.strip())\n",
    "        if not job_id_match: raise RuntimeError(f\"Could not parse Job ID for {job_type}: {process.stdout}\")\n",
    "        \n",
    "        job_id = job_id_match.group(1)\n",
    "        print(f\"--> [Agent 4] Submitted {job_type.upper()} job with ID: {job_id}\")\n",
    "        return job_id\n",
    "\n",
    "    def _check_job_completion(self, job_id: str, job_type: str, benchmark_dir: str) -> dict:\n",
    "        # Helper to check if a job is done and parse its output\n",
    "        queue_process = subprocess.run(f\"squeue -u {self.user} -j {job_id}\", shell=True, capture_output=True, text=True)\n",
    "        if job_id not in queue_process.stdout:\n",
    "            print(f\"--> [Agent 4] {job_type.upper()} Job {job_id} completed.\")\n",
    "            output_file_path = os.path.join(benchmark_dir, f\"slurm-{job_type}-{job_id}.out\")\n",
    "            if os.path.exists(output_file_path):\n",
    "                with open(output_file_path, \"r\") as f:\n",
    "                    output_content = f.read()\n",
    "                real_time_match = re.search(r\"real\\s+([\\d.]+)\", output_content)\n",
    "                time_val = float(real_time_match.group(1)) if real_time_match else None\n",
    "                return {\"status\": \"complete\", \"job_type\": job_type, \"time\": time_val}\n",
    "            else:\n",
    "                return {\"status\": \"error\", \"job_type\": job_type, \"message\": \"Output file not found.\"}\n",
    "        return {\"status\": \"running\"}\n",
    "\n",
    "    def run_benchmark_parallel(self, cpu_code: str, gpu_code: str):\n",
    "        # The main method, now a generator that yields results as they complete.\n",
    "        benchmark_dir = os.path.join(os.getcwd(), \"benchmark_files\")\n",
    "        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            gpu_job_id = self._submit_job(gpu_code, \"gpu\", benchmark_dir)\n",
    "            cpu_job_id = self._submit_job(cpu_code, \"cpu\", benchmark_dir)\n",
    "\n",
    "            active_jobs = {\"gpu\": gpu_job_id, \"cpu\": cpu_job_id}\n",
    "\n",
    "            while active_jobs:\n",
    "                # Check the status of each active job\n",
    "                for job_type, job_id in list(active_jobs.items()):\n",
    "                    result = self._check_job_completion(job_id, job_type, benchmark_dir)\n",
    "                    if result[\"status\"] != \"running\":\n",
    "                        yield result # Yield the result as soon as a job finishes\n",
    "                        del active_jobs[job_type] # Remove from active monitoring\n",
    "                \n",
    "                if active_jobs:\n",
    "                    time.sleep(5) # Wait before polling again\n",
    "\n",
    "        except Exception as e:\n",
    "            yield {\"status\": \"error\", \"message\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67cd2448-7084-4dec-a1ae-645cafad0657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# # Cell 4: Agent 4 - Sol Benchmarker\n",
    "# #\n",
    "# # Description: This agent is responsible for executing code on the Sol supercomputer\n",
    "# # to benchmark the performance difference between CPU and GPU implementations. It writes\n",
    "# # and submits a SLURM batch script.\n",
    "# #\n",
    "\n",
    "# class SolBenchmarker:\n",
    "#     def __init__(self, user: str, python_env: str = \"rapids25.02\"): # Note: Kernel name from docs\n",
    "#         if not user or user == \"YOUR_ASURITE_ID\":\n",
    "#             raise ValueError(\"A valid ASURITE username is required for SolBenchmarker.\")\n",
    "#         self.user = user\n",
    "#         self.python_env = python_env\n",
    "\n",
    "#     def _generate_sbatch_script(self, script_dir: str, cpu_script_name: str, gpu_script_name: str) -> str:\n",
    "#         # This SLURM script is configured according to the hackathon's resources \n",
    "#         return f\"\"\"#!/bin/bash\n",
    "# #SBATCH -p general\n",
    "# #SBATCH -q public\n",
    "# #SBATCH -G 1\n",
    "# #SBATCH -A grp_hackathon2025\n",
    "# #SBATCH --reservation=hackathon2025\n",
    "# #SBATCH -t 0-00:10:00\n",
    "# #SBATCH -c 1\n",
    "# #SBATCH -o {script_dir}/slurm-%j.out\n",
    "# #SBATCH -e {script_dir}/slurm-%j.err\n",
    "\n",
    "# module load mamba/latest\n",
    "# source activate {self.python_env}\n",
    "\n",
    "# echo \"--- STARTING CPU BENCHMARK ---\"\n",
    "# /usr/bin/time -p python3 {script_dir}/{cpu_script_name} 2>&1\n",
    "# echo \"--- FINISHED CPU BENCHMARK ---\"\n",
    "\n",
    "# echo \"\"\n",
    "# echo \"--- STARTING GPU BENCHMARK ---\"\n",
    "# /usr/bin/time -p python3 {script_dir}/{gpu_script_name} 2>&1\n",
    "# echo \"--- FINISHED GPU BENCHMARK ---\"\n",
    "# \"\"\"\n",
    "#     def _parse_output(self, output_content: str) -> dict:\n",
    "#         try:\n",
    "#             # --- FIX 1: Removed erroneous backslash before the string literal ---\n",
    "#             # This regex finds the execution time from the '/usr/bin/time' command output.\n",
    "#             real_times = re.findall(r\"real\\s+([\\d.]+)\", output_content)\n",
    "            \n",
    "#             cpu_time = float(real_times[0]) if len(real_times) > 0 else None\n",
    "#             gpu_time = float(real_times[1]) if len(real_times) > 1 else None\n",
    "#             print(\"cpu_time_seconds\", cpu_time, \"gpu_time_seconds\", gpu_time)\n",
    "#             return {\"status\": \"success\", \"cpu_time_seconds\": cpu_time, \"gpu_time_seconds\": gpu_time}\n",
    "#         except (IndexError, ValueError) as e:\n",
    "#             return {\"status\": \"error\", \"message\": f\"Failed to parse benchmark times. Error: {e}\", \"raw_log\": output_content}\n",
    "\n",
    "#     def run_benchmark(self, cpu_code: str, gpu_code: str) -> dict:\n",
    "#         benchmark_dir = os.path.join(os.getcwd(), \"benchmark_files\")\n",
    "#         os.makedirs(benchmark_dir, exist_ok=True)\n",
    "        \n",
    "#         cpu_script_path = os.path.join(benchmark_dir, \"cpu_benchmark.py\")\n",
    "#         gpu_script_path = os.path.join(benchmark_dir, \"gpu_benchmark.py\")\n",
    "#         sbatch_path = os.path.join(benchmark_dir, \"benchmark_job.sh\")\n",
    "\n",
    "#         try:\n",
    "#             with open(cpu_script_path, \"w\") as f: f.write(cpu_code)\n",
    "#             with open(gpu_script_path, \"w\") as f: f.write(gpu_code)\n",
    "#             sbatch_script = self._generate_sbatch_script(benchmark_dir, \"cpu_benchmark.py\", \"gpu_benchmark.py\")\n",
    "#             with open(sbatch_path, \"w\") as f: f.write(sbatch_script)\n",
    "\n",
    "#             process = subprocess.run(f\"sbatch {sbatch_path}\", shell=True, capture_output=True, text=True)\n",
    "#             if process.returncode != 0: raise RuntimeError(f\"sbatch submission failed: {process.stderr}\")\n",
    "\n",
    "#             # --- FIX 2: Removed erroneous backslash before the string literal ---\n",
    "#             # This regex finds the Job ID from the sbatch submission output.\n",
    "#             job_id_match = re.search(r\"Submitted batch job (\\d+)\", process.stdout.strip())\n",
    "\n",
    "#             if not job_id_match: raise RuntimeError(f\"Could not parse Job ID from sbatch output: {process.stdout}\")\n",
    "#             job_id = job_id_match.group(1)\n",
    "#             print(f\"--> [Agent 4] Submitted benchmark job to SLURM with ID: {job_id}\")\n",
    "\n",
    "#             print(\"--> [Agent 4] Waiting for job to complete...\")\n",
    "#             while True:\n",
    "#                 queue_process = subprocess.run(f\"squeue -u {self.user} -j {job_id}\", shell=True, capture_output=True, text=True)\n",
    "#                 if job_id not in queue_process.stdout: break\n",
    "#                 time.sleep(10)\n",
    "\n",
    "#             print(f\"--> [Agent 4] Job {job_id} completed.\")\n",
    "#             output_file_path = os.path.join(benchmark_dir, f\"slurm-{job_id}.out\")\n",
    "            \n",
    "#             if not os.path.exists(output_file_path):\n",
    "#                  err_file_path = os.path.join(benchmark_dir, f\"slurm-{job_id}.err\")\n",
    "#                  if os.path.exists(err_file_path):\n",
    "#                      with open(err_file_path, \"r\") as f: error_content = f.read()\n",
    "#                      return {\"status\": \"error\", \"message\": f\"Job failed. See error log: {error_content}\"}\n",
    "#                  return {\"status\": \"error\", \"message\": f\"Output file not found.\"}\n",
    "\n",
    "#             with open(output_file_path, \"r\") as f: output_content = f.read()\n",
    "#             return self._parse_output(output_content)\n",
    "#         except Exception as e:\n",
    "#             return {\"status\": \"error\", \"message\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dd130d8-4e78-48ce-9ce5-6bd897dbd2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEW: Code Upscaler Function with Strict Constraints ---\n",
    "def _upscale_code_agentic(original_code: str, llm, library_string: str) -> str:\n",
    "    print(\"--> [Agent 3] Invoking LLM with STRICT CONSTRAINTS to upscale code...\")\n",
    "    \n",
    "    # This class is used to ensure the LLM returns only a code string.\n",
    "    class CodeOutput(BaseModel):\n",
    "        code: str = Field(description=\"The complete, upscaled Python code.\")\n",
    "        \n",
    "    # THE NEW, MORE SPECIFIC PROMPT\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are a benchmark preparation assistant. Your task is to analyze the following Python script and make it suitable for a meaningful performance benchmark.\n",
    "\n",
    "        **Your instructions are very strict:**\n",
    "        1.  **DO NOT** change any of the `import` statements.\n",
    "        2.  **ONLY** find the variable that defines the primary data size (e.g., `size = 10`, `n_rows = 1000`).\n",
    "        3.  Change the value of that single variable to a much larger number to ensure the benchmark is meaningful. For example, change matrix dimensions to at least 8000 or DataFrame rows to over 5 million.\n",
    "        4.  Return the complete, modified Python script and nothing else.\n",
    "\n",
    "        **Constraint:** The final code must only use libraries from this list: {available_libraries}\n",
    "\n",
    "        Original Code:\n",
    "        ```python\n",
    "        {original_code}\n",
    "        ```\n",
    "\n",
    "        Upscaled Code:\n",
    "        \"\"\",\n",
    "        input_variables=[\"original_code\", \"available_libraries\"],\n",
    "    )\n",
    "    \n",
    "    # The chain now uses the new, stricter prompt\n",
    "    structured_chain = prompt_template | llm.with_structured_output(CodeOutput)\n",
    "    \n",
    "    try:\n",
    "        response_model = structured_chain.invoke({\n",
    "            \"original_code\": original_code,\n",
    "            \"available_libraries\": library_string\n",
    "        })\n",
    "        upscaled_code = response_model.code\n",
    "        print(\"--> [Agent 3] Code successfully upscaled.\")\n",
    "        return upscaled_code\n",
    "    except Exception as e:\n",
    "        print(f\"--> [Agent 3] Could not upscale code, using original. Error: {e}\")\n",
    "        # Fallback to original code if upscaling fails\n",
    "        return original_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d282d80-7f9b-4ed0-9714-2430235e8602",
   "metadata": {},
   "source": [
    "## Agent 2 - Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e908618-cce9-4711-b02f-daba7f1fdbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# AGENTS 2 & 3 HELPER FUNCTIONS\n",
    "# This cell contains all the supporting logic for retrieval and code processing.\n",
    "# =================================================================\n",
    "from sentence_transformers import util\n",
    "import feedparser\n",
    "\n",
    "class SearchQueryGenerator(BaseModel):\n",
    "    queries: List[str] = Field(description=\"A list of targeted, keyword-focused search queries.\")\n",
    "\n",
    "def generate_search_queries(query: str, llm) -> List[str]:\n",
    "    print(\"-> [Agent 2] Using LLM to generate search queries...\")\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are an expert at generating web search queries. Analyze the user's question to identify the core technical task and programming language.\n",
    "        Generate 3 concise, targeted search queries. One query should be for the standard, CPU-based approach. Two queries should be for GPU-accelerated approaches, prioritizing NVIDIA-based solutions if they exist.\n",
    "        User Question: \"{question}\"\n",
    "        Generate a JSON list of 3 search query strings.\n",
    "        \"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "    query_generation_chain = prompt_template | llm.with_structured_output(SearchQueryGenerator)\n",
    "    try:\n",
    "        response_model = query_generation_chain.invoke({\"question\": query})\n",
    "        print(f\"-> [Agent 2] Generated queries: {response_model.queries}\")\n",
    "        return response_model.queries\n",
    "    except Exception as e:\n",
    "        print(f\"-> [Agent 2] LLM failed to generate structured output: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- NEW: Hybrid Retriever Functions (Tier 1, 2, 3) ---\n",
    "\n",
    "def fetch_articles_from_rss(query: str, embedding_model) -> list[str]:\n",
    "    print(\"-> [Tier 1] Trying Semantic Search on RSS feeds...\")\n",
    "    RSS_FEEDS = {\n",
    "        \"NVIDIA Developer Blog\": \"https://developer.nvidia.com/blog/feed/\",\n",
    "        \"RAPIDS AI (Medium)\": \"https://medium.com/feed/rapids-ai\",\n",
    "        \"CuPy (Medium)\": \"https://medium.com/feed/cupy-team\"\n",
    "    }\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    found_articles = []\n",
    "    for name, url in RSS_FEEDS.items():\n",
    "        feed = feedparser.parse(url)\n",
    "        for entry in feed.entries:\n",
    "            entry_text = entry.title + \". \" + entry.summary\n",
    "            entry_embedding = embedding_model.embed_query(entry_text)\n",
    "            similarity = util.pytorch_cos_sim(query_embedding, entry_embedding)\n",
    "            if similarity.item() > 0.35: # Keep a minimal threshold\n",
    "                found_articles.append({'link': entry.link, 'similarity': similarity.item()})\n",
    "    \n",
    "    found_articles.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    final_urls = [article['link'] for article in found_articles[:5]]\n",
    "    print(f\"-> [Tier 1] Found {len(final_urls)} semantically relevant URLs from RSS.\")\n",
    "    return final_urls\n",
    "\n",
    "def dynamic_web_search(queries: List[str]) -> list[str]:\n",
    "    print(\"-> [Tier 2] Falling back to polite Web Search...\")\n",
    "    all_urls = set()\n",
    "    for q in queries:\n",
    "        try:\n",
    "            search_results = list(search(q, num_results=2))\n",
    "            all_urls.update(url for url in search_results if url)\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during web search: {e}\")\n",
    "            continue\n",
    "    return list(all_urls)\n",
    "\n",
    "def hybrid_retriever(query: str, llm, embedding_model) -> list[str]:\n",
    "    # 1. First, try the high-quality RSS feeds\n",
    "    urls = fetch_articles_from_rss(query, embedding_model)\n",
    "\n",
    "    # 2. If RSS finds nothing, fallback to a general web search\n",
    "    if not urls:\n",
    "        print(\"--> RSS search found no relevant articles. Falling back to web search.\")\n",
    "        search_queries = generate_search_queries(query, llm)\n",
    "        if search_queries:\n",
    "            urls = dynamic_web_search(search_queries)\n",
    "\n",
    "    # 3. If both searches fail, use a final safety net of default links\n",
    "    if not urls:\n",
    "        print(\"--> All searches failed. Using default fallback links.\")\n",
    "        urls = [\n",
    "            \"https://developer.nvidia.com/blog/icymi-leveraging-the-power-of-gpus-with-cupy-in-python/\",\n",
    "            \"https://medium.com/rapids-ai/rapids-23-08-release-23db51c255f0\"\n",
    "        ]\n",
    "        \n",
    "    print(f\"-> [Hybrid Retriever] Found {len(urls)} final URLs to use for context.\")\n",
    "    return urls\n",
    "\n",
    "# --- Other Helper Functions ---\n",
    "def _get_available_libraries(filepath: str) -> List[str]:\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"--> Library file not found at {filepath}. Using a default list.\")\n",
    "        return ['numpy', 'cupy', 'pandas', 'cudf', 'cuml', 'scipy', 'sklearn']\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    libraries = []\n",
    "    for line in lines:\n",
    "        if not line.startswith('#'):\n",
    "            package_name = line.split()[0]\n",
    "            libraries.append(package_name)\n",
    "    print(f\"--> Successfully loaded {len(libraries)} available libraries from file.\")\n",
    "    return libraries\n",
    "\n",
    "# DEFINITIVE, \"FIND ALL PAIRS\" CODE EXTRACTOR\n",
    "def _extract_python_code(markdown_text: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    The definitive helper function to parse CPU and GPU code blocks.\n",
    "    It uses a robust regex with findall to capture all heading-and-code pairs\n",
    "    and then assigns them correctly.\n",
    "    \"\"\"\n",
    "    print(\"--> [Agent 3] Extracting code blocks using definitive 'Find All Pairs' parser...\")\n",
    "    \n",
    "    cpu_code = \"\"\n",
    "    gpu_code = \"\"\n",
    "    \n",
    "    # This pattern looks for a line that starts with hashes (optional) and contains \"Solution\",\n",
    "    # then captures that heading line (group 1) and the subsequent python code block (group 2).\n",
    "    pattern = re.compile(\n",
    "        r\"(###?\\s*.*?Solution.*?)\\n*```python\\n(.*?)\\n```\", \n",
    "        re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    matches = pattern.findall(markdown_text)\n",
    "    print(matches)\n",
    "    \n",
    "    for heading, code in matches:\n",
    "        heading_lower = heading.lower()\n",
    "        if \"gpu\" in heading_lower:\n",
    "            gpu_code = code.strip()\n",
    "        elif \"cpu\" in heading_lower:\n",
    "            cpu_code = code.strip()\n",
    "            \n",
    "    # Your helpful debugging prints\n",
    "    print(\"################################################\")\n",
    "    print(\"GPU Code Found:\\n\", gpu_code)\n",
    "    print(\"################################################\")\n",
    "    print(\"CPU Code Found:\\n\", cpu_code)\n",
    "    print(\"################################################\")\n",
    "\n",
    "    return {\"cpu_code\": cpu_code, \"gpu_code\": gpu_code}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a993605-224d-4f4d-a280-2e16bc70a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resp = \"\"\"Recommended GPU Solution (Using CuPy)\n",
    "# For large matrices like 500x500, CuPy (a GPU-accelerated library) can significantly speed up computations by leveraging the parallel power of GPUs. Here's how to multiply two matrices with CuPy:\n",
    "\n",
    "\n",
    "# import cupy as cp\n",
    "\n",
    "# # Create two 500x500 matrices on the GPU\n",
    "# a = cp.random.rand(500, 500)\n",
    "# b = cp.random.rand(500, 500)\n",
    "\n",
    "# # Matrix multiplication using CuPy\n",
    "# result = a @ b  # or cp.dot(a, b)\n",
    "# print(result)\n",
    "# Key Benefits:\n",
    "\n",
    "# Speed: GPU parallelism can make this operation up to 10x faster than CPU-based NumPy for large matrices.\n",
    "# Scalability: Easily handles larger matrices (e.g., 10,000x10,000) with minimal changes to code.\n",
    "# Standard CPU Solution (Using NumPy)\n",
    "# If you're working on a CPU or don't have access to a GPU, NumPy is the standard tool for matrix operations:\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# # Create two 500x500 matrices on the CPU\n",
    "# a = np.random.rand(500, 500)\n",
    "# b = np.random.rand(500, 500)\n",
    "\n",
    "# # Matrix multiplication using NumPy\n",
    "# result = a @ b  # or np.dot(a, b)\n",
    "# print(result)\n",
    "# Performance Note:\n",
    "\n",
    "# For small matrices (like 500x500), the CPU solution is efficient and sufficient.\n",
    "# For larger matrices or repeated operations, the GPU solution with CuPy becomes significantly more efficient.\n",
    "# GPU Acceleration Note\n",
    "# Matrix multiplication is a perfect use case for GPUs due to its high parallelism. CuPy's implementation is optimized for NVIDIA GPUs, making it ideal for scientific computing, machine learning, and data analysis workflows. If you're working on larger-scale problems (e.g., 10,000x10,000 matrices), consider switching to CuPy for dramatic speedups!\"\"\"\n",
    "# extracted_code = _extract_python_code(resp)\n",
    "# cpu_code, gpu_code = extracted_code[\"cpu_code\"], extracted_code[\"gpu_code\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b07ac0d-0378-4122-9bbe-969f6bf7e83b",
   "metadata": {},
   "source": [
    "## Agent 3 - Synthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93850901-0361-4fb9-838d-2eed9257268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_rag(query: str, user_profile_path: str = None) -> str:\n",
    "    print(\"\\n--- Running FINAL Integrated RAG Pipeline ---\")\n",
    "    host_node = socket.gethostname()\n",
    "    asurite_id = \"apoojar4\"\n",
    "    llm = ChatOllama(model=\"qwen3:14b\", base_url=f\"http://{asurite_id}@{host_node}:11434/\")\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Load available libraries from file\n",
    "    available_libs = _get_available_libraries(\"installed_libraries.txt\")\n",
    "    library_string = \", \".join(available_libs)\n",
    "\n",
    "    # Call the hybrid retriever to get URLs\n",
    "    # Note: hybrid_retriever is assumed to be defined in the cell above (Cell 5)\n",
    "    urls = hybrid_retriever(query, llm, embedding_model)\n",
    "\n",
    "    context_text = \"\"\n",
    "    if urls:\n",
    "        print(\"-> Found documents. Loading and processing context...\")\n",
    "        docs = [WebBaseLoader(url).load() for url in urls]\n",
    "        docs_list = [item for sublist in docs for item in sublist]\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500, chunk_overlap=100)\n",
    "        doc_splits = text_splitter.split_documents(docs_list)\n",
    "        vectorstore = Chroma.from_documents(documents=doc_splits, embedding=embedding_model, collection_name=\"rag-chroma\")\n",
    "        retriever = vectorstore.as_retriever()\n",
    "        retrieved_docs = retriever.invoke(query)\n",
    "        context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "        vectorstore.delete_collection()\n",
    "\n",
    "    user_profile_content = \"Knowledge Level: Intermediate\\nTUTORING STRATEGY: The user is at an intermediate level. Provide direct code comparisons (e.g., NumPy vs. CuPy), focus on performance benefits, and show clear benchmarking examples.\"\n",
    "    if user_profile_path and os.path.exists(user_profile_path):\n",
    "        print(\"\\n--- Loading the user data:\", user_profile_path)\n",
    "        with open(user_profile_path, 'r') as f:\n",
    "            user_profile_content = f.read()\n",
    "\n",
    "    # THE DEFINITIVE PROMPT, COMBINING ALL LOGIC: USER PROFILE, 3-PATH REASONING, AND LIBRARY CONSTRAINTS\n",
    "    # In Cell 6, replace the final_prompt_template string with this:\n",
    "\n",
    "    # THE ULTIMATE, BALANCED PROMPT (LOGIC + PERSONA)\n",
    "    final_prompt_template = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are an expert AI Tutor and an enthusiastic advocate for NVIDIA technology. Your primary mission is to provide an encouraging, insightful, and clear answer that is perfectly tailored to the user's knowledge level. Your tone should be that of a patient expert who is excited to help users discover the power of GPU acceleration.\n",
    "\n",
    "        **STEP 1: ANALYZE YOUR INSTRUCTIONS**\n",
    "        - First, read the **TUTORING STRATEGY DOCUMENT**. This is your most important instruction. You MUST tailor the tone, depth, and complexity of your final answer to this level.\n",
    "        - Second, read the user's **QUESTION**. Determine if its intent is **theoretical** (what/why) or **procedural** (how-to).\n",
    "        - Third, note the **CODE GENERATION CONSTRAINT**.\n",
    "\n",
    "        **TUTORING STRATEGY DOCUMENT:**\n",
    "        {user_profile}\n",
    "\n",
    "        **CODE GENERATION CONSTRAINT:**\n",
    "        You MUST only use libraries from the following list: `{available_libraries}`.\n",
    "\n",
    "        **STEP 2: CHOOSE YOUR RESPONSE PATH AND APPLY THE TUTOR PERSONA**\n",
    "        Based on the user's intent, follow ONE of the three paths below.\n",
    "\n",
    "        ---\n",
    "        **PATH 1: The question is THEORETICAL.**\n",
    "        - Provide a clear, insightful, and conversational explanation.\n",
    "        - **TAILOR YOUR TONE:** For a Beginner, use simple analogies and focus on the 'wow factor' of the technology. For an Intermediate user, be technically precise. For an Advanced user, discuss deeper architectural concepts and trade-offs.\n",
    "        - You may include ONE concise code snippet ONLY if it is essential to illustrate a key concept. Do not provide comparative code blocks.\n",
    "\n",
    "        ---\n",
    "        **PATH 2: The question is PROCEDURAL and a common NVIDIA-based GPU library EXISTS for the task.**\n",
    "        - Your answer MUST frame the GPU solution as the modern, high-performance standard.\n",
    "        - **TAILOR YOUR TONE:** For a Beginner, start with an encouraging tone, explaining what the code is doing in simple terms and focusing on how accessible GPU power can be. For an Intermediate user, focus on the performance benefits with clear code comparisons. For an Advanced user, provide more optimized code and add notes about advanced features like CUDA streams or memory management.\n",
    "        - Present the `### Recommended GPU Solution` first, followed by the `### Standard CPU Solution` clearly labeled as a \"baseline for comparison.\"\n",
    "        - Always add a \"Performance Note\" or \"Key Benefits\" section to explain the trade-offs and advantages.\n",
    "\n",
    "        ---\n",
    "        **PATH 3: The question is PROCEDURAL and a common NVIDIA-based GPU library DOES NOT EXIST for the task.**\n",
    "        - **TAILOR YOUR TONE:** Explain the standard solution at a level appropriate for the user.\n",
    "        - Provide the `### Standard Solution` with a clear code example.\n",
    "        - After the solution, add a `### GPU Acceleration Note`. In this section, explain *why* this specific task is not typically GPU-accelerated. Then, proactively pivot to an encouraging and helpful suggestion. Frame it as an exciting next step, explaining where the user *could* apply GPU acceleration in a related part of their workflow.\n",
    "        ---\n",
    "\n",
    "        **STEP 3: GENERATE THE FINAL, PERSONALIZED ANSWER**\n",
    "        Use the **RELEVANT CONTEXT** below to find supporting facts or code if helpful, but always follow your primary logic and persona from STEP 1 and STEP 2.\n",
    "\n",
    "        **RELEVANT CONTEXT FROM WEB SEARCH:**\n",
    "        {context}\n",
    "\n",
    "        **USER'S QUESTION:**\n",
    "        {question}\n",
    "\n",
    "        YOUR FINAL, TAILORED ANSWER:\n",
    "        \"\"\",\n",
    "        input_variables=[\"user_profile\", \"question\", \"context\", \"available_libraries\"],\n",
    "    )\n",
    "    \n",
    "    final_chain = final_prompt_template | llm\n",
    "    llm_response_text = final_chain.invoke({\n",
    "        \"user_profile\": user_profile_content,\n",
    "        \"question\": query,\n",
    "        \"context\": context_text,\n",
    "        \"available_libraries\": library_string\n",
    "    }).content\n",
    "    \n",
    "    print(\"--> [Agent 3] Generated conversational answer.\")\n",
    "    print(llm_response_text)\n",
    "\n",
    "    yield {\"status\": \"llm_complete\", \"content\": llm_response_text}\n",
    "\n",
    "    # Code extraction, upscaling, and benchmarking logic remains the same\n",
    "    extracted_code = _extract_python_code(llm_response_text)\n",
    "    cpu_code, gpu_code = extracted_code[\"cpu_code\"], extracted_code[\"gpu_code\"]\n",
    "\n",
    "    if cpu_code and gpu_code:\n",
    "        print(\"--> [Agent 3] Both CPU and GPU code found. Proceeding to upscale and benchmark in parallel.\")\n",
    "        \n",
    "        upscaled_cpu_code = _upscale_code_agentic(cpu_code, llm, library_string)\n",
    "        upscaled_gpu_code = _upscale_code_agentic(gpu_code, llm, library_string)\n",
    "        \n",
    "        yield {\"status\": \"benchmark_running\", \"content\": \"\\n\\n---\\n*⏳ Submitting parallel benchmark jobs to Sol...*\"}\n",
    "\n",
    "        try:\n",
    "            benchmarker = SolBenchmarker(user=asurite_id)\n",
    "            # Loop through the results as they come in from the parallel jobs\n",
    "            for result in benchmarker.run_benchmark_parallel(upscaled_cpu_code, upscaled_gpu_code):\n",
    "                yield result # Pass the result directly up to the UI handler\n",
    "\n",
    "        except ValueError as e:\n",
    "            yield {\"status\": \"error\", \"message\": f\"Benchmark configuration error: {e}\"}\n",
    "    else:\n",
    "        print(\"--> [Agent 3] Did not find both code types. Skipping benchmark.\")\n",
    "\n",
    "    # if cpu_code and gpu_code:\n",
    "    #     print(\"--> [Agent 3] Both CPU and GPU code found. Proceeding to upscale and benchmark.\")\n",
    "    #     yield {\"status\": \"benchmark_running\", \"content\": \"\\n\\n---\\n*⏳ Benchmark in progress on the Sol Supercomputer...*\"}\n",
    "    #     upscaled_cpu_code = _upscale_code_agentic(cpu_code, llm, library_string)\n",
    "    #     upscaled_gpu_code = _upscale_code_agentic(gpu_code, llm, library_string)\n",
    "    #     try:\n",
    "    #         benchmarker = SolBenchmarker(user=asurite_id)\n",
    "    #         benchmark_results = benchmarker.run_benchmark(upscaled_cpu_code, upscaled_gpu_code)\n",
    "    #         if benchmark_results.get(\"status\") == \"success\":\n",
    "    #             cpu_time = benchmark_results.get('cpu_time_seconds')\n",
    "    #             gpu_time = benchmark_results.get('gpu_time_seconds')\n",
    "    #             cpu_time_str = f\"{cpu_time:.4f} seconds\" if cpu_time is not None else \"N/A\"\n",
    "    #             gpu_time_str = f\"{gpu_time:.4f} seconds\" if gpu_time is not None else \"N/A\"\n",
    "                \n",
    "    #             benchmark_md = \"\\n\\n---\\n### 📊 Real-World Benchmark Results (from ASU's Sol Supercomputer)\\n\"\n",
    "    #             benchmark_md += \"| Metric | Result |\\n|---|---|\\n\"\n",
    "    #             benchmark_md += f\"| CPU Time | {cpu_time_str} |\\n\"\n",
    "    #             benchmark_md += f\"| GPU Time | {gpu_time_str} |\\n\"\n",
    "\n",
    "    #             if cpu_time and gpu_time and gpu_time > 0:\n",
    "    #                 speedup = cpu_time / gpu_time\n",
    "    #                 benchmark_md += f\"| **Speedup** | **{speedup:.2f}x faster on GPU!** |\\\\n\"\n",
    "                \n",
    "    #             # llm_response_text += benchmark_md\n",
    "    #         else:\n",
    "    #             benchmark_md = f\"\\n\\n---\\n### ⚠️ Benchmark Failed\\n{benchmark_results.get('message')}\"\n",
    "    #             # llm_response_text += f\"\\n\\n---\\n### ⚠️ Benchmark Failed\\\\n{benchmark_results.get('message')}\"\n",
    "\n",
    "    #         # --- YIELD 3: Send the final benchmark table back to the UI ---\n",
    "    #         yield {\"status\": \"benchmark_complete\", \"content\": benchmark_md}\n",
    "    #     except ValueError as e:\n",
    "    #         # llm_response_text += f\"\\n\\n---\\n### ⚠️ Benchmark Skipped\\\\n{e}\"\n",
    "    #         yield {\"status\": \"benchmark_complete\", \"content\": f\"\\\\n\\\\n---\\n### ⚠️ Benchmark Skipped\\\\n{e}\"}\n",
    "    # else:\n",
    "    #     print(\"--> [Agent 3] Did not find both code types. Skipping benchmark.\")\n",
    "        \n",
    "    print(\"--- Pipeline Complete ---\")\n",
    "    # return llm_response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24215d5-6697-4a43-94cb-f2477d06e7b0",
   "metadata": {},
   "source": [
    "# Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b727ad29-8c12-4802-947b-83eddff3741b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [Agent 1] Successfully connected to Ollama LLM.\n",
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://a36a3a1b0f804b3b06.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a36a3a1b0f804b3b06.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running FINAL Integrated RAG Pipeline ---\n",
      "--> Successfully loaded 575 available libraries from file.\n",
      "-> [Tier 1] Trying Semantic Search on RSS feeds...\n",
      "-> [Tier 1] Found 5 semantically relevant URLs from RSS.\n",
      "-> [Hybrid Retriever] Found 5 final URLs to use for context.\n",
      "-> Found documents. Loading and processing context...\n",
      "\n",
      "--- Loading the user data: user_profiles/ashwith.txt\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Cell 6: Gradio User Interface\n",
    "#\n",
    "# Description: This cell builds the complete Gradio app, integrating all agents.\n",
    "# It includes controls for creating and selecting user profiles, a chat interface for\n",
    "# posing questions to the tutor, and logic to connect the selected user profile to\n",
    "# the RAG pipeline.\n",
    "#\n",
    "\n",
    "# --- Instantiate Agent 1 for use in the UI ---\n",
    "profiler_agent = UserProfilerAgent_V3()\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft(), css=\".gradio-container {background-color: #f5f5f5;}\") as demo:\n",
    "    \n",
    "    # --- State Management for user profiles ---\n",
    "    user_state = gr.State({\n",
    "        \"all_users\": profiler_agent.get_user_list(),\n",
    "        \"current_user\": None\n",
    "    })\n",
    "\n",
    "    gr.Markdown(\"# 🤖 AI Accelerated Data Science Tutor\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            gr.Markdown(\"Ask a question about a data science task. For a personalized response, create or select a user profile.\")\n",
    "        with gr.Column(scale=2, min_width=300):\n",
    "            # --- User Management UI ---\n",
    "            user_dropdown = gr.Dropdown(\n",
    "                label=\"Current User\",\n",
    "                choices=user_state.value[\"all_users\"],\n",
    "                interactive=True\n",
    "            )\n",
    "            with gr.Row():\n",
    "                new_user_btn = gr.Button(\"✚ New / Update User\")\n",
    "\n",
    "    gr.Markdown(\"---\")\n",
    "\n",
    "    # --- Main Chat UI ---\n",
    "    chatbot = gr.Chatbot(label=\"Conversation\", height=500, type=\"messages\")\n",
    "\n",
    "    with gr.Accordion(\"🔎 Show Agent's Thought Process\", open=False):\n",
    "        cot_output = gr.Markdown(\"The agent's reasoning will appear here after it responds.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        msg_textbox = gr.Textbox(\n",
    "            label=\"Your Question\",\n",
    "            placeholder=\"e.g., How do I multiply two 10x10 arrays in Python?\",\n",
    "            scale=4,\n",
    "            lines=2,\n",
    "            container=False\n",
    "        )\n",
    "        submit_btn = gr.Button(\"Ask Tutor\", variant=\"primary\", scale=1, min_width=150)\n",
    "\n",
    "    # --- Questionnaire Modal (Hidden by default) ---\n",
    "    with gr.Group(visible=False) as profiler_ui_group:\n",
    "        with gr.Blocks() as profiler_modal:\n",
    "            gr.Markdown(\"### User Profile Questionnaire\")\n",
    "            gr.Markdown(\"Please answer the following questions to help me tailor my explanations to your knowledge level.\")\n",
    "            profiler_name_input = gr.Textbox(label=\"First and Last Name\")\n",
    "            \n",
    "            q_inputs = [gr.Textbox(label=f\"Question {i+1}\", visible=False) for i in range(5)]\n",
    "\n",
    "            submit_profile_btn = gr.Button(\"Submit Profile\")\n",
    "            cancel_profile_btn = gr.Button(\"Cancel\")\n",
    "\n",
    "    # ==================================\n",
    "    # GRADIO HANDLER FUNCTIONS\n",
    "    # ==================================\n",
    "\n",
    "    # --- NEW: Function 1 - Instant UI Update ---\n",
    "    def on_user_submit(user_message, chat_history):\n",
    "        chat_history = chat_history or []\n",
    "        # Append the user's message and a \"Thinking...\" placeholder immediately\n",
    "        chat_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": \"🧠 Thinking...\"})\n",
    "        # This function returns instantly, guaranteeing the UI updates\n",
    "        return gr.update(value=\"\"), chat_history\n",
    "\n",
    "    # --- NEW: Function 2 - Backend Processing ---\n",
    "    # def get_bot_response(chat_history, selected_user):\n",
    "    #     user_message = chat_history[-2][\"content\"] # Get the user's latest message\n",
    "\n",
    "    #     # Initial check for selected user\n",
    "    #     if not selected_user:\n",
    "    #         chat_history[-1][\"content\"] = \"⚠️ **Warning:** No user profile selected. Please create or select a profile first.\"\n",
    "    #         yield chat_history, \"No user selected.\"\n",
    "    #         return\n",
    "\n",
    "    #     user_profile_path = os.path.join(profiler_agent.profiles_dir, \"_\".join(selected_user.lower().split()) + \".txt\")\n",
    "        \n",
    "    #     # Call the backend generator\n",
    "    #     response_generator = process_with_rag(user_message, user_profile_path)\n",
    "        \n",
    "    #     # Loop through streaming updates\n",
    "    #     for update in response_generator:\n",
    "    #         status = update.get(\"status\")\n",
    "    #         content = update.get(\"content\")\n",
    "            \n",
    "    #         think_match = re.search(r\"<think>(.*?)</think>\", content, re.DOTALL)\n",
    "    #         thought_process = think_match.group(1).strip() if think_match else \"No thought process was found.\"\n",
    "    #         clean_content = re.sub(r\"<think>.*?</think>\", \"\", content, flags=re.DOTALL).strip()\n",
    "            \n",
    "    #         if status == \"llm_complete\":\n",
    "    #             chat_history[-1][\"content\"] = clean_content\n",
    "    #             yield chat_history, thought_process\n",
    "    #         elif status == \"benchmark_running\":\n",
    "    #             chat_history.append({\"role\": \"assistant\", \"content\": clean_content})\n",
    "    #             yield chat_history, thought_process\n",
    "    #         elif status == \"benchmark_complete\":\n",
    "    #             chat_history[-1][\"content\"] = clean_content\n",
    "    #             yield chat_history, thought_process\n",
    "\n",
    "    # REPLACE THE get_bot_response FUNCTION IN CELL 10\n",
    "\n",
    "    def get_bot_response(chat_history, selected_user):\n",
    "        user_message = chat_history[-2][\"content\"]\n",
    "\n",
    "        if not selected_user:\n",
    "            chat_history[-1][\"content\"] = \"⚠️ **Warning:** No user profile selected.\"\n",
    "            yield chat_history, \"No user selected.\"\n",
    "            return\n",
    "\n",
    "        user_profile_path = os.path.join(profiler_agent.profiles_dir, \"_\".join(selected_user.lower().split()) + \".txt\")\n",
    "        response_generator = process_with_rag(user_message, user_profile_path)\n",
    "        \n",
    "        # This dictionary will hold the results as they stream in\n",
    "        benchmark_data = {\"cpu_time\": None, \"gpu_time\": None}\n",
    "        benchmark_message_index = -1\n",
    "\n",
    "        for update in response_generator:\n",
    "            status = update.get(\"status\")\n",
    "            \n",
    "            if status == \"llm_complete\":\n",
    "                content = update.get(\"content\")\n",
    "                think_match = re.search(r\"<think>(.*?)</think>\", content, re.DOTALL)\n",
    "                thought_process = think_match.group(1).strip() if think_match else \"No thought process found.\"\n",
    "                clean_content = re.sub(r\"<think>.*?</think>\", \"\", content, flags=re.DOTALL).strip()\n",
    "                chat_history[-1][\"content\"] = clean_content\n",
    "                yield chat_history, thought_process\n",
    "            \n",
    "            elif status == \"benchmark_running\":\n",
    "                # Append a new message bubble for the benchmark status\n",
    "                chat_history.append({\"role\": \"assistant\", \"content\": update.get(\"content\")})\n",
    "                benchmark_message_index = len(chat_history) - 1 # Remember the position of this message\n",
    "                yield chat_history, \"Benchmark jobs submitted...\"\n",
    "\n",
    "            elif status == \"complete\":\n",
    "                # A job (CPU or GPU) has finished!\n",
    "                job_type = update.get(\"job_type\")\n",
    "                time_val = update.get(\"time\")\n",
    "                if job_type == \"gpu\": benchmark_data[\"gpu_time\"] = time_val\n",
    "                elif job_type == \"cpu\": benchmark_data[\"cpu_time\"] = time_val\n",
    "                \n",
    "                # --- Re-create the results table with whatever data we have so far ---\n",
    "                gpu_time_str = f\"{benchmark_data['gpu_time']:.4f}s\" if benchmark_data[\"gpu_time\"] is not None else \"⏳ In Progress...\"\n",
    "                cpu_time_str = f\"{benchmark_data['cpu_time']:.4f}s\" if benchmark_data[\"cpu_time\"] is not None else \"⏳ In Progress...\"\n",
    "\n",
    "                benchmark_md = \"\\n\\n---\\n### 📊 Live Benchmark Results (from ASU's Sol Supercomputer)\\n\"\n",
    "                benchmark_md += \"| Metric | Result |\\n|---|---|\\n\"\n",
    "                benchmark_md += f\"| CPU Time | {cpu_time_str} |\\n\"\n",
    "                benchmark_md += f\"| GPU Time | {gpu_time_str} |\\n\"\n",
    "                if benchmark_data[\"cpu_time\"] and benchmark_data[\"gpu_time\"]:\n",
    "                    speedup = benchmark_data[\"cpu_time\"] / benchmark_data[\"gpu_time\"]\n",
    "                    benchmark_md += f\"| **Speedup** | **{speedup:.2f}x faster on GPU!** |\\\\n\"\n",
    "\n",
    "                # Update the benchmark status message with the new table\n",
    "                if benchmark_message_index != -1:\n",
    "                    chat_history[benchmark_message_index][\"content\"] = benchmark_md\n",
    "                yield chat_history, \"Benchmark update received...\"\n",
    "\n",
    "            elif status == \"error\":\n",
    "                 chat_history.append({\"role\": \"assistant\", \"content\": f\"⚠️ Benchmark Error: {update.get('message')}\"})\n",
    "                 yield chat_history, \"Benchmark error.\"\n",
    "\n",
    "    # --- User Profile Logic ---\n",
    "    def start_profiling_flow(current_user):\n",
    "        \"\"\"Called when 'New / Update User' is clicked.\"\"\"\n",
    "        questions = profiler_agent._generate_questions_with_llm()\n",
    "        \n",
    "        updates = [gr.update(visible=True)]\n",
    "        updates.append(gr.update(value=current_user if current_user else \"\", visible=True))\n",
    "        \n",
    "        for i in range(5):\n",
    "            if i < len(questions):\n",
    "                updates.append(gr.update(label=questions[i], value=\"\", visible=True))\n",
    "            else:\n",
    "                updates.append(gr.update(visible=False))\n",
    "        \n",
    "        return *updates, questions\n",
    "\n",
    "    # --- FIX 1: Add 'questions' to the function signature to receive it from the state component. ---\n",
    "    def process_profile_submission(user_name, state, questions, *answers):\n",
    "        \"\"\"Called when 'Submit Profile' is clicked.\"\"\"\n",
    "        \n",
    "        if not user_name:\n",
    "            gr.Warning(\"User name cannot be empty!\")\n",
    "            return state, gr.update(choices=state[\"all_users\"], value=state[\"current_user\"]), gr.update(visible=True)\n",
    "\n",
    "        answers_dict = {q: a for q, a in zip(questions, answers) if q and a}\n",
    "        profiler_agent.generate_and_save_report(user_name, answers_dict)\n",
    "        \n",
    "        gr.Info(f\"Profile for {user_name} has been saved!\")\n",
    "\n",
    "        updated_users = profiler_agent.get_user_list()\n",
    "        state[\"all_users\"] = updated_users\n",
    "        state[\"current_user\"] = user_name\n",
    "        \n",
    "        return state, gr.update(choices=updated_users, value=user_name), gr.update(visible=False)\n",
    "\n",
    "    def cancel_profiling():\n",
    "        return gr.update(visible=False)\n",
    "\n",
    "    def update_current_user(selected_user, state):\n",
    "        state[\"current_user\"] = selected_user\n",
    "        gr.Info(f\"Switched to user: {selected_user}\")\n",
    "        return state\n",
    "\n",
    "    # ==================================\n",
    "    # WIRING UP THE UI EVENTS\n",
    "    # ==================================\n",
    "    \n",
    "    question_state = gr.State([])\n",
    "\n",
    "    submit_btn.click(\n",
    "        on_user_submit,\n",
    "        [msg_textbox, chatbot],\n",
    "        [msg_textbox, chatbot]\n",
    "    ).then(\n",
    "        get_bot_response,\n",
    "        [chatbot, user_dropdown],\n",
    "        [chatbot, cot_output],\n",
    "        show_progress=\"hidden\"\n",
    "    )\n",
    "\n",
    "    msg_textbox.submit(\n",
    "        on_user_submit,\n",
    "        [msg_textbox, chatbot],\n",
    "        [msg_textbox, chatbot]\n",
    "    ).then(\n",
    "        get_bot_response,\n",
    "        [chatbot, user_dropdown],\n",
    "        [chatbot, cot_output],\n",
    "        show_progress=\"hidden\"\n",
    "    )\n",
    "\n",
    "    new_user_btn.click(\n",
    "        start_profiling_flow,\n",
    "        inputs=[user_dropdown],\n",
    "        outputs=[profiler_ui_group, profiler_name_input] + q_inputs + [question_state]\n",
    "    )\n",
    "    \n",
    "    submit_profile_btn.click(\n",
    "        process_profile_submission,\n",
    "        # --- FIX 2: Pass the 'question_state' component as an input. ---\n",
    "        inputs=[profiler_name_input, user_state, question_state, *q_inputs],\n",
    "        outputs=[user_state, user_dropdown, profiler_ui_group]\n",
    "    )\n",
    "\n",
    "    cancel_profile_btn.click(cancel_profiling, outputs=[profiler_ui_group])\n",
    "\n",
    "    user_dropdown.change(\n",
    "        update_current_user,\n",
    "        inputs=[user_dropdown, user_state],\n",
    "        outputs=[user_state]\n",
    "    )\n",
    "\n",
    "# --- Launch the Application ---\n",
    "demo.queue().launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef308b8b-f117-48af-be24-bda680fa8377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai25.07",
   "language": "python",
   "name": "genai25.07"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
