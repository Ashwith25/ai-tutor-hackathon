{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9cb884c2-8e0d-459c-a2d2-07d56734e661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import socket\n",
    "from typing import List, Dict\n",
    "import subprocess\n",
    "import time\n",
    "import re\n",
    "\n",
    "# LangChain and related libraries\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "# Search library\n",
    "try:\n",
    "    from googlesearch import search\n",
    "except ImportError:\n",
    "    print(\"Error: 'googlesearch-python' is not installed. Please run 'pip install googlesearch-python'\")\n",
    "\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0cdcd502-f05b-4322-b9c7-6d13ed1c802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# AGENT 4: SolBenchmarker CLASS (from Agent4-1.ipynb)\n",
    "# =================================================================\n",
    "class SolBenchmarker:\n",
    "    def __init__(self, user: str, python_env: str = \"rapids25.02\"):\n",
    "        if not user or user == \"YOUR_ASURITE_ID\":\n",
    "            raise ValueError(\"A valid ASURITE username is required for SolBenchmarker.\")\n",
    "        self.user = user\n",
    "        self.python_env = python_env\n",
    "\n",
    "    def _generate_sbatch_script(self, script_dir: str, cpu_script_name: str, gpu_script_name: str) -> str:\n",
    "        return f\"\"\"#!/bin/bash\n",
    "#SBATCH -p general\n",
    "#SBATCH -q public\n",
    "#SBATCH -G 1\n",
    "#SBATCH -A grp_hackathon2025\n",
    "#SBATCH --reservation=hackathon2025\n",
    "#SBATCH -t 0-00:10:00\n",
    "#SBATCH -c 1\n",
    "#SBATCH -o {script_dir}/slurm-%j.out\n",
    "#SBATCH -e {script_dir}/slurm-%j.err\n",
    "\n",
    "module load mamba/latest\n",
    "source activate {self.python_env}\n",
    "\n",
    "echo \"--- STARTING CPU BENCHMARK ---\"\n",
    "/usr/bin/time -p python3 {script_dir}/{cpu_script_name} 2>&1\n",
    "echo \"--- FINISHED CPU BENCHMARK ---\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"--- STARTING GPU BENCHMARK ---\"\n",
    "/usr/bin/time -p python3 {script_dir}/{gpu_script_name} 2>&1\n",
    "echo \"--- FINISHED GPU BENCHMARK ---\"\n",
    "\"\"\"\n",
    "\n",
    "    def _parse_output(self, output_content: str) -> dict:\n",
    "        try:\n",
    "            real_times = re.findall(r\"real\\s+([\\d.]+)\", output_content)\n",
    "            cpu_time = float(real_times[0]) if len(real_times) > 0 else None\n",
    "            gpu_time = float(real_times[1]) if len(real_times) > 1 else None\n",
    "            print(\"cpu_time_seconds\", cpu_time, \"gpu_time_seconds\", gpu_time)\n",
    "            return {\"status\": \"success\", \"cpu_time_seconds\": cpu_time, \"gpu_time_seconds\": gpu_time}\n",
    "        except (IndexError, ValueError) as e:\n",
    "            return {\"status\": \"error\", \"message\": f\"Failed to parse benchmark times. Error: {e}\", \"raw_log\": output_content}\n",
    "\n",
    "    def run_benchmark(self, cpu_code: str, gpu_code: str) -> dict:\n",
    "        benchmark_dir = os.path.join(os.getcwd(), \"benchmark_files\")\n",
    "        os.makedirs(benchmark_dir, exist_ok=True)\n",
    "        \n",
    "        cpu_script_path = os.path.join(benchmark_dir, \"cpu_benchmark.py\")\n",
    "        gpu_script_path = os.path.join(benchmark_dir, \"gpu_benchmark.py\")\n",
    "        sbatch_path = os.path.join(benchmark_dir, \"benchmark_job.sh\")\n",
    "\n",
    "        try:\n",
    "            with open(cpu_script_path, \"w\") as f: f.write(cpu_code)\n",
    "            with open(gpu_script_path, \"w\") as f: f.write(gpu_code)\n",
    "            sbatch_script = self._generate_sbatch_script(benchmark_dir, \"cpu_benchmark.py\", \"gpu_benchmark.py\")\n",
    "            with open(sbatch_path, \"w\") as f: f.write(sbatch_script)\n",
    "\n",
    "            process = subprocess.run(f\"sbatch {sbatch_path}\", shell=True, capture_output=True, text=True)\n",
    "            if process.returncode != 0: raise RuntimeError(f\"sbatch submission failed: {process.stderr}\")\n",
    "\n",
    "            job_id_match = re.search(r\"Submitted batch job (\\d+)\", process.stdout.strip())\n",
    "            if not job_id_match: raise RuntimeError(f\"Could not parse Job ID from sbatch output: {process.stdout}\")\n",
    "            job_id = job_id_match.group(1)\n",
    "            print(f\"--> [Agent 4] Submitted benchmark job to SLURM with ID: {job_id}\")\n",
    "\n",
    "            print(\"--> [Agent 4] Waiting for job to complete...\")\n",
    "            while True:\n",
    "                queue_process = subprocess.run(f\"squeue -u {self.user} -j {job_id}\", shell=True, capture_output=True, text=True)\n",
    "                if job_id not in queue_process.stdout: break\n",
    "                time.sleep(10)\n",
    "\n",
    "            print(f\"--> [Agent 4] Job {job_id} completed.\")\n",
    "            output_file_path = os.path.join(benchmark_dir, f\"slurm-{job_id}.out\")\n",
    "            \n",
    "            if not os.path.exists(output_file_path):\n",
    "                 err_file_path = os.path.join(benchmark_dir, f\"slurm-{job_id}.err\")\n",
    "                 if os.path.exists(err_file_path):\n",
    "                     with open(err_file_path, \"r\") as f: error_content = f.read()\n",
    "                     return {\"status\": \"error\", \"message\": f\"Job failed. See error log: {error_content}\"}\n",
    "                 return {\"status\": \"error\", \"message\": f\"Output file not found.\"}\n",
    "\n",
    "            with open(output_file_path, \"r\") as f: output_content = f.read()\n",
    "            return self._parse_output(output_content)\n",
    "        except Exception as e:\n",
    "            return {\"status\": \"error\", \"message\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ecd732b6-39d7-4603-ad91-5a2cb41f6102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# AI TUTOR AGENT (DEFINITIVE, DYNAMIC & FULLY ABSTRACT FINAL)\n",
    "# =================================================================\n",
    "\n",
    "class SearchQueryGenerator(BaseModel):\n",
    "    queries: List[str] = Field(description=\"A list of targeted, keyword-focused search queries.\")\n",
    "\n",
    "# --- Part 1: Fully Abstracted Search Query Generation ---\n",
    "def generate_search_queries(query: str, llm) -> List[str]:\n",
    "    \"\"\"\n",
    "    Uses a dynamic prompt to generate language-and-library-specific search queries.\n",
    "    \"\"\"\n",
    "    print(\"-> Using LLM with FULLY ABSTRACTED prompt to generate search queries...\")\n",
    "    \n",
    "    # DYNAMIC PROMPT: The LLM now identifies the correct libraries and language on its own.\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are an expert at generating web search queries for a technical audience.\n",
    "        Analyze the user's question to identify the core technical task and the programming language.\n",
    "        Based on your knowledge, generate 5 concise, targeted search queries. Two queries should be for the standard, CPU-based library for that task/language. Three queries should be for potential GPU-accelerated libraries for that task/language, prioritizing NVIDIA-based solutions if they exist.\n",
    "\n",
    "        User Question: \"{question}\"\n",
    "        \n",
    "        Generate a JSON list of 5 search query strings.\n",
    "        \"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "    \n",
    "    query_generation_chain = prompt_template | llm.with_structured_output(SearchQueryGenerator)\n",
    "\n",
    "    try:\n",
    "        response_model = query_generation_chain.invoke({\"question\": query})\n",
    "        print(f\"-> Generated queries: {response_model.queries}\")\n",
    "        return response_model.queries\n",
    "    except Exception as e:\n",
    "        print(f\"-> LLM failed to generate structured output: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Part 2: The Main Search Function (Unchanged) ---\n",
    "def dynamic_search_agentic(queries: List[str]) -> list[str]:\n",
    "    print(\"-> Executing dynamic search...\")\n",
    "    all_urls = set()\n",
    "    for q in queries:\n",
    "        try:\n",
    "            enhanced_query = f\"{q} site:developer.nvidia.com OR site:medium.com/rapids-ai OR site:medium.com/cupy-team\"\n",
    "            search_results = list(search(enhanced_query, num_results=2))\n",
    "            for url in search_results:\n",
    "                all_urls.add(url)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during search for query '{q}': {e}\")\n",
    "            continue\n",
    "    final_urls = [url for url in list(all_urls) if url]\n",
    "    print(f\"-> Found {len(final_urls)} unique URLs: {final_urls}\")\n",
    "    return final_urls\n",
    "\n",
    "def _extract_python_code(markdown_text: str) -> Dict[str, str]:\n",
    "    \"\"\"Helper function to parse CPU and GPU code blocks from the LLM's response.\"\"\"\n",
    "    # This helper function remains the same as before...\n",
    "    code_pattern = r\"```python\\n(.*?)\\n```\"\n",
    "    gpu_heading_pattern = r\"### Recommended GPU Solution.*?\\n\"\n",
    "    cpu_heading_pattern = r\"### Standard CPU Solution.*?\\n\"\n",
    "    gpu_section_match = re.search(gpu_heading_pattern, markdown_text, re.DOTALL | re.IGNORECASE)\n",
    "    cpu_section_match = re.search(cpu_heading_pattern, markdown_text, re.DOTALL | re.IGNORECASE)\n",
    "    gpu_code = \"\"\n",
    "    cpu_code = \"\"\n",
    "    if gpu_section_match:\n",
    "        section_start = gpu_section_match.end()\n",
    "        code_match = re.search(code_pattern, markdown_text[section_start:], re.DOTALL)\n",
    "        if code_match: gpu_code = code_match.group(1).strip()\n",
    "    if cpu_section_match:\n",
    "        section_start = cpu_section_match.end()\n",
    "        code_match = re.search(code_pattern, markdown_text[section_start:], re.DOTALL)\n",
    "        if code_match: cpu_code = code_match.group(1).strip()\n",
    "    return {\"cpu_code\": cpu_code, \"gpu_code\": gpu_code}\n",
    "\n",
    "# --- Part 3: The Definitive Dynamic RAG Pipeline ---\n",
    "def process_with_rag(query: str) -> str:\n",
    "    print(\"--- Running FINAL DYNAMIC RAG Pipeline ---\")\n",
    "\n",
    "    host_node = socket.gethostname()\n",
    "    llm = ChatOllama(model=\"qwen3:14b\", base_url=f\"http://apoojar4@{host_node}:11434/\")\n",
    "\n",
    "    search_queries = generate_search_queries(query, llm)\n",
    "    urls = []\n",
    "    if search_queries:\n",
    "        urls = dynamic_search_agentic(search_queries)\n",
    "\n",
    "    context_text = \"\"\n",
    "    if urls:\n",
    "        print(\"-> Found documents. Loading and processing context...\")\n",
    "        docs = [WebBaseLoader(url).load() for url in urls]\n",
    "        docs_list = [item for sublist in docs for item in sublist]\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500, chunk_overlap=100)\n",
    "        doc_splits = text_splitter.split_documents(docs_list)\n",
    "        embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        vectorstore = Chroma.from_documents(documents=doc_splits, embedding=embedding_model, collection_name=\"rag-chroma\")\n",
    "        retriever = vectorstore.as_retriever()\n",
    "        retrieved_docs = retriever.invoke(query)\n",
    "        context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "        vectorstore.delete_collection()\n",
    "\n",
    "    # THE DEFINITIVE, FULLY ABSTRACTED \"NVIDIA-FIRST EDUCATIONAL PRIORITY\" PROMPT\n",
    "    final_prompt_template = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are a friendly and knowledgeable AI Tutor for a project focused on data science acceleration. Your primary mission is to educate users on leveraging NVIDIA-based GPU libraries.\n",
    "\n",
    "        First, analyze the user's QUESTION to identify the core task and programming language. Based on your knowledge, determine if a common NVIDIA-based GPU-accelerated library (like cuPy, cuDF, Rapids) exists for that specific task and language.\n",
    "\n",
    "        Then, follow the appropriate path below to structure your conversational and helpful answer:\n",
    "\n",
    "        **PATH 1: An NVIDIA-based GPU-accelerated library EXISTS for this task.**\n",
    "        1.  Identify the standard CPU library and the NVIDIA GPU library for the user's language and task.\n",
    "        2.  Start with a friendly opening that explains you will show both the GPU-accelerated and standard methods.\n",
    "        3.  Provide a heading for the GPU solution, dynamically inserting the library name (e.g., `### Recommended GPU Solution (with [GPU Library Name])`).\n",
    "        4.  Write the code example for the GPU solution.\n",
    "        5.  Add a \"Performance Note\" section. Explain the benefits of the GPU approach (e.g., for large datasets) and the trade-offs (e.g., performance on small data vs. large data, data transfer overhead).\n",
    "        6.  Provide a heading for the CPU solution, dynamically inserting the library name (e.g., `### Standard CPU Solution (with [CPU Library Name])`).\n",
    "        7.  Write the CPU-based code for comparison.\n",
    "        8.  Do NOT add any disclaimer note at the end.\n",
    "\n",
    "        **PATH 2: An NVIDIA-based GPU-accelerated library DOES NOT EXIST for this task.**\n",
    "        1.  Identify the standard library for the user's specified language and task.\n",
    "        2.  Start with a friendly opening explaining the standard approach.\n",
    "        3.  Provide a heading for the standard solution, dynamically inserting the library name (e.g., `### Standard Solution (with [Library Name])`).\n",
    "        4.  Write the code example using the identified standard library.\n",
    "        5.  End your entire response with the exact sentence: \"Note: The provided solution is the standard method for this task, as a direct NVIDIA-based GPU library for it is not common.\"\n",
    "\n",
    "        Use the CONTEXT below to inform your answer if it is relevant, but your primary instruction is to follow the mission and logic paths described above.\n",
    "\n",
    "        CONTEXT:\n",
    "        {context}\n",
    "\n",
    "        QUESTION:\n",
    "        {question}\n",
    "\n",
    "        YOUR FINAL ANSWER:\n",
    "        \"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "    \n",
    "    final_chain = final_prompt_template | llm\n",
    "    llm_response_text = final_chain.invoke({\"context\": context_text, \"question\": query}).content\n",
    "    asurite_id = \"apoojar4\"\n",
    "\n",
    "    print(\"--> [Agent 3] Generated conversational answer.\")\n",
    "\n",
    "    extracted_code = _extract_python_code(llm_response_text)\n",
    "    cpu_code = extracted_code[\"cpu_code\"]\n",
    "    gpu_code = extracted_code[\"gpu_code\"]\n",
    "    \n",
    "    if cpu_code and gpu_code:\n",
    "        print(\"--> [Agent 3] Both CPU and GPU code found. Invoking Agent 4 for benchmarking.\")\n",
    "        try:\n",
    "            benchmarker = SolBenchmarker(user=asurite_id)\n",
    "            benchmark_results = benchmarker.run_benchmark(cpu_code, gpu_code)\n",
    "            \n",
    "            if benchmark_results.get(\"status\") == \"success\":\n",
    "                cpu_time = benchmark_results.get('cpu_time_seconds')\n",
    "                gpu_time = benchmark_results.get('gpu_time_seconds')\n",
    "                print(benchmark_results.get('raw_log', 'No raw log available.'))\n",
    "                \n",
    "                # --- THIS IS THE FIX ---\n",
    "                # We now check if the time values are None before formatting them.\n",
    "                cpu_time_str = f\"{cpu_time:.4f} seconds\" if cpu_time is not None else \"N/A (script may have failed)\"\n",
    "                gpu_time_str = f\"{gpu_time:.4f} seconds\" if gpu_time is not None else \"N/A (script may have failed)\"\n",
    "                \n",
    "                benchmark_md = \"\\n\\n---\\n### ðŸ“Š Real-World Benchmark Results (from ASU's Sol Supercomputer)\\n\"\n",
    "                benchmark_md += \"| Metric | Result |\\n|---|---|\\n\"\n",
    "                benchmark_md += f\"| CPU Time | {cpu_time_str} |\\n\"\n",
    "                benchmark_md += f\"| GPU Time | {gpu_time_str} |\\n\"\n",
    "                \n",
    "                if cpu_time and gpu_time and gpu_time > 0:\n",
    "                    speedup = cpu_time / gpu_time\n",
    "                    benchmark_md += f\"| **Speedup** | **{speedup:.2f}x faster on GPU!** |\\n\"\n",
    "                \n",
    "                llm_response_text += benchmark_md\n",
    "            else:\n",
    "                llm_response_text += f\"\\n\\n---\\n### âš ï¸ Benchmark Failed\\nCould not retrieve benchmark results: {benchmark_results.get('message')}\"\n",
    "    \n",
    "        except ValueError as e:\n",
    "            llm_response_text += f\"\\n\\n---\\n### âš ï¸ Benchmark Skipped\\nConfiguration error: {e}\"\n",
    "    else:\n",
    "        print(\"--> [Agent 3] Did not find both code types. Skipping benchmark.\")\n",
    "        \n",
    "    print(\"--- Pipeline Complete ---\")\n",
    "    print(llm_response_text)\n",
    "    return llm_response_text\n",
    "    # return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24215d5-6697-4a43-94cb-f2477d06e7b0",
   "metadata": {},
   "source": [
    "# Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b727ad29-8c12-4802-947b-83eddff3741b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1217384/790829847.py:51: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
      "  chatbot = gr.Chatbot(label=\"Conversation\", height=450, bubble_full_width=False, type=\"messages\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://0a9c369662697857b8.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://0a9c369662697857b8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running FINAL DYNAMIC RAG Pipeline ---\n",
      "-> Using LLM with FULLY ABSTRACTED prompt to generate search queries...\n",
      "-> Generated queries: ['python matrix multiplication numpy', 'numpy matrix multiply 25x50', 'pycuda matrix multiplication', 'cupy matrix multiply', 'pytorch matrix multiplication gpu']\n",
      "-> Executing dynamic search...\n",
      "-> Found 8 unique URLs: ['https://forums.developer.nvidia.com/t/how-to-increase-speed-transfer-of-matrices-gpu-cpu-for-matrix-multiplication-it-is-the-limiting-factor/56251', 'https://forums.developer.nvidia.com/t/passing-scalar-to-functions-cupy-pycuda-scalar-multiplication-of-a-vector/179453', 'https://forums.developer.nvidia.com/t/how-can-i-customize-matrix-multiplication-on-dla/303139', 'https://forums.developer.nvidia.com/t/need-help-in-implementing-matrix-multiplication-using-shared-memory-in-numba/111461', 'https://forums.developer.nvidia.com/t/matrix-multiplication-of-many-small-sized-matrices/117564', 'https://forums.developer.nvidia.com/t/batch-matrix-multiplication-using-cublas/167325', 'https://forums.developer.nvidia.com/t/matrix-multiplication-in-cuda/54391', 'https://developer.nvidia.com/blog/icymi-leveraging-the-power-of-gpus-with-cupy-in-python/']\n",
      "-> Found documents. Loading and processing context...\n",
      "--> [Agent 3] Generated conversational answer.\n",
      "--> [Agent 3] Both CPU and GPU code found. Invoking Agent 4 for benchmarking.\n",
      "--> [Agent 4] Submitted benchmark job to SLURM with ID: 28523278\n",
      "--> [Agent 4] Waiting for job to complete...\n",
      "--> [Agent 4] Job 28523278 completed.\n",
      "cpu_time_seconds 0.19 gpu_time_seconds 2.09\n",
      "No raw log available.\n",
      "--- Pipeline Complete ---\n",
      "<think>\n",
      "Okay, the user is asking how to multiply two matrices of size 25x50 in Python. Let me think about the best way to approach this.\n",
      "\n",
      "First, I need to check if there's an NVIDIA GPU-accelerated library that can handle matrix multiplication for this task. The standard CPU library for matrix operations in Python is NumPy. For GPU acceleration, cuPy is a drop-in replacement for NumPy that uses CUDA, so that's probably the GPU library here.\n",
      "\n",
      "The user's question is straightforward matrix multiplication, which is a common operation. Both NumPy and cuPy have functions for this. So PATH 1 applies here because cuPy exists for this task.\n",
      "\n",
      "I should start by explaining both the CPU and GPU solutions. For the GPU solution, I'll use cuPy's matmul function. For the CPU, it's NumPy's dot or matmul. \n",
      "\n",
      "Wait, the example in the context used cuBLAS and Numba, but the user's question is about a simple multiplication. Since cuPy is more straightforward here, I should recommend that. \n",
      "\n",
      "I need to make sure the code examples are correct. For cuPy, the code would be something like importing cupy as cp, converting the matrices to cupy arrays, then using cp.matmul. For NumPy, it's similar but with np.\n",
      "\n",
      "Also, the performance note should mention that for small matrices like 25x50, the overhead of transferring data to the GPU might make CPU faster, but for larger data, GPU would be better. But since the user's matrices are small, maybe the CPU is better here. However, the user might be interested in learning how to use GPU libraries even for small matrices.\n",
      "\n",
      "I should structure the answer with the headings as specified, showing both solutions. Then the performance note. No disclaimer at the end. Let me put that all together.\n",
      "</think>\n",
      "\n",
      "### Recommended GPU Solution (with cuPy)\n",
      "To perform matrix multiplication on the GPU, we can use **cuPy**, an NVIDIA-accelerated library that mirrors NumPy's API. Below is an example:\n",
      "\n",
      "```python\n",
      "import cupy as cp\n",
      "\n",
      "# Define matrices (25x50)\n",
      "A = cp.random.rand(25, 50)\n",
      "B = cp.random.rand(50, 25)  # Note: Second matrix must be 50x25 for multiplication\n",
      "\n",
      "# Perform matrix multiplication on the GPU\n",
      "result_gpu = cp.matmul(A, B)\n",
      "print(\"GPU Result:\\n\", result_gpu)\n",
      "```\n",
      "\n",
      "### Standard CPU Solution (with NumPy)\n",
      "For comparison, here's the equivalent implementation using **NumPy** on the CPU:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "# Define matrices (25x50)\n",
      "A = np.random.rand(25, 50)\n",
      "B = np.random.rand(50, 25)  # Note: Second matrix must be 50x25 for multiplication\n",
      "\n",
      "# Perform matrix multiplication on the CPU\n",
      "result_cpu = np.matmul(A, B)\n",
      "print(\"CPU Result:\\n\", result_cpu)\n",
      "```\n",
      "\n",
      "### Performance Note\n",
      "For small matrices (e.g., 25x50), the CPU solution is typically faster due to the overhead of transferring data to the GPU. However, for large-scale computations or repeated operations, the GPU solution (cuPy) can provide significant speedups. Always profile both approaches for your specific use case.\n",
      "\n",
      "---\n",
      "### ðŸ“Š Real-World Benchmark Results (from ASU's Sol Supercomputer)\n",
      "| Metric | Result |\n",
      "|---|---|\n",
      "| CPU Time | 0.1900 seconds |\n",
      "| GPU Time | 2.0900 seconds |\n",
      "| **Speedup** | **0.09x faster on GPU!** |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# GRADIO CHAT APPLICATION FOR AI TUTOR (FINAL)\n",
    "#\n",
    "# This version fixes the UserWarning by specifying the modern 'messages'\n",
    "# format for the chatbot component.\n",
    "# =================================================================\n",
    "\n",
    "import gradio as gr\n",
    "import re\n",
    "import time\n",
    "\n",
    "# This wrapper function connects our backend logic to the Gradio UI.\n",
    "# It assumes 'process_with_rag' is defined and available in the notebook environment.\n",
    "def tutor_chat_interface(user_message, history):\n",
    "    # The 'history' is now a list of dictionaries. Append the new user message.\n",
    "    history.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    # Show a \"thinking...\" message while processing\n",
    "    yield history + [{\"role\": \"assistant\", \"content\": \"Thinking...\"}], \"Thinking...\"\n",
    "\n",
    "    # Call our existing RAG pipeline function\n",
    "    full_response = process_with_rag(user_message)\n",
    "\n",
    "    # Use regex to separate the <think> block from the final answer\n",
    "    think_pattern = re.compile(r\"<think>(.*?)</think>\", re.DOTALL)\n",
    "    think_match = think_pattern.search(full_response)\n",
    "    \n",
    "    thought_process = \"No thought process was found in the response.\"\n",
    "    if think_match:\n",
    "        # Extract the thought process and clean it up\n",
    "        thought_process = think_match.group(1).strip()\n",
    "        # Remove the <think> block from the final answer shown to the user\n",
    "        final_answer = think_pattern.sub(\"\", full_response).strip()\n",
    "    else:\n",
    "        # If no <think> block, the whole response is the answer\n",
    "        final_answer = full_response\n",
    "\n",
    "    # Update the chatbot history with the AI's clean answer\n",
    "    history.append({\"role\": \"assistant\", \"content\": final_answer})\n",
    "    \n",
    "    # Return the final history and the extracted thought process\n",
    "    yield history, thought_process\n",
    "\n",
    "\n",
    "# Build the Gradio UI using Blocks for more control\n",
    "with gr.Blocks(theme=gr.themes.Soft(), css=\".gradio-container {background-color: #f5f5f5;}\") as demo:\n",
    "    gr.Markdown(\"# ðŸ¤– AI Accelerated Data Science Tutor\")\n",
    "    gr.Markdown(\"Ask a question about a data science task. The tutor will provide an explanation and code, prioritizing NVIDIA GPU-accelerated solutions where possible.\")\n",
    "\n",
    "    # THE FIX IS HERE: Added 'type=\"messages\"' to the chatbot component.\n",
    "    chatbot = gr.Chatbot(label=\"Conversation\", height=450, bubble_full_width=False, type=\"messages\")\n",
    "    \n",
    "    with gr.Accordion(\"ðŸ”Ž Show Agent's Thought Process\", open=False):\n",
    "        cot_output = gr.Markdown(\"The agent's reasoning will appear here after it responds.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        msg_textbox = gr.Textbox(\n",
    "            label=\"Your Question\",\n",
    "            placeholder=\"e.g., How do I multiply two 10x10 arrays in Python?\",\n",
    "            scale=4,\n",
    "            autofocus=True,\n",
    "            container=False # This makes the textbox look cleaner\n",
    "        )\n",
    "        submit_btn = gr.Button(\"Ask\", variant=\"primary\", scale=1, min_width=150)\n",
    "\n",
    "    # Main function to handle the chat logic\n",
    "    def handle_submit(user_message, chat_history):\n",
    "        response_generator = tutor_chat_interface(user_message, chat_history)\n",
    "        \n",
    "        # The generator now yields the full message history and the thought process\n",
    "        for history_state, thought_process in response_generator:\n",
    "             yield history_state, thought_process\n",
    "\n",
    "\n",
    "    # Connect the submit button and textbox to the handler function\n",
    "    submit_btn.click(\n",
    "        handle_submit, \n",
    "        [msg_textbox, chatbot], \n",
    "        [chatbot, cot_output]\n",
    "    ).then(lambda: gr.update(value=\"\"), None, [msg_textbox], queue=False)\n",
    "\n",
    "    msg_textbox.submit(\n",
    "        handle_submit, \n",
    "        [msg_textbox, chatbot], \n",
    "        [chatbot, cot_output]\n",
    "    ).then(lambda: gr.update(value=\"\"), None, [msg_textbox], queue=False)\n",
    "\n",
    "\n",
    "# Launch the application\n",
    "# Set share=True if you need a public link from the Sol jupyter notebook\n",
    "demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc567dd-6a47-465c-9ce8-eac5c35b0752",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai25.06",
   "language": "python",
   "name": "genai25.06"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
